{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import Perplexity\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.vocab import GloVe\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import class Cider_Scorer from pycocoevalcap/cider/cider_scorer.py file\n",
    "from pycocoevalcap.cider.cider_scorer import CiderScorer\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for training\n",
    "cwd = os.getcwd()\n",
    "DATA_DIR = cwd + '/data-processed'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'memes')\n",
    "CAPTIONS_PATH = os.path.join(DATA_DIR, 'cleaned_english_captions_v1.json')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "This is where we define the text embedding model to use. Here we are building our embedding model from the approach provided here\n",
    "\n",
    "https://www.kaggle.com/code/ratthachat/flickr-image-captioning-tpu-tf2-glove?scriptVersionId=34452283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens for the caption (they exist in the GloVe vocabulary)\n",
    "SPECIAL_TOKENS = {\n",
    "    'UNK': '<u>',\n",
    "    'PAD': '<p>',\n",
    "    'EOS': '<s>',\n",
    "}\n",
    "EMBEDDING_DIMENSIONS = 50\n",
    "VOCABULARY_PATH = os.path.join(DATA_DIR, 'vocabulary.json')\n",
    "VECTORS_PATH = os.path.join(DATA_DIR, f'vectors{EMBEDDING_DIMENSIONS}.pt')\n",
    "MAX_CAPTION_LENGTH = 50 # Maximum number of words in a caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 84880\n"
     ]
    }
   ],
   "source": [
    "# Generate the vocabulary from all captions we have\n",
    "\n",
    "if os.path.exists(VOCABULARY_PATH):\n",
    "    # If file already exists read from it\n",
    "    with open(VOCABULARY_PATH, \"r\") as f:\n",
    "        vocabulary = json.load(f)\n",
    "else:\n",
    "    # Generate the vocabulary from all captions we have\n",
    "    \n",
    "    vocabulary = set(SPECIAL_TOKENS.values())\n",
    "\n",
    "    with open(CAPTIONS_PATH, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for image_name in data:\n",
    "            for caption in data[image_name]:\n",
    "                vocabulary.update(tokenizer(caption))\n",
    "                \n",
    "    vocabulary = list(vocabulary)\n",
    "\n",
    "    # Write the vocabulary to a file\n",
    "    with open(VOCABULARY_PATH, \"w\") as f:\n",
    "        json.dump(vocabulary, f)\n",
    "        \n",
    "print('Vocabulary size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings from the GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding matrix for the vocabulary\n",
    "def build_matrix(vocabulary):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the memes vocabulary from the GloVe embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load GloVe Embeddings\n",
    "    vector_embeddings = GloVe(name=\"twitter.27B\", dim=EMBEDDING_DIMENSIONS)    \n",
    "\n",
    "    # Initialize the embedding matrix with random tensors taken from a uniform distribution\n",
    "    # with provided mean and std\n",
    "    emb_mean, emb_std = -0.0033470048, 0.109855264\n",
    "\n",
    "    # torch.normal--> Returns a tensor of random numbers drawn from separate normal distributions\n",
    "    # whose mean and standard deviation are given.\n",
    "    matrix = torch.normal(mean=emb_mean, std=emb_std, size=(len(vocabulary), EMBEDDING_DIMENSIONS))\n",
    "\n",
    "    print(\"Generating embeddings\")\n",
    "    \n",
    "    # The for loop will update the embedding matrix for the tokens found in the GloVe vocabulary with\n",
    "    # the corresponding embedding vectors in GloVe\n",
    "    for i, word in tqdm(list(enumerate(vocabulary))):\n",
    "        # Look up embedding vectors of tokens. Our tokens are already in the lowercase\n",
    "        # so we do not require to use parameter 'lower_case_backup'\n",
    "        matrix[i] = vector_embeddings.get_vecs_by_tokens(word)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.embedding is a sparse layer. It is a lookup table that stores embeddings of a fixed dictionary and size.\n",
    "# This module is often used to store word embeddings and retrieve them using indices.\n",
    "embedding = nn.Embedding(len(vocabulary), EMBEDDING_DIMENSIONS)\n",
    "\n",
    "# We do not want to train the embeddings (as we are using the GloVe embeddings)\n",
    "embedding.weight.requires_grad = False\n",
    "\n",
    "# stoi is a dictionary that returns the index of a word in the embeddings vocabulary\n",
    "embedding.stoi = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "#itos is a dictionary that returns the word given the index\n",
    "embedding.itos = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "if os.path.exists(VECTORS_PATH):\n",
    "    embedding.weight.data = torch.load(VECTORS_PATH)\n",
    "else:\n",
    "    # Build the embedding matrix\n",
    "    embedding.weight.data = build_matrix(vocabulary)\n",
    "    # Save the embeddings of our captions to file\n",
    "    torch.save(embedding.weight.data, VECTORS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "Create a Dataset and the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE=15 # Came up with this batch size considering the model architecture and the GPU memory\n",
    "\n",
    "# This variable is created when required to train the model on few images\n",
    "# to check if the code is working\n",
    "IMAGES_LIMIT = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, images_path, captions_path, image_names, embedding):\n",
    "        \"\"\"\n",
    "        Initializes a MemeDataset object.\n",
    "\n",
    "        Args:\n",
    "            images_path (str): The path to the directory containing the images.\n",
    "            captions_path (str): The path to the file containing the captions.\n",
    "            image_names (list): A list of image names to load.\n",
    "            embedding : To read the indices of the captions vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.captions_path = captions_path\n",
    "        self.image_names = image_names\n",
    "        self.images = {}\n",
    "        self.captions = []\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "\n",
    "        # Check if the images directory exist\n",
    "        assert os.path.exists(\n",
    "            self.images_path\n",
    "        ), f\"Images directory {self.images_path} is not found\"\n",
    "\n",
    "        # Check if the prepared captions (after preprocessing) file exist\n",
    "        assert os.path.exists(\n",
    "            self.captions_path\n",
    "        ), f\"Captions file {self.captions_path} is not found\"\n",
    "\n",
    "        # load the meme images\n",
    "        for image_name in self.image_names:\n",
    "            image_path = os.path.join(self.images_path, image_name)\n",
    "            assert os.path.exists(image_path), f\"Image file {image_path} is not found\"\n",
    "            # Open the image and apply pre-processing\n",
    "            self.images[image_name] = self._preprocess_image(Image.open(image_path))\n",
    "\n",
    "        # load the memes\n",
    "        with open(self.captions_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for image_name in self.image_names:\n",
    "                for caption in data[image_name]:\n",
    "                    # Store a tuple of image name and caption (in form of tokens)\n",
    "                    self.captions.append((image_name, self._preprocess_text(caption)))\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # tokenize\n",
    "        tokens = tokenizer(text)\n",
    "\n",
    "        # replace with `UNK`\n",
    "        tokens = [\n",
    "            tok if tok in self.embedding.stoi else SPECIAL_TOKENS[\"UNK\"] for tok in tokens\n",
    "        ]\n",
    "\n",
    "        # Truncate the caption if length > MAX_CAPTION_LENGTH\n",
    "        if len(tokens) > MAX_CAPTION_LENGTH:\n",
    "            tokens = tokens[:MAX_CAPTION_LENGTH]\n",
    "\n",
    "        # add `EOS`\n",
    "        tokens += [SPECIAL_TOKENS[\"EOS\"]]\n",
    "\n",
    "        # convert to ids\n",
    "        tokens = [self.embedding.stoi[token] for token in tokens]\n",
    "\n",
    "        # casts the tensor to a torch.int64 data type to ensure int64 data type\n",
    "        return torch.tensor(tokens).long() \n",
    "\n",
    "    def _preprocess_image(self, image):\n",
    "        # Ref:- https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), # this will convert the image to (C x H x W)\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        return preprocess(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the name of the image and it's associated meme\n",
    "        image_name, caption = self.captions[idx]\n",
    "        # Get the preprocessed image\n",
    "        image = self.images[image_name]\n",
    "        \n",
    "        return image_name, image, caption, torch.tensor(caption.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 282838\n",
      "Test dataset size: 70190\n"
     ]
    }
   ],
   "source": [
    "with open(CAPTIONS_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    # all_images will have the name of all the images\n",
    "    all_images = list(data.keys())\n",
    "\n",
    "# Limit the images to shorten dataset\n",
    "if IMAGES_LIMIT != -1:\n",
    "    all_images = all_images[:IMAGES_LIMIT]\n",
    "\n",
    "train_images, test_images = train_test_split(all_images, test_size=TEST_SIZE,random_state=42)\n",
    "\n",
    "# Create train & test datasets\n",
    "train_dataset = MemeDataset(\n",
    "    images_path=IMAGES_DIR, \n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    image_names=train_images,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "test_dataset = MemeDataset(\n",
    "    images_path=IMAGES_DIR,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    image_names=test_images,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "# Print the size of the datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([15, 3, 224, 224]), pinned: True, device: cpu\n",
      "Captions shape: torch.Size([15, 17]), pinned: True, device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Memory pinning for faster loading to GPU https://pytorch.org/docs/stable/data.html#memory-pinning\n",
    "\n",
    "# As the batch preparation is custom and to enable memory pinning for custom batch, define a pin_memory() method on the custom type(s).\n",
    "class MemeDatasetBatch():\n",
    "    def __init__(self, image_names, images, captions, caption_lengths):\n",
    "        self.image_names = image_names\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.caption_lengths = caption_lengths\n",
    "    \n",
    "    def pin_memory(self):\n",
    "        self.images = self.images.pin_memory()\n",
    "        self.captions = self.captions.pin_memory()\n",
    "        return self\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"Batch collate with padding for Dataloader.\"\"\"\n",
    "    # unpack batch - zip returns an iterator of tuples, where length of tuple would be the batch size     \n",
    "    image_names, images, captions, caption_lengths = zip(*batch)           \n",
    "\n",
    "    # torch.stack : This will convert the tuple of tensors to a tensor\n",
    "    images = torch.stack(images, dim=0)    \n",
    "\n",
    "    caption_lengths = torch.stack(caption_lengths, dim=0)    \n",
    "\n",
    "    # pad captions (Pad a list of variable length Tensors with padding_value) --> will convert tuple of tensors to a tensor\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]])    \n",
    "\n",
    "    return MemeDatasetBatch(image_names, images, captions, caption_lengths)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate,\n",
    "    # Host to GPU copies are much faster when they originate from pinned (page-locked) memory\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device.type,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate,\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device.type\n",
    ")\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "print(f'Images shape: {sample.images.shape}, pinned: {sample.images.is_pinned()}, device: {sample.images.device}')\n",
    "print(f'Captions shape: {sample.captions.shape}, pinned: {sample.captions.is_pinned()}, device: {sample.captions.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'young', 'is', 'only', 'a', 'name', 'in', 'china', '<s>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>']\n"
     ]
    }
   ],
   "source": [
    "# Check one of the caption prepared by the dataset in the batch\n",
    "tokens = [embedding.itos[index.item()] for index in sample.captions[6]]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder\n",
    "\n",
    "This is the image encoding layer. The output of this layer must be same as the caption embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5087, -0.2634, -0.0874,  ...,  0.0981,  0.6670, -0.4541],\n",
       "        [ 0.3786, -0.4217,  0.2891,  ...,  0.3653,  0.3646,  0.1346],\n",
       "        [-0.4632,  0.0996,  0.2649,  ..., -0.1964,  0.1326, -0.3502],\n",
       "        ...,\n",
       "        [ 0.5647, -0.9807,  0.1648,  ...,  0.7679,  1.3639,  0.3783],\n",
       "        [ 0.3423, -0.1479,  0.1648,  ...,  0.6693,  0.7633, -0.0663],\n",
       "        [ 0.8684,  0.4170,  0.2787,  ..., -0.3431, -0.4439,  0.2624]],\n",
       "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, output_size=256, dropout=0.2):\n",
    "        \"\"\"Initializes ImageEncoder.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): dimensions of the output embedding\n",
    "            dropout (float): dropout for the encoded features\n",
    "        \"\"\"\n",
    "\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Use the pre-trained weights of the ResNet50 model\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Set the parameters of the ResNet50 model to not require gradients\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.resnet.fc.out_features, out_features=output_size)\n",
    "\n",
    "        # Batch Normalization (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
    "        self.batch_normalization = nn.BatchNorm1d(output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def train(self, mode):\n",
    "        super().train(mode)\n",
    "\n",
    "        # Keep resnet always in eval mode\n",
    "        self.resnet.eval()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Encodes input images into a global image embedding.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): input images of shape `[batch size, channel, width, height]`\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: global image embedding of shape `[batch size, emb_dim]`\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.resnet(images)\n",
    "\n",
    "        img_embedding = self.dropout(self.batch_normalization(self.linear(features)))\n",
    "\n",
    "        return img_embedding\n",
    "\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "image_encoder.eval()\n",
    "image_encoder.to(device)\n",
    "# Evaluating the image encoding on the sample created\n",
    "image_encoder(sample.images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  9,  9,  7,  9,  9,  9, 11,  7, 10,  8,  5,  9, 13, 17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.caption_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder\n",
    "\n",
    "This is the LSTM Decoder with the Beam Search Helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchHelper:\n",
    "    \"\"\"Helper class with common functions for beam search sampling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature,\n",
    "        beam_width,\n",
    "        top_k,\n",
    "        unk_index,\n",
    "        eos_index,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        assert beam_width <= top_k, \"`beam_width` should be less than `top_k`\"\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.beam_width = beam_width\n",
    "        self.top_k = top_k\n",
    "        self.unk_index = unk_index\n",
    "        self.eos_index = eos_index\n",
    "        self.device = device\n",
    "        self._build_has_ended_variables()\n",
    "\n",
    "    def _build_has_ended_variables(self):\n",
    "        \"\"\"\n",
    "        Returns flags and masks for monitoring if generation has ended.\n",
    "        \"\"\"\n",
    "        # flags showing if text sequence has ended --> Create a tensor of size beam_width with all values as False\n",
    "        self.has_ended = torch.tensor([False] * self.beam_width).to(self.device)\n",
    "\n",
    "        # masks for filtering out predictions for ended/not_ended sequences\n",
    "        self._n_copies_has_ended = torch.tensor([[self.beam_width], [1]]).to(self.device)\n",
    "        self._mask_has_ended = torch.stack(\n",
    "            [\n",
    "                torch.tensor([True] * self.beam_width),\n",
    "                torch.tensor([True] + [False] * (self.beam_width - 1)),\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).to(self.device)\n",
    "\n",
    "    def filter_top_k(self, logits):\n",
    "        \"\"\"Filters `top_k` logit values by zeroing out others.\"\"\"\n",
    "        \n",
    "        # torch.topk will return the tuple of values and indices for top k elements of logits.\n",
    "        # We want the smallest value out of the top k elements. Use this value to compare\n",
    "        # with the values of output tokens, and put True at the indexes where it is smaller    \n",
    "        filter_ind = logits < torch.topk(logits, self.top_k, dim=-1).values[:, -1].unsqueeze(-1)\n",
    "        # Also put the True value at the index of UNK token, as we do not want to have UNK\n",
    "        # token in our output\n",
    "        filter_ind[:, self.unk_index] = True  \n",
    "        # Put -inf at the indexes where the filter_ind is True\n",
    "        logits[filter_ind] = float(\"-inf\")        \n",
    "        return logits\n",
    "\n",
    "    def sample_k_indices(self, logits, k=None):\n",
    "        \"\"\"Samples `beam_width` indices for each sequence in the batch.\"\"\"        \n",
    "        \n",
    "        # compute probabilities\n",
    "        p_next = torch.softmax(logits / self.temperature, dim=-1)       \n",
    "        \n",
    "        # if the value of k is None, then take the beam_width value as k\n",
    "        k = self.beam_width if k is None else k\n",
    "        # Get the indices sampled from the multinomial probability distribution\n",
    "        # located in the corresponding row of tensor input\n",
    "        sample_ind = torch.multinomial(input=p_next, num_samples=k, replacement=False)\n",
    "\n",
    "        return sample_ind\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_by_indices(values, indices):\n",
    "        # Collect the logits values corresponding to the indices\n",
    "        sample_val = torch.gather(input=values, dim=1, index=indices)\n",
    "        return sample_val\n",
    "\n",
    "    def process_logits(self, logits, sample_seq, sample_val):\n",
    "        \"\"\"Main logic of beam search sampling step.\n",
    "\n",
    "        Steps:\n",
    "            - filter `top_k` logit scores\n",
    "            - filter out predictions for already ended sequences\n",
    "            - check if new predictions end sequence\n",
    "            - update `has_ended` indices\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): logit predictions, outputs of the classifier layer\n",
    "            sample_seq (torch.Tensor): `beam_width` sequences from the previous sampling step\n",
    "            sample_val (torch.Tensor): scores for the sequences from the previous sampling step\n",
    "\n",
    "        Returns:\n",
    "            (prev_seqs, prev_vals), (new_ind, new_val):\n",
    "                expanded sequences and their scores from the previous sampling step\n",
    "                + new candidate predictions and their scores\n",
    "        \"\"\"\n",
    "        \n",
    "        # filter `top_k` values\n",
    "        logits = self.filter_top_k(logits)\n",
    "\n",
    "        # sample `beam` sequences for each branch\n",
    "        # get the top 10 indexes of the highest probability token \n",
    "        new_ind = self.sample_k_indices(logits, k=self.beam_width)\n",
    "        new_val = self.filter_by_indices(logits, new_ind).log_softmax(-1)\n",
    "        \n",
    "        new_ind, new_val = new_ind.flatten(), new_val.flatten()        \n",
    "\n",
    "        # numbers of repeat_interleave copies (if ended, only a single copy)\n",
    "        n_copies = self._n_copies_has_ended[self.has_ended.long(), :].flatten()       \n",
    "\n",
    "        # mask for unique rows\n",
    "        unique_rows = self._mask_has_ended[self.has_ended.long(), :].flatten()\n",
    "        \n",
    "        # filter values\n",
    "        new_ind = new_ind[unique_rows]\n",
    "        new_val = new_val[unique_rows]\n",
    "\n",
    "        # check if the sequences already ended\n",
    "        # (no need to predict and evaluate new scores)\n",
    "        self.has_ended = torch.repeat_interleave(self.has_ended, n_copies, dim=0)\n",
    "        \n",
    "        new_ind[self.has_ended], new_val[self.has_ended] = 0, 0.0\n",
    "\n",
    "        # update `had_ended` based on new predictions\n",
    "        self.has_ended = self.has_ended | (new_ind == self.eos_index)\n",
    "\n",
    "        # repeat current sampled sequences\n",
    "        prev_seqs = torch.repeat_interleave(sample_seq, n_copies, dim=0)\n",
    "        prev_vals = torch.repeat_interleave(sample_val, n_copies, dim=0)\n",
    "\n",
    "        if len(prev_seqs.size()) == 1:\n",
    "            prev_seqs = prev_seqs.unsqueeze(0)\n",
    "            prev_vals = prev_vals.unsqueeze(0)\n",
    "\n",
    "        return (prev_seqs, prev_vals), (new_ind, new_val)\n",
    "\n",
    "    def all_ended(self):\n",
    "        \"\"\"Returns bool indicating if all sequences have ended.\"\"\"\n",
    "        return torch.all(self.has_ended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(0 if num_layers == 1 else dropout),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)        \n",
    "\n",
    "    def forward(self, image_embeddings, caption_embeddings, caption_lengths):\n",
    "        \"\"\"\n",
    "        This method will return the output of shape (batch_size, max_caption_length, output_size)\n",
    "        max_caption_length is the maximum length of the caption in the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # image embedding + caption embeddings (packed so that they can be sent to LSTM at once)\n",
    "        # Start with image embedding as the first time-step input\n",
    "        x = torch.cat((image_embeddings.unsqueeze(1), caption_embeddings), dim=1)\n",
    "\n",
    "        # Pack the captions embeddings to avoid training on the padded tokens\n",
    "        packed = pack_padded_sequence(\n",
    "            x, caption_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        outputs, _ = self.lstm(packed)\n",
    "        # Return the outputs with the padding\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)        \n",
    "\n",
    "        # mapping into token space\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    # The embedding here is the embedding instance because we are not keeping it in this class, but we need it for the generation\n",
    "    def generate(self, image_embedding, embedding, max_len, temperature, beam_width, top_k, eos_index):\n",
    "        # beam search sampling helper\n",
    "        helper = BeamSearchHelper(\n",
    "            temperature=temperature,\n",
    "            beam_width=beam_width,\n",
    "            top_k=top_k,\n",
    "            unk_index=embedding.stoi[SPECIAL_TOKENS[\"UNK\"]],\n",
    "            eos_index=eos_index,\n",
    "            device='cpu'\n",
    "        )\n",
    "\n",
    "        # run LSTM over the inputs and predict the next token\n",
    "        outputs, (h, c) = self.lstm(image_embedding)        \n",
    "        \n",
    "        # As the start to the generation of the tokens we only have the image embedding, so resulting\n",
    "        # logits would be of shape torch.Size([1, 84880])\n",
    "        logits = self.classifier(outputs[:, -1, :])       \n",
    "        \n",
    "        # repeat hidden state  and cell state `beam` times\n",
    "        # The hidden state and cell state will be of dimension (num_layers, batch_size, hidden_size)\n",
    "        # Repeat the hidden state for each layer to the beam_width times. \n",
    "        h, c = h.repeat((1, beam_width, 1)), c.repeat((1, beam_width, 1))        \n",
    "\n",
    "        # filter `top_k` values\n",
    "        logits = helper.filter_top_k(logits)       \n",
    "        \n",
    "        # compute probabilities and sample k values\n",
    "        sample_ind = helper.sample_k_indices(logits, k=beam_width)  \n",
    "        # Apply the log softmax to the filtered out values. The log softmax is applied to\n",
    "        # avoid the numerical underflow problem      \n",
    "        sample_val = helper.filter_by_indices(logits, sample_ind).log_softmax(-1)\n",
    "        \n",
    "        sample_ind, sample_val = sample_ind.T, sample_val.T\n",
    "       \n",
    "        # define total prediction sequences\n",
    "        sample_seq = sample_ind.clone().detach()\n",
    "\n",
    "        # reusable parameters\n",
    "        beam_copies = torch.tensor([beam_width] * beam_width).to(outputs.device)\n",
    "        \n",
    "        # update `has_ended` index\n",
    "        # contiguous is used to ensure resulting tensor is stored in contiguous block of memory,\n",
    "        # as for view to reshape it requires tensor to be stored in contiguously\n",
    "\n",
    "        helper.has_ended = (sample_ind == eos_index).contiguous().view(-1)    \n",
    "        # print(\"helper.has_ended\", helper.has_ended)    \n",
    "     \n",
    "        for i in range(sample_seq.size(1), max_len):\n",
    "            # predict the next time step\n",
    "            # Get the embedding of the predicted tokens from the previous time-step\n",
    "            inputs = embedding(sample_ind)\n",
    "            \n",
    "            # Everytime it will have 10 hidden states, as for each time step we are predicting 10 tokens\n",
    "            outputs, (h, c) = self.lstm(inputs, (h, c))\n",
    "            \n",
    "            # logits would be of shape torch.Size([10, 84880]) because we want to predict tokens = beam_width\n",
    "            logits = self.classifier(outputs[:, -1, :])            \n",
    "\n",
    "            (prev_seqs, prev_vals), (new_ind, new_val) = helper.process_logits(\n",
    "                logits, sample_seq, sample_val\n",
    "            )\n",
    "\n",
    "            # create candidate sequences and compute their probabilities\n",
    "            cand_seq = torch.cat((prev_seqs, new_ind.unsqueeze(0).T), -1)\n",
    "            cand_val = prev_vals.flatten() + new_val\n",
    "\n",
    "            # sample `beam` sequences\n",
    "            filter_ind = helper.sample_k_indices(cand_val, k=beam_width)\n",
    "\n",
    "            # update total sequences and their scores\n",
    "            sample_val = cand_val[filter_ind]\n",
    "            sample_seq = cand_seq[filter_ind]\n",
    "            sample_ind = sample_seq[:, -1].unsqueeze(-1)\n",
    "\n",
    "            # filter `has_ended` flags\n",
    "            helper.has_ended = helper.has_ended[filter_ind]\n",
    "\n",
    "            # check if every branch has ended\n",
    "            if helper.all_ended():\n",
    "                break\n",
    "\n",
    "            # repeat hidden state `beam` times and filter by sampled indices\n",
    "            h = torch.repeat_interleave(h, beam_copies, dim=1)\n",
    "            c = torch.repeat_interleave(c, beam_copies, dim=1)\n",
    "\n",
    "            h, c = h[:, filter_ind, :], c[:, filter_ind, :]\n",
    "           \n",
    "        # sample output sequence\n",
    "        ind = helper.sample_k_indices(sample_val, k=2)\n",
    "        output_seq = sample_seq[ind, :].squeeze()\n",
    "\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meme Generation LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeGenerationLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based image captioning model.\n",
    "\n",
    "    Encodes input images into a embeddings of size `emb_dim`\n",
    "    and passes them as the first token to the caption generation decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding, hidden_size, num_layers, encoder_dropout, decoder_dropout):\n",
    "        super(MemeGenerationLSTM, self).__init__()\n",
    "\n",
    "        # We need to keep them same for LSTM because the first\n",
    "        # input to the LSTM is the image embedding and rest is caption\n",
    "        # embedding\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_dropout = encoder_dropout\n",
    "        self.decoder_dropout = decoder_dropout\n",
    "\n",
    "        # Initialize the Encoder which is the Image Encoder (ResNet50 model)\n",
    "        self.encoder = ImageEncoder(\n",
    "            output_size=self.embedding.embedding_dim,# Embedding dimension of the Image which is 50 defined in variable EMBEDDING_DIMENSIONS\n",
    "            dropout=self.encoder_dropout,\n",
    "        )\n",
    "        \n",
    "        self.decoder = LSTMDecoder(\n",
    "            input_size=self.embedding.embedding_dim, # Embedding dimension of the Image which is 50 defined in variable EMBEDDING_DIMENSIONS\n",
    "            hidden_size=self.hidden_size,\n",
    "            output_size=self.embedding.num_embeddings,# Number of tokens in the vocabulary including the special tokens\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.decoder_dropout,\n",
    "        )\n",
    "\n",
    "        self.params = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'encoder_dropout': encoder_dropout,\n",
    "            'decoder_dropout': decoder_dropout,\n",
    "        }\n",
    "\n",
    "    def forward(self, images, captions, caption_lengths):\n",
    "        \n",
    "        # Retrieve Image Embeddings for the processed batch\n",
    "        image_embeddings = self.encoder(images)\n",
    "        # Retrieve Caption Embeddings for the processed batch\n",
    "        caption_embeddings = self.embedding(captions)\n",
    "        # Get the outputs from the decoder\n",
    "        output = self.decoder(image_embeddings, caption_embeddings, caption_lengths)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(self, image, max_len, temperature, beam_width, top_k):\n",
    "        \"\"\"Generates caption for an image.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): input image of shape `[1, width, height]`\n",
    "            caption (torch.Tensor, optional): beginning tokens of the caption of shape `[1, seq_len]`\n",
    "            max_len (int): maximum length of the caption\n",
    "            temperature (float): temperature for softmax over logits\n",
    "            beam_width (int): number of maintained branches at each step\n",
    "            top_k (int): number of the most probable tokens to consider during sampling\n",
    "      \n",
    "        Returns:\n",
    "            torch.Tensor: generated caption tokens of shape `[1, min(output_len, max_len)]`\n",
    "        \"\"\"\n",
    "\n",
    "        # get image embedding\n",
    "        image_embedding = self.encoder(image).unsqueeze(1)\n",
    "\n",
    "        sampled_ids = self.decoder.generate(\n",
    "            image_embedding,\n",
    "            embedding=self.embedding,\n",
    "            max_len=max_len,\n",
    "            temperature=temperature,\n",
    "            beam_width=beam_width,\n",
    "            top_k=top_k,\n",
    "            eos_index=self.embedding.stoi[SPECIAL_TOKENS[\"EOS\"]],\n",
    "        )\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "    def save(self, ckpt_path):\n",
    "        \"\"\"Saves the model's state and hyperparameters.\"\"\"\n",
    "        torch.save({\"model\": self.state_dict(), \"params\": self.params}, ckpt_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(ckpt_path):\n",
    "        \"\"\"Loads and builds the model from the checkpoint file.\"\"\"\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        params = ckpt[\"params\"]\n",
    "\n",
    "        model = MemeGenerationLSTM(\n",
    "            embedding=embedding,\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            encoder_dropout=params[\"encoder_dropout\"],\n",
    "            decoder_dropout=params[\"decoder_dropout\"],\n",
    "        )\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 17, 84880])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "ENCODER_DROPOUT = 0.2\n",
    "DECODER_DROPOUT = 0.1\n",
    "\n",
    "# Create model instance\n",
    "model = MemeGenerationLSTM(\n",
    "    embedding=embedding,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    encoder_dropout=ENCODER_DROPOUT,\n",
    "    decoder_dropout=DECODER_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# Just checking the output shape of the model on the sample data\n",
    "output = model.eval()(sample.images.to(device), sample.captions.to(device), sample.caption_lengths)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"A final class for running the models.\"\"\"\n",
    "\n",
    "    def __init__(self, experiment_title, log_dir=\"./logs1\", checkpoint_dir=\"./checkpoints\", phases=(\"train\", \"val\"), device=\"cpu\"):\n",
    "        self.experiment_data = self._setup_experiment(experiment_title, log_dir, checkpoint_dir)\n",
    "\n",
    "        self.phases = phases\n",
    "        self.device = device\n",
    "\n",
    "        self.writers = self._setup_writers()\n",
    "\n",
    "    @staticmethod\n",
    "    def _setup_experiment(title, log_dir, checkpoint_dir):\n",
    "        experiment_name = \"{}@{}\".format(title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "        experiment_dir = os.path.join(log_dir, experiment_name)\n",
    "        best_model_path = f\"{title}.best.pth\"\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        experiment_data = {\n",
    "            \"model_name\": title,\n",
    "            \"name\": experiment_name,\n",
    "            \"dir\": experiment_dir,\n",
    "            \"checkpoint_dir\": checkpoint_dir,\n",
    "            \"best_model_path\": best_model_path,\n",
    "            \"epochs\": 0,\n",
    "            \"iterations\": 0,\n",
    "        }\n",
    "\n",
    "        return experiment_data\n",
    "\n",
    "    def _setup_writers(self):\n",
    "        return {phase: SummaryWriter(log_dir=os.path.join(self.experiment_data[\"dir\"], phase)) for phase in self.phases}\n",
    "\n",
    "    def run_epoch(self, model, dataloader, optimizer, criterion_main, criterion_aux, phase=\"train\"):\n",
    "        # Boolean variable to check if the model is in training mode\n",
    "        is_train = phase == \"train\"\n",
    "\n",
    "        # Set the model to train mode if it is in training phase otherwise in evaluation mode\n",
    "        model.train() if is_train else model.eval()\n",
    "\n",
    "        epoch = self.experiment_data[\"epochs\"] # this epoch will be the epoch set in train_model method\n",
    "        iterations = self.experiment_data[\"iterations\"]\n",
    "        # Initialize the epoch loss and epoch perplexity to 0\n",
    "        epoch_loss, epoch_pp = 0.0, 0.0\n",
    "\n",
    "        # On Training we want to enable the gradients\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            tqdm_object = tqdm(dataloader, desc=f\"{phase} ({epoch})\")\n",
    "\n",
    "            for batch in tqdm_object: # Iterations Loop (number of iterations to complete one epoch over complete training/evaluation data)\n",
    "                # max_len <-- maximum length of the caption in the batch\n",
    "                batch_size, max_len = batch.captions.size()                \n",
    "                images = batch.images.to(self.device)\n",
    "                captions = batch.captions.to(self.device)\n",
    "                caption_lengths = batch.caption_lengths                \n",
    "\n",
    "                # Call the forward method of the MemeGenerationLSTM class\n",
    "                pred = model(images, captions, caption_lengths)                \n",
    "\n",
    "                # Trim if there are more than max_len tokens of the processed batch\n",
    "                pred = pred[:, :max_len, :]\n",
    "                \n",
    "                # mask will have value True for all the tokens that are not PAD tokens\n",
    "                mask = captions != embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]\n",
    "\n",
    "                # The calculation of both the CrossEntropyLoss and Perplexity requires the unnormlized probabilities\n",
    "                # of the predictions.\n",
    "                # Before applying the mask, the shape of the pred will be (batch_size, max_len, vocab_size).\n",
    "                # After applying mask, the shape of the pred will be (sum of number of non PAD tokens in the batch which can be\n",
    "                # calculated as sum of caption_lengths, vocab_size). In other words, the batch_size, max_len dimensions will be\n",
    "                # flattened to a single dimension.\n",
    "                # Cross Entropy Loss requires 2D data, which is the reason to apply mask\n",
    "                loss = criterion_main(pred[mask], captions[mask])\n",
    "                # The calculation of perplexity requires input to be of shape (batch_size, seq_len, vocab_size)\n",
    "                # and target to be of shape (batch_size, seq_len).\n",
    "                pp = criterion_aux.update(pred, captions).compute()\n",
    "\n",
    "                if is_train:\n",
    "                    # make optimization step\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if is_train:\n",
    "                    iterations += 1                \n",
    "                 \n",
    "                epoch_loss += loss.item() * batch_size # de-average the loss as later it is being averaged over the entire dataset\n",
    "                epoch_pp += pp.item() * batch_size # de-average the perplexity\n",
    "\n",
    "                # dump batch metrics to tensorboard\n",
    "                if self.writers is not None and phase in self.writers and is_train:\n",
    "                    self.writers[phase].add_scalar(f\"train/batch_loss\", loss.item(), iterations)\n",
    "                    self.writers[phase].add_scalar(f\"train/batch_perplexity\", pp.item(), iterations)\n",
    "\n",
    "                tqdm_object.set_postfix(loss=loss.item(), perplexity=pp.item())\n",
    "\n",
    "            epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "            epoch_pp = epoch_pp / len(dataloader.dataset)\n",
    "\n",
    "            # dump epoch metrics to tensorboard\n",
    "            if self.writers is not None and phase in self.writers:\n",
    "                self.writers[phase].add_scalar(f\"eval/loss\", epoch_loss, epoch)\n",
    "                self.writers[phase].add_scalar(f\"eval/perplexity\", epoch_pp, epoch)\n",
    "\n",
    "        if is_train:\n",
    "            self.experiment_data[\"iterations\"] = iterations\n",
    "\n",
    "        return epoch_loss, epoch_pp\n",
    "\n",
    "    def train_model(self, model, dataloaders, optimizer, criterion_main, criterion_aux, scheduler=None, n_epochs=10):\n",
    "        # Initialize the best epoch and best validation loss\n",
    "        best_epoch, best_val_loss = 0, float(\"+inf\")\n",
    "\n",
    "        # This would be useful to resume training from a checkpoint\n",
    "        past_epochs = self.experiment_data[\"epochs\"]       \n",
    "\n",
    "        if self.writers is None:\n",
    "            self._setup_writers()\n",
    "\n",
    "        for epoch in range(past_epochs + 1, past_epochs + n_epochs + 1): # Epoch Loop\n",
    "            self.experiment_data[\"epochs\"] = epoch # Set the current epoch in the experiment data\n",
    "            print(f\"Epoch {epoch:02d}/{past_epochs + n_epochs:02d}\")\n",
    "\n",
    "            st = time()\n",
    "            for phase in self.phases:                \n",
    "                epoch_loss, epoch_pp = self.run_epoch(\n",
    "                    model, dataloaders[phase], optimizer, criterion_main, criterion_aux, phase=phase\n",
    "                )\n",
    "\n",
    "                print(f\"  {phase:5s} loss: {epoch_loss:.5f}, perplexity: {epoch_pp:.3f}\")\n",
    "\n",
    "                # If during the validation, the training loss is less than the best validation loss,\n",
    "                # then save the model\n",
    "                if phase == \"val\" and epoch_loss < best_val_loss:\n",
    "                    best_epoch, best_val_loss = epoch, epoch_loss\n",
    "                    # Save the best model\n",
    "                    model.save(os.path.join(self.experiment_data['checkpoint_dir'], self.experiment_data[\"best_model_path\"]))\n",
    "\n",
    "                # Sae the model after every epoch\n",
    "                model.save(os.path.join(self.experiment_data['checkpoint_dir'], f\"{self.experiment_data['model_name']}.e{epoch}.pth\"))\n",
    "\n",
    "                if phase == \"val\" and scheduler is not None:\n",
    "                    # Learning rate scheduling should be applied after optimizerâ€™s update\n",
    "                    # https://discuss.pytorch.org/t/reduce-lr-on-plateau-based-on-training-loss-or-validation/183344\n",
    "                    scheduler.step(epoch_loss)\n",
    "\n",
    "            et = time() - st\n",
    "            print(f\"  epoch time: {et:.2f}s\")            \n",
    "\n",
    "        print(f\"Best val_loss: {best_val_loss} (epoch: {best_epoch})\")\n",
    "\n",
    "        self.experiment_data[\"epochs\"] = epoch # Set the last epoch value\n",
    "        \n",
    "\n",
    "        return self.experiment_data\n",
    "\n",
    "    def close(self):\n",
    "        for writer in self.writers.values():\n",
    "            writer.close()\n",
    "        self.writers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for training\n",
    "EPOCHS = 10\n",
    "LOGS_DIR = \"./logs_resnet\"\n",
    "CHECKPOINTS_DIR = \"./checkpoints_resnet\"\n",
    "TITLE = 'MemeGenerationResnet'\n",
    "LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [17:35<00:00, 17.86it/s, loss=5.34, perplexity=335]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 5.82090, perplexity: 706.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:59<00:00, 26.11it/s, loss=5.08, perplexity=315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.54355, perplexity: 325.855\n",
      "  epoch time: 1235.93s\n",
      "Epoch 02/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [17:33<00:00, 17.89it/s, loss=4.9, perplexity=245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 5.19330, perplexity: 272.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:59<00:00, 26.02it/s, loss=5.05, perplexity=242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.41542, perplexity: 243.782\n",
      "  epoch time: 1235.43s\n",
      "Epoch 03/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:54<00:00, 18.59it/s, loss=5.11, perplexity=209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.98361, perplexity: 222.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:47<00:00, 27.90it/s, loss=5.03, perplexity=209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.37896, perplexity: 209.113\n",
      "  epoch time: 1183.44s\n",
      "Epoch 04/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:21<00:00, 19.21it/s, loss=4.58, perplexity=188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.85381, perplexity: 196.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:48<00:00, 27.85it/s, loss=5.05, perplexity=189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.37812, perplexity: 188.753\n",
      "  epoch time: 1151.07s\n",
      "Epoch 05/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:21<00:00, 19.22it/s, loss=4.33, perplexity=174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.76106, perplexity: 180.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:47<00:00, 27.92it/s, loss=5.06, perplexity=175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.38482, perplexity: 174.966\n",
      "  epoch time: 1149.60s\n",
      "Epoch 06/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (6): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:21<00:00, 19.22it/s, loss=4.61, perplexity=164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.69243, perplexity: 169.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (6): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:47<00:00, 27.89it/s, loss=5.18, perplexity=165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.38389, perplexity: 164.877\n",
      "  epoch time: 1149.81s\n",
      "Epoch 07/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:24<00:00, 19.15it/s, loss=5.89, perplexity=156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.61404, perplexity: 160.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:48<00:00, 27.81it/s, loss=5.16, perplexity=157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.40089, perplexity: 156.668\n",
      "  epoch time: 1153.87s\n",
      "Epoch 08/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:22<00:00, 19.20it/s, loss=4.72, perplexity=150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.57669, perplexity: 152.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:48<00:00, 27.84it/s, loss=5.09, perplexity=151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.41372, perplexity: 150.291\n",
      "  epoch time: 1151.05s\n",
      "Epoch 09/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (9): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:22<00:00, 19.19it/s, loss=4.41, perplexity=144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.52016, perplexity: 147.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (9): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:48<00:00, 27.85it/s, loss=5.27, perplexity=145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.42510, perplexity: 144.833\n",
      "  epoch time: 1151.40s\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18856/18856 [16:23<00:00, 19.17it/s, loss=4.26, perplexity=140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.49284, perplexity: 142.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [02:47<00:00, 27.90it/s, loss=5.14, perplexity=141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.43464, perplexity: 140.332\n",
      "  epoch time: 1152.26s\n",
      "Best val_loss: 5.378121027589473 (epoch: 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'MemeGenerationResnet',\n",
       " 'name': 'MemeGenerationResnet@13.11.2023-21:58:11',\n",
       " 'dir': './logs_resnet/MemeGenerationResnet@13.11.2023-21:58:11',\n",
       " 'checkpoint_dir': './checkpoints_resnet',\n",
       " 'best_model_path': 'MemeGenerationResnet.best.pth',\n",
       " 'epochs': 10,\n",
       " 'iterations': 188560}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(TITLE, log_dir=LOGS_DIR, checkpoint_dir=CHECKPOINTS_DIR, device=device)\n",
    "\n",
    "# Create model instance\n",
    "model = MemeGenerationLSTM(\n",
    "    embedding=embedding,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    encoder_dropout=ENCODER_DROPOUT,\n",
    "    decoder_dropout=DECODER_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# Training helpers\n",
    "criterion_main = nn.CrossEntropyLoss(ignore_index=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]).to(device)\n",
    "criterion_aux = Perplexity(ignore_index=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=1, factor=0.8\n",
    "    )\n",
    "\n",
    "trainer.train_model(\n",
    "    model=model,\n",
    "    dataloaders={\"train\": train_dataloader, \"val\": test_dataloader},\n",
    "    optimizer=optimizer,\n",
    "    criterion_main=criterion_main,\n",
    "    criterion_aux=criterion_aux,\n",
    "    n_epochs=EPOCHS,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best checkpoint\n",
    "best_model = MemeGenerationLSTM.from_pretrained(os.path.join(CHECKPOINTS_DIR, TITLE + '.best.pth')).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BLEU Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the validation data\n",
    "validation_data = {}\n",
    "for batch in test_dataloader:\n",
    "    for i, image_name in enumerate(batch.image_names):\n",
    "        if image_name not in validation_data:\n",
    "            validation_data[image_name] = {\n",
    "                'image_tensor': batch.images[i],\n",
    "                'captions': []\n",
    "            }\n",
    "        caption = [embedding.itos[index.item()] for index in batch.captions[i] if index.item() != embedding.stoi[SPECIAL_TOKENS[\"PAD\"]] and index.item() != embedding.stoi[SPECIAL_TOKENS['EOS']]]\n",
    "        validation_data[image_name]['captions'].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sum = 0 # Initialize the score\n",
    "for image_name, data in validation_data.items():\n",
    "\n",
    "    candidate_corpus = []\n",
    "\n",
    "    processed_image = data['image_tensor']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = best_model.cpu().generate(\n",
    "                    processed_image.unsqueeze(0).cpu(), max_len=25, temperature=1.0, beam_width=10, top_k=10\n",
    "        )\n",
    "        for entry in output:            \n",
    "            # Convert the predicted output to a list of words\n",
    "            generated_caption = [\n",
    "                embedding.itos[index.item()]\n",
    "                for index in entry\n",
    "                if index.item() != 0 and index.item() != embedding.stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
    "            ]\n",
    "            candidate_corpus.append(generated_caption)\n",
    "        \n",
    "        # Get the references corpus for the processed_image\n",
    "        reference_corpus = data['captions']\n",
    "        \n",
    "        # As the top 2 sentences are generates, we will pass the reference corpus twice\n",
    "        score = bleu_score(candidate_corpus, [reference_corpus] * 2, max_n=3, weights=[0.6, 0.25, 0.15])\n",
    "        score_sum += score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score on 100 test images is 0.317\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average BLEU score on 100 test images is {score_sum/len(validation_data.keys()):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvmeme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
