{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from bert_score import score\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import Perplexity\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.vocab import GloVe\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import class Cider_Scorer from pycocoevalcap/cider/cider_scorer.py file\n",
    "from pycocoevalcap.cider.cider_scorer import CiderScorer\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from BARTScore.bart_score import BARTScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print the values in bold or in different colors\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for training\n",
    "cwd = os.getcwd()\n",
    "DATA_DIR = cwd + '/data-processed'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'memes')\n",
    "CAPTIONS_PATH = os.path.join(DATA_DIR, 'cleaned_english_captions_v1.json')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "This is where we define the text embedding model to use. Here we are building our embedding model from the approach provided here\n",
    "\n",
    "https://www.kaggle.com/code/ratthachat/flickr-image-captioning-tpu-tf2-glove?scriptVersionId=34452283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens for the caption (they exist in the GloVe vocabulary)\n",
    "SPECIAL_TOKENS = {\n",
    "    'UNK': '<u>',\n",
    "    'PAD': '<p>',\n",
    "    'EOS': '<s>',\n",
    "}\n",
    "EMBEDDING_DIMENSIONS = 50\n",
    "VOCABULARY_PATH = os.path.join(DATA_DIR, 'vocabulary.json')\n",
    "VECTORS_PATH = os.path.join(DATA_DIR, f'vectors{EMBEDDING_DIMENSIONS}.pt')\n",
    "MAX_CAPTION_LENGTH = 50 # Maximum number of words in a caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 84880\n"
     ]
    }
   ],
   "source": [
    "# Generate the vocabulary from all captions we have\n",
    "\n",
    "if os.path.exists(VOCABULARY_PATH):\n",
    "    # If file already exists read from it\n",
    "    with open(VOCABULARY_PATH, \"r\") as f:\n",
    "        vocabulary = json.load(f)\n",
    "else:\n",
    "    # Generate the vocabulary from all captions we have\n",
    "    \n",
    "    vocabulary = set(SPECIAL_TOKENS.values())\n",
    "\n",
    "    with open(CAPTIONS_PATH, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for image_name in data:\n",
    "            for caption in data[image_name]:\n",
    "                vocabulary.update(tokenizer(caption))\n",
    "                \n",
    "    vocabulary = list(vocabulary)\n",
    "\n",
    "    # Write the vocabulary to a file\n",
    "    with open(VOCABULARY_PATH, \"w\") as f:\n",
    "        json.dump(vocabulary, f)\n",
    "        \n",
    "print('Vocabulary size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings from the GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding matrix for the vocabulary\n",
    "def build_matrix(vocabulary):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the memes vocabulary from the GloVe embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load GloVe Embeddings\n",
    "    vector_embeddings = GloVe(name=\"twitter.27B\", dim=EMBEDDING_DIMENSIONS)    \n",
    "\n",
    "    # Initialize the embedding matrix with random tensors taken from a uniform distribution\n",
    "    # with provided mean and std\n",
    "    emb_mean, emb_std = -0.0033470048, 0.109855264\n",
    "\n",
    "    # torch.normal--> Returns a tensor of random numbers drawn from separate normal distributions\n",
    "    # whose mean and standard deviation are given.\n",
    "    matrix = torch.normal(mean=emb_mean, std=emb_std, size=(len(vocabulary), EMBEDDING_DIMENSIONS))\n",
    "\n",
    "    print(\"Generating embeddings\")\n",
    "    \n",
    "    # The for loop will update the embedding matrix for the tokens found in the GloVe vocabulary with\n",
    "    # the corresponding embedding vectors in GloVe\n",
    "    for i, word in tqdm(list(enumerate(vocabulary))):\n",
    "        # Look up embedding vectors of tokens. Our tokens are already in the lowercase\n",
    "        # so we do not require to use parameter 'lower_case_backup'\n",
    "        matrix[i] = vector_embeddings.get_vecs_by_tokens(word)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.embedding is a sparse layer. It is a lookup table that stores embeddings of a fixed dictionary and size.\n",
    "# This module is often used to store word embeddings and retrieve them using indices.\n",
    "embedding = nn.Embedding(len(vocabulary), EMBEDDING_DIMENSIONS)\n",
    "\n",
    "# We do not want to train the embeddings (as we are using the GloVe embeddings)\n",
    "embedding.weight.requires_grad = False\n",
    "\n",
    "# stoi is a dictionary that returns the index of a word in the embeddings vocabulary\n",
    "embedding.stoi = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "#itos is a dictionary that returns the word given the index\n",
    "embedding.itos = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "if os.path.exists(VECTORS_PATH):\n",
    "    embedding.weight.data = torch.load(VECTORS_PATH)\n",
    "else:\n",
    "    # Build the embedding matrix\n",
    "    embedding.weight.data = build_matrix(vocabulary)\n",
    "    # Save the embeddings of our captions to file\n",
    "    torch.save(embedding.weight.data, VECTORS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "Create a Dataset and the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE=15 # Came up with this batch size considering the model architecture and the GPU memory\n",
    "\n",
    "# This variable is created when required to train the model on few images\n",
    "# to check if the code is working\n",
    "IMAGES_LIMIT = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, images_path, captions_path, image_names, embedding):\n",
    "        \"\"\"\n",
    "        Initializes a MemeDataset object.\n",
    "\n",
    "        Args:\n",
    "            images_path (str): The path to the directory containing the images.\n",
    "            captions_path (str): The path to the file containing the captions.\n",
    "            image_names (list): A list of image names to load.\n",
    "            embedding : To read the indices of the captions vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.captions_path = captions_path\n",
    "        self.image_names = image_names\n",
    "        self.images = {}\n",
    "        self.captions = []\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "\n",
    "        # Check if the images directory exist\n",
    "        assert os.path.exists(\n",
    "            self.images_path\n",
    "        ), f\"Images directory {self.images_path} is not found\"\n",
    "\n",
    "        # Check if the prepared captions (after preprocessing) file exist\n",
    "        assert os.path.exists(\n",
    "            self.captions_path\n",
    "        ), f\"Captions file {self.captions_path} is not found\"\n",
    "\n",
    "        # load the meme images\n",
    "        for image_name in self.image_names:\n",
    "            image_path = os.path.join(self.images_path, image_name)\n",
    "            assert os.path.exists(image_path), f\"Image file {image_path} is not found\"\n",
    "            # Open the image and apply pre-processing\n",
    "            self.images[image_name] = self._preprocess_image(Image.open(image_path))\n",
    "\n",
    "        # load the memes\n",
    "        with open(self.captions_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for image_name in self.image_names:\n",
    "                for caption in data[image_name]:\n",
    "                    # Store a tuple of image name and caption (in form of tokens)\n",
    "                    self.captions.append((image_name, self._preprocess_text(caption)))\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # tokenize\n",
    "        tokens = tokenizer(text)\n",
    "\n",
    "        # replace with `UNK`\n",
    "        tokens = [\n",
    "            tok if tok in self.embedding.stoi else SPECIAL_TOKENS[\"UNK\"] for tok in tokens\n",
    "        ]\n",
    "\n",
    "        # Truncate the caption if length > MAX_CAPTION_LENGTH\n",
    "        if len(tokens) > MAX_CAPTION_LENGTH:\n",
    "            tokens = tokens[:MAX_CAPTION_LENGTH]\n",
    "\n",
    "        # add `EOS`\n",
    "        tokens += [SPECIAL_TOKENS[\"EOS\"]]\n",
    "\n",
    "        # convert to ids\n",
    "        tokens = [self.embedding.stoi[token] for token in tokens]\n",
    "\n",
    "        # casts the tensor to a torch.int64 data type to ensure int64 data type\n",
    "        return torch.tensor(tokens).long() \n",
    "\n",
    "    def _preprocess_image(self, image):\n",
    "        # Ref:- https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), # this will convert the image to (C x H x W)\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        return preprocess(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the name of the image and it's associated meme\n",
    "        image_name, caption = self.captions[idx]\n",
    "        # Get the preprocessed image\n",
    "        image = self.images[image_name]\n",
    "        \n",
    "        return image_name, image, caption, torch.tensor(caption.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 282838\n",
      "Test dataset size: 70190\n"
     ]
    }
   ],
   "source": [
    "with open(CAPTIONS_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    # all_images will have the name of all the images\n",
    "    all_images = list(data.keys())\n",
    "\n",
    "# Limit the images to shorten dataset\n",
    "if IMAGES_LIMIT != -1:\n",
    "    all_images = all_images[:IMAGES_LIMIT]\n",
    "\n",
    "train_images, test_images = train_test_split(all_images, test_size=TEST_SIZE,random_state=42)\n",
    "\n",
    "# Create train & test datasets\n",
    "train_dataset = MemeDataset(\n",
    "    images_path=IMAGES_DIR, \n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    image_names=train_images,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "test_dataset = MemeDataset(\n",
    "    images_path=IMAGES_DIR,\n",
    "    captions_path=CAPTIONS_PATH,\n",
    "    image_names=test_images,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "# Print the size of the datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([15, 3, 224, 224]), pinned: True, device: cpu\n",
      "Captions shape: torch.Size([15, 49]), pinned: True, device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Memory pinning for faster loading to GPU https://pytorch.org/docs/stable/data.html#memory-pinning\n",
    "\n",
    "# As the batch preparation is custom and to enable memory pinning for custom batch, define a pin_memory() method on the custom type(s).\n",
    "class MemeDatasetBatch():\n",
    "    def __init__(self, image_names, images, captions, caption_lengths):\n",
    "        self.image_names = image_names\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.caption_lengths = caption_lengths\n",
    "    \n",
    "    def pin_memory(self):\n",
    "        self.images = self.images.pin_memory()\n",
    "        self.captions = self.captions.pin_memory()\n",
    "        return self\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"Batch collate with padding for Dataloader.\"\"\"\n",
    "    # unpack batch - zip returns an iterator of tuples, where length of tuple would be the batch size     \n",
    "    image_names, images, captions, caption_lengths = zip(*batch)           \n",
    "\n",
    "    # torch.stack : This will convert the tuple of tensors to a tensor\n",
    "    images = torch.stack(images, dim=0)    \n",
    "\n",
    "    caption_lengths = torch.stack(caption_lengths, dim=0)    \n",
    "\n",
    "    # pad captions (Pad a list of variable length Tensors with padding_value) --> will convert tuple of tensors to a tensor\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]])    \n",
    "\n",
    "    return MemeDatasetBatch(image_names, images, captions, caption_lengths)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate,\n",
    "    # Host to GPU copies are much faster when they originate from pinned (page-locked) memory\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device.type,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate,\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=device.type\n",
    ")\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "print(f'Images shape: {sample.images.shape}, pinned: {sample.images.is_pinned()}, device: {sample.images.device}')\n",
    "print(f'Captions shape: {sample.captions.shape}, pinned: {sample.captions.is_pinned()}, device: {sample.captions.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'i', 'have', 'a', 'g', 'f', '?', 'of', 'course', 'me', 'and', 'shanon', 'are', 'stil', 'together', '<s>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>']\n"
     ]
    }
   ],
   "source": [
    "# Check one of the caption prepared by the dataset in the batch\n",
    "tokens = [embedding.itos[index.item()] for index in sample.captions[6]]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder\n",
    "\n",
    "This is the image encoding layer. The output of this layer must be same as the caption embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6331,  0.7783,  0.0976,  ...,  0.5142,  0.4120,  0.4581],\n",
       "        [ 0.1994,  0.5115, -0.5783,  ...,  0.0594,  0.0959, -0.2089],\n",
       "        [-0.1293,  0.4814, -0.4774,  ...,  0.4472,  0.2338,  0.1581],\n",
       "        ...,\n",
       "        [-0.0510, -0.2043, -0.7082,  ..., -0.4042, -0.2921, -0.0511],\n",
       "        [-0.6453, -0.5168, -0.8420,  ..., -0.5380, -0.0243,  0.0921],\n",
       "        [ 0.6059,  0.4157, -0.8140,  ...,  0.1826,  0.4312,  0.6405]],\n",
       "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, output_size=256, dropout=0.2):\n",
    "        \"\"\"Initializes ImageEncoder.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): dimensions of the output embedding\n",
    "            dropout (float): dropout for the encoded features\n",
    "        \"\"\"\n",
    "\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Use the pre-trained weights of the ResNet50 model\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Set the parameters of the ResNet50 model to not require gradients\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.resnet.fc.out_features, out_features=output_size)\n",
    "\n",
    "        # Batch Normalization (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
    "        self.batch_normalization = nn.BatchNorm1d(output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def train(self, mode):\n",
    "        super().train(mode)\n",
    "\n",
    "        # Keep resnet always in eval mode\n",
    "        self.resnet.eval()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Encodes input images into a global image embedding.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): input images of shape `[batch size, channel, width, height]`\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: global image embedding of shape `[batch size, emb_dim]`\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.resnet(images)\n",
    "\n",
    "        img_embedding = self.dropout(self.batch_normalization(self.linear(features)))\n",
    "\n",
    "        return img_embedding\n",
    "\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "image_encoder.eval()\n",
    "image_encoder.to(device)\n",
    "# Evaluating the image encoding on the sample created\n",
    "image_encoder(sample.images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 13,  9,  5, 32, 49, 16, 15, 12,  9, 15, 11,  7, 12,  7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.caption_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder\n",
    "\n",
    "This is the LSTM Decoder with the Beam Search Helper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchHelper:\n",
    "    \"\"\"Helper class with common functions for beam search sampling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature,\n",
    "        beam_width,\n",
    "        top_k,\n",
    "        unk_index,\n",
    "        eos_index,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        assert beam_width <= top_k, \"`beam_width` should be less than `top_k`\"\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.beam_width = beam_width\n",
    "        self.top_k = top_k\n",
    "        self.unk_index = unk_index\n",
    "        self.eos_index = eos_index\n",
    "        self.device = device\n",
    "        self._build_has_ended_variables()\n",
    "\n",
    "    def _build_has_ended_variables(self):\n",
    "        \"\"\"\n",
    "        Returns flags and masks for monitoring if generation has ended.\n",
    "        \"\"\"\n",
    "        # flags showing if text sequence has ended --> Create a tensor of size beam_width with all values as False\n",
    "        self.has_ended = torch.tensor([False] * self.beam_width).to(self.device)\n",
    "\n",
    "        # masks for filtering out predictions for ended/not_ended sequences\n",
    "        self._n_copies_has_ended = torch.tensor([[self.beam_width], [1]]).to(self.device)\n",
    "        self._mask_has_ended = torch.stack(\n",
    "            [\n",
    "                torch.tensor([True] * self.beam_width),\n",
    "                torch.tensor([True] + [False] * (self.beam_width - 1)),\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).to(self.device)\n",
    "\n",
    "    def filter_top_k(self, logits):\n",
    "        \"\"\"Filters `top_k` logit values by zeroing out others.\"\"\"\n",
    "        \n",
    "        # torch.topk will return the tuple of values and indices for top k elements of logits.\n",
    "        # We want the smallest value out of the top k elements. Use this value to compare\n",
    "        # with the values of output tokens, and put True at the indexes where it is smaller    \n",
    "        filter_ind = logits < torch.topk(logits, self.top_k, dim=-1).values[:, -1].unsqueeze(-1)\n",
    "        # Also put the True value at the index of UNK token, as we do not want to have UNK\n",
    "        # token in our output\n",
    "        filter_ind[:, self.unk_index] = True  \n",
    "        # Put -inf at the indexes where the filter_ind is True\n",
    "        logits[filter_ind] = float(\"-inf\")        \n",
    "        return logits\n",
    "\n",
    "    def sample_k_indices(self, logits, k=None):\n",
    "        \"\"\"Samples `beam_width` indices for each sequence in the batch.\"\"\"        \n",
    "        \n",
    "        # compute probabilities\n",
    "        p_next = torch.softmax(logits / self.temperature, dim=-1)       \n",
    "        \n",
    "        # if the value of k is None, then take the beam_width value as k\n",
    "        k = self.beam_width if k is None else k\n",
    "        # Get the indices sampled from the multinomial probability distribution\n",
    "        # located in the corresponding row of tensor input\n",
    "        sample_ind = torch.multinomial(input=p_next, num_samples=k, replacement=False)\n",
    "\n",
    "        return sample_ind\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_by_indices(values, indices):\n",
    "        # Collect the logits values corresponding to the indices\n",
    "        sample_val = torch.gather(input=values, dim=1, index=indices)\n",
    "        return sample_val\n",
    "\n",
    "    def process_logits(self, logits, sample_seq, sample_val):\n",
    "        \"\"\"Main logic of beam search sampling step.\n",
    "\n",
    "        Steps:\n",
    "            - filter `top_k` logit scores\n",
    "            - filter out predictions for already ended sequences\n",
    "            - check if new predictions end sequence\n",
    "            - update `has_ended` indices\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): logit predictions, outputs of the classifier layer\n",
    "            sample_seq (torch.Tensor): `beam_width` sequences from the previous sampling step\n",
    "            sample_val (torch.Tensor): scores for the sequences from the previous sampling step\n",
    "\n",
    "        Returns:\n",
    "            (prev_seqs, prev_vals), (new_ind, new_val):\n",
    "                expanded sequences and their scores from the previous sampling step\n",
    "                + new candidate predictions and their scores\n",
    "        \"\"\"\n",
    "        \n",
    "        # filter `top_k` values\n",
    "        logits = self.filter_top_k(logits)\n",
    "\n",
    "        # sample `beam` sequences for each branch\n",
    "        # get the top 10 indexes of the highest probability token \n",
    "        new_ind = self.sample_k_indices(logits, k=self.beam_width)\n",
    "        new_val = self.filter_by_indices(logits, new_ind).log_softmax(-1)\n",
    "        \n",
    "        new_ind, new_val = new_ind.flatten(), new_val.flatten()        \n",
    "\n",
    "        # numbers of repeat_interleave copies (if ended, only a single copy)\n",
    "        n_copies = self._n_copies_has_ended[self.has_ended.long(), :].flatten()       \n",
    "\n",
    "        # mask for unique rows\n",
    "        unique_rows = self._mask_has_ended[self.has_ended.long(), :].flatten()\n",
    "        \n",
    "        # filter values\n",
    "        new_ind = new_ind[unique_rows]\n",
    "        new_val = new_val[unique_rows]\n",
    "\n",
    "        # check if the sequences already ended\n",
    "        # (no need to predict and evaluate new scores)\n",
    "        self.has_ended = torch.repeat_interleave(self.has_ended, n_copies, dim=0)\n",
    "        \n",
    "        new_ind[self.has_ended], new_val[self.has_ended] = 0, 0.0\n",
    "\n",
    "        # update `had_ended` based on new predictions\n",
    "        self.has_ended = self.has_ended | (new_ind == self.eos_index)\n",
    "\n",
    "        # repeat current sampled sequences\n",
    "        prev_seqs = torch.repeat_interleave(sample_seq, n_copies, dim=0)\n",
    "        prev_vals = torch.repeat_interleave(sample_val, n_copies, dim=0)\n",
    "\n",
    "        if len(prev_seqs.size()) == 1:\n",
    "            prev_seqs = prev_seqs.unsqueeze(0)\n",
    "            prev_vals = prev_vals.unsqueeze(0)\n",
    "\n",
    "        return (prev_seqs, prev_vals), (new_ind, new_val)\n",
    "\n",
    "    def all_ended(self):\n",
    "        \"\"\"Returns bool indicating if all sequences have ended.\"\"\"\n",
    "        return torch.all(self.has_ended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(0 if num_layers == 1 else dropout),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)        \n",
    "\n",
    "    def forward(self, image_embeddings, caption_embeddings, caption_lengths):\n",
    "        \"\"\"\n",
    "        This method will return the output of shape (batch_size, max_caption_length, output_size)\n",
    "        max_caption_length is the maximum length of the caption in the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # image embedding + caption embeddings (packed so that they can be sent to LSTM at once)\n",
    "        # Start with image embedding as the first time-step input\n",
    "        x = torch.cat((image_embeddings.unsqueeze(1), caption_embeddings), dim=1)\n",
    "\n",
    "        # Pack the captions embeddings to avoid training on the padded tokens\n",
    "        packed = pack_padded_sequence(\n",
    "            x, caption_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        outputs, _ = self.lstm(packed)\n",
    "        # Return the outputs with the padding\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)        \n",
    "\n",
    "        # mapping into token space\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    # The embedding here is the embedding instance because we are not keeping it in this class, but we need it for the generation\n",
    "    def generate(self, image_embedding, embedding, max_len, temperature, beam_width, top_k, eos_index):\n",
    "        # beam search sampling helper\n",
    "        helper = BeamSearchHelper(\n",
    "            temperature=temperature,\n",
    "            beam_width=beam_width,\n",
    "            top_k=top_k,\n",
    "            unk_index=embedding.stoi[SPECIAL_TOKENS[\"UNK\"]],\n",
    "            eos_index=eos_index,\n",
    "            device='cpu'\n",
    "        )\n",
    "\n",
    "        # run LSTM over the inputs and predict the next token\n",
    "        outputs, (h, c) = self.lstm(image_embedding)        \n",
    "        \n",
    "        # As the start to the generation of the tokens we only have the image embedding, so resulting\n",
    "        # logits would be of shape torch.Size([1, 84880])\n",
    "        logits = self.classifier(outputs[:, -1, :])       \n",
    "        \n",
    "        # repeat hidden state  and cell state `beam` times\n",
    "        # The hidden state and cell state will be of dimension (num_layers, batch_size, hidden_size)\n",
    "        # Repeat the hidden state for each layer to the beam_width times. \n",
    "        h, c = h.repeat((1, beam_width, 1)), c.repeat((1, beam_width, 1))        \n",
    "\n",
    "        # filter `top_k` values\n",
    "        logits = helper.filter_top_k(logits)       \n",
    "        \n",
    "        # compute probabilities and sample k values\n",
    "        sample_ind = helper.sample_k_indices(logits, k=beam_width)  \n",
    "        # Apply the log softmax to the filtered out values. The log softmax is applied to\n",
    "        # avoid the numerical underflow problem      \n",
    "        sample_val = helper.filter_by_indices(logits, sample_ind).log_softmax(-1)\n",
    "        \n",
    "        sample_ind, sample_val = sample_ind.T, sample_val.T\n",
    "       \n",
    "        # define total prediction sequences\n",
    "        sample_seq = sample_ind.clone().detach()\n",
    "\n",
    "        # reusable parameters\n",
    "        beam_copies = torch.tensor([beam_width] * beam_width).to(outputs.device)\n",
    "        \n",
    "        # update `has_ended` index\n",
    "        # contiguous is used to ensure resulting tensor is stored in contiguous block of memory,\n",
    "        # as for view to reshape it requires tensor to be stored in contiguously\n",
    "\n",
    "        helper.has_ended = (sample_ind == eos_index).contiguous().view(-1)    \n",
    "        # print(\"helper.has_ended\", helper.has_ended)    \n",
    "     \n",
    "        for i in range(sample_seq.size(1), max_len):\n",
    "            # predict the next time step\n",
    "            # Get the embedding of the predicted tokens from the previous time-step\n",
    "            inputs = embedding(sample_ind)\n",
    "            \n",
    "            # Everytime it will have 10 hidden states, as for each time step we are predicting 10 tokens\n",
    "            outputs, (h, c) = self.lstm(inputs, (h, c))\n",
    "            \n",
    "            # logits would be of shape torch.Size([10, 84880]) because we want to predict tokens = beam_width\n",
    "            logits = self.classifier(outputs[:, -1, :])            \n",
    "\n",
    "            (prev_seqs, prev_vals), (new_ind, new_val) = helper.process_logits(\n",
    "                logits, sample_seq, sample_val\n",
    "            )\n",
    "\n",
    "            # create candidate sequences and compute their probabilities\n",
    "            cand_seq = torch.cat((prev_seqs, new_ind.unsqueeze(0).T), -1)\n",
    "            cand_val = prev_vals.flatten() + new_val\n",
    "\n",
    "            # sample `beam` sequences\n",
    "            filter_ind = helper.sample_k_indices(cand_val, k=beam_width)\n",
    "\n",
    "            # update total sequences and their scores\n",
    "            sample_val = cand_val[filter_ind]\n",
    "            sample_seq = cand_seq[filter_ind]\n",
    "            sample_ind = sample_seq[:, -1].unsqueeze(-1)\n",
    "\n",
    "            # filter `has_ended` flags\n",
    "            helper.has_ended = helper.has_ended[filter_ind]\n",
    "\n",
    "            # check if every branch has ended\n",
    "            if helper.all_ended():\n",
    "                break\n",
    "\n",
    "            # repeat hidden state `beam` times and filter by sampled indices\n",
    "            h = torch.repeat_interleave(h, beam_copies, dim=1)\n",
    "            c = torch.repeat_interleave(c, beam_copies, dim=1)\n",
    "\n",
    "            h, c = h[:, filter_ind, :], c[:, filter_ind, :]\n",
    "           \n",
    "        # sample output sequence\n",
    "        ind = helper.sample_k_indices(sample_val, k=2)\n",
    "        output_seq = sample_seq[ind, :].squeeze()\n",
    "\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meme Generation LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeGenerationLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based image captioning model.\n",
    "\n",
    "    Encodes input images into a embeddings of size `emb_dim`\n",
    "    and passes them as the first token to the caption generation decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding, hidden_size, num_layers, encoder_dropout, decoder_dropout):\n",
    "        super(MemeGenerationLSTM, self).__init__()\n",
    "\n",
    "        # We need to keep them same for LSTM because the first\n",
    "        # input to the LSTM is the image embedding and rest is caption\n",
    "        # embedding\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_dropout = encoder_dropout\n",
    "        self.decoder_dropout = decoder_dropout\n",
    "\n",
    "        # Initialize the Encoder which is the Image Encoder (ResNet50 model)\n",
    "        self.encoder = ImageEncoder(\n",
    "            output_size=self.embedding.embedding_dim,# Embedding dimension of the Image which is 50 defined in variable EMBEDDING_DIMENSIONS\n",
    "            dropout=self.encoder_dropout,\n",
    "        )\n",
    "        \n",
    "        self.decoder = LSTMDecoder(\n",
    "            input_size=self.embedding.embedding_dim, # Embedding dimension of the Image which is 50 defined in variable EMBEDDING_DIMENSIONS\n",
    "            hidden_size=self.hidden_size,\n",
    "            output_size=self.embedding.num_embeddings,# Number of tokens in the vocabulary including the special tokens\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.decoder_dropout,\n",
    "        )\n",
    "\n",
    "        self.params = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'encoder_dropout': encoder_dropout,\n",
    "            'decoder_dropout': decoder_dropout,\n",
    "        }\n",
    "\n",
    "    def forward(self, images, captions, caption_lengths):\n",
    "        \n",
    "        # Retrieve Image Embeddings for the processed batch\n",
    "        image_embeddings = self.encoder(images)\n",
    "        # Retrieve Caption Embeddings for the processed batch\n",
    "        caption_embeddings = self.embedding(captions)\n",
    "        # Get the outputs from the decoder\n",
    "        output = self.decoder(image_embeddings, caption_embeddings, caption_lengths)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(self, image, max_len, temperature, beam_width, top_k):\n",
    "        \"\"\"Generates caption for an image.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): input image of shape `[1, width, height]`\n",
    "            caption (torch.Tensor, optional): beginning tokens of the caption of shape `[1, seq_len]`\n",
    "            max_len (int): maximum length of the caption\n",
    "            temperature (float): temperature for softmax over logits\n",
    "            beam_width (int): number of maintained branches at each step\n",
    "            top_k (int): number of the most probable tokens to consider during sampling\n",
    "      \n",
    "        Returns:\n",
    "            torch.Tensor: generated caption tokens of shape `[1, min(output_len, max_len)]`\n",
    "        \"\"\"\n",
    "\n",
    "        # get image embedding\n",
    "        image_embedding = self.encoder(image).unsqueeze(1)\n",
    "\n",
    "        sampled_ids = self.decoder.generate(\n",
    "            image_embedding,\n",
    "            embedding=self.embedding,\n",
    "            max_len=max_len,\n",
    "            temperature=temperature,\n",
    "            beam_width=beam_width,\n",
    "            top_k=top_k,\n",
    "            eos_index=self.embedding.stoi[SPECIAL_TOKENS[\"EOS\"]],\n",
    "        )\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "    def save(self, ckpt_path):\n",
    "        \"\"\"Saves the model's state and hyperparameters.\"\"\"\n",
    "        torch.save({\"model\": self.state_dict(), \"params\": self.params}, ckpt_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(ckpt_path):\n",
    "        \"\"\"Loads and builds the model from the checkpoint file.\"\"\"\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        params = ckpt[\"params\"]\n",
    "\n",
    "        model = MemeGenerationLSTM(\n",
    "            embedding=embedding,\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            encoder_dropout=params[\"encoder_dropout\"],\n",
    "            decoder_dropout=params[\"decoder_dropout\"],\n",
    "        )\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 49, 84880])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "ENCODER_DROPOUT = 0.2\n",
    "DECODER_DROPOUT = 0.1\n",
    "\n",
    "# Create model instance\n",
    "model = MemeGenerationLSTM(\n",
    "    embedding=embedding,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    encoder_dropout=ENCODER_DROPOUT,\n",
    "    decoder_dropout=DECODER_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# Just checking the output shape of the model on the sample data\n",
    "output = model.eval()(sample.images.to(device), sample.captions.to(device), sample.caption_lengths)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"A final class for running the models.\"\"\"\n",
    "\n",
    "    def __init__(self, experiment_title, log_dir=\"./logs1\", checkpoint_dir=\"./checkpoints\", phases=(\"train\", \"val\"), device=\"cpu\"):\n",
    "        self.experiment_data = self._setup_experiment(experiment_title, log_dir, checkpoint_dir)\n",
    "\n",
    "        self.phases = phases\n",
    "        self.device = device\n",
    "\n",
    "        self.writers = self._setup_writers()\n",
    "\n",
    "    @staticmethod\n",
    "    def _setup_experiment(title, log_dir, checkpoint_dir):\n",
    "        experiment_name = \"{}@{}\".format(title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "        experiment_dir = os.path.join(log_dir, experiment_name)\n",
    "        best_model_path = f\"{title}.best.pth\"\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        experiment_data = {\n",
    "            \"model_name\": title,\n",
    "            \"name\": experiment_name,\n",
    "            \"dir\": experiment_dir,\n",
    "            \"checkpoint_dir\": checkpoint_dir,\n",
    "            \"best_model_path\": best_model_path,\n",
    "            \"epochs\": 0,\n",
    "            \"iterations\": 0,\n",
    "        }\n",
    "\n",
    "        return experiment_data\n",
    "\n",
    "    def _setup_writers(self):\n",
    "        return {phase: SummaryWriter(log_dir=os.path.join(self.experiment_data[\"dir\"], phase)) for phase in self.phases}\n",
    "\n",
    "    def run_epoch(self, model, dataloader, optimizer, criterion_main, criterion_aux, phase=\"train\"):\n",
    "        # Boolean variable to check if the model is in training mode\n",
    "        is_train = phase == \"train\"\n",
    "\n",
    "        # Set the model to train mode if it is in training phase otherwise in evaluation mode\n",
    "        model.train() if is_train else model.eval()\n",
    "\n",
    "        epoch = self.experiment_data[\"epochs\"] # this epoch will be the epoch set in train_model method\n",
    "        iterations = self.experiment_data[\"iterations\"]\n",
    "        # Initialize the epoch loss and epoch perplexity to 0\n",
    "        epoch_loss, epoch_pp = 0.0, 0.0\n",
    "\n",
    "        # On Training we want to enable the gradients\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            tqdm_object = tqdm(dataloader, desc=f\"{phase} ({epoch})\")\n",
    "\n",
    "            for batch in tqdm_object: # Iterations Loop (number of iterations to complete one epoch over complete training/evaluation data)\n",
    "                # max_len <-- maximum length of the caption in the batch\n",
    "                batch_size, max_len = batch.captions.size()                \n",
    "                images = batch.images.to(self.device)\n",
    "                captions = batch.captions.to(self.device)\n",
    "                caption_lengths = batch.caption_lengths                \n",
    "\n",
    "                # Call the forward method of the MemeGenerationLSTM class\n",
    "                pred = model(images, captions, caption_lengths)                \n",
    "\n",
    "                # Trim if there are more than max_len tokens of the processed batch\n",
    "                pred = pred[:, :max_len, :]\n",
    "                \n",
    "                # mask will have value True for all the tokens that are not PAD tokens\n",
    "                mask = captions != embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]\n",
    "\n",
    "                # The calculation of both the CrossEntropyLoss and Perplexity requires the unnormlized probabilities\n",
    "                # of the predictions.\n",
    "                # Before applying the mask, the shape of the pred will be (batch_size, max_len, vocab_size).\n",
    "                # After applying mask, the shape of the pred will be (sum of number of non PAD tokens in the batch which can be\n",
    "                # calculated as sum of caption_lengths, vocab_size). In other words, the batch_size, max_len dimensions will be\n",
    "                # flattened to a single dimension.\n",
    "                # Cross Entropy Loss requires 2D data, which is the reason to apply mask\n",
    "                loss = criterion_main(pred[mask], captions[mask])\n",
    "                # The calculation of perplexity requires input to be of shape (batch_size, seq_len, vocab_size)\n",
    "                # and target to be of shape (batch_size, seq_len).\n",
    "                pp = criterion_aux.update(pred, captions).compute()\n",
    "\n",
    "                if is_train:\n",
    "                    # make optimization step\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if is_train:\n",
    "                    iterations += 1                \n",
    "                 \n",
    "                epoch_loss += loss.item() * batch_size # de-average the loss as later it is being averaged over the entire dataset\n",
    "                epoch_pp += pp.item() * batch_size # de-average the perplexity\n",
    "\n",
    "                # dump batch metrics to tensorboard\n",
    "                if self.writers is not None and phase in self.writers and is_train:\n",
    "                    self.writers[phase].add_scalar(f\"train/batch_loss\", loss.item(), iterations)\n",
    "                    self.writers[phase].add_scalar(f\"train/batch_perplexity\", pp.item(), iterations)\n",
    "\n",
    "                tqdm_object.set_postfix(loss=loss.item(), perplexity=pp.item())\n",
    "\n",
    "            epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "            epoch_pp = epoch_pp / len(dataloader.dataset)\n",
    "\n",
    "            # dump epoch metrics to tensorboard\n",
    "            if self.writers is not None and phase in self.writers:\n",
    "                self.writers[phase].add_scalar(f\"eval/loss\", epoch_loss, epoch)\n",
    "                self.writers[phase].add_scalar(f\"eval/perplexity\", epoch_pp, epoch)\n",
    "\n",
    "        if is_train:\n",
    "            self.experiment_data[\"iterations\"] = iterations\n",
    "\n",
    "        return epoch_loss, epoch_pp\n",
    "\n",
    "    def train_model(self, model, dataloaders, optimizer, criterion_main, criterion_aux, scheduler=None, n_epochs=10):\n",
    "        # Initialize the best epoch and best validation loss\n",
    "        best_epoch, best_val_loss = 0, float(\"+inf\")\n",
    "\n",
    "        # This would be useful to resume training from a checkpoint\n",
    "        past_epochs = self.experiment_data[\"epochs\"]       \n",
    "\n",
    "        if self.writers is None:\n",
    "            self._setup_writers()\n",
    "\n",
    "        for epoch in range(past_epochs + 1, past_epochs + n_epochs + 1): # Epoch Loop\n",
    "            self.experiment_data[\"epochs\"] = epoch # Set the current epoch in the experiment data\n",
    "            print(f\"Epoch {epoch:02d}/{past_epochs + n_epochs:02d}\")\n",
    "\n",
    "            st = time()\n",
    "            for phase in self.phases:                \n",
    "                epoch_loss, epoch_pp = self.run_epoch(\n",
    "                    model, dataloaders[phase], optimizer, criterion_main, criterion_aux, phase=phase\n",
    "                )\n",
    "\n",
    "                print(f\"  {phase:5s} loss: {epoch_loss:.5f}, perplexity: {epoch_pp:.3f}\")\n",
    "\n",
    "                # If during the validation, the training loss is less than the best validation loss,\n",
    "                # then save the model\n",
    "                if phase == \"val\" and epoch_loss < best_val_loss:\n",
    "                    best_epoch, best_val_loss = epoch, epoch_loss\n",
    "                    # Save the best model\n",
    "                    model.save(os.path.join(self.experiment_data['checkpoint_dir'], self.experiment_data[\"best_model_path\"]))\n",
    "\n",
    "                # Sae the model after every epoch\n",
    "                model.save(os.path.join(self.experiment_data['checkpoint_dir'], f\"{self.experiment_data['model_name']}.e{epoch}.pth\"))\n",
    "\n",
    "                if phase == \"val\" and scheduler is not None:\n",
    "                    # Learning rate scheduling should be applied after optimizers update\n",
    "                    # https://discuss.pytorch.org/t/reduce-lr-on-plateau-based-on-training-loss-or-validation/183344\n",
    "                    scheduler.step(epoch_loss)\n",
    "\n",
    "            et = time() - st\n",
    "            print(f\"  epoch time: {et:.2f}s\")            \n",
    "\n",
    "        print(f\"Best val_loss: {best_val_loss} (epoch: {best_epoch})\")\n",
    "\n",
    "        self.experiment_data[\"epochs\"] = epoch # Set the last epoch value\n",
    "        \n",
    "\n",
    "        return self.experiment_data\n",
    "\n",
    "    def close(self):\n",
    "        for writer in self.writers.values():\n",
    "            writer.close()\n",
    "        self.writers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for training\n",
    "EPOCHS = 10\n",
    "LOGS_DIR = \"./logs_resnet\"\n",
    "CHECKPOINTS_DIR = \"./checkpoints_resnet\"\n",
    "TITLE = 'MemeGenerationResnet'\n",
    "LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (1): 100%|| 18856/18856 [17:35<00:00, 17.86it/s, loss=5.34, perplexity=335]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 5.82090, perplexity: 706.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (1): 100%|| 4680/4680 [02:59<00:00, 26.11it/s, loss=5.08, perplexity=315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.54355, perplexity: 325.855\n",
      "  epoch time: 1235.93s\n",
      "Epoch 02/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (2): 100%|| 18856/18856 [17:33<00:00, 17.89it/s, loss=4.9, perplexity=245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 5.19330, perplexity: 272.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (2): 100%|| 4680/4680 [02:59<00:00, 26.02it/s, loss=5.05, perplexity=242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.41542, perplexity: 243.782\n",
      "  epoch time: 1235.43s\n",
      "Epoch 03/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (3): 100%|| 18856/18856 [16:54<00:00, 18.59it/s, loss=5.11, perplexity=209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.98361, perplexity: 222.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (3): 100%|| 4680/4680 [02:47<00:00, 27.90it/s, loss=5.03, perplexity=209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.37896, perplexity: 209.113\n",
      "  epoch time: 1183.44s\n",
      "Epoch 04/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (4): 100%|| 18856/18856 [16:21<00:00, 19.21it/s, loss=4.58, perplexity=188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.85381, perplexity: 196.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (4): 100%|| 4680/4680 [02:48<00:00, 27.85it/s, loss=5.05, perplexity=189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.37812, perplexity: 188.753\n",
      "  epoch time: 1151.07s\n",
      "Epoch 05/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (5): 100%|| 18856/18856 [16:21<00:00, 19.22it/s, loss=4.33, perplexity=174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.76106, perplexity: 180.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (5): 100%|| 4680/4680 [02:47<00:00, 27.92it/s, loss=5.06, perplexity=175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.38482, perplexity: 174.966\n",
      "  epoch time: 1149.60s\n",
      "Epoch 06/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (6): 100%|| 18856/18856 [16:21<00:00, 19.22it/s, loss=4.61, perplexity=164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.69243, perplexity: 169.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (6): 100%|| 4680/4680 [02:47<00:00, 27.89it/s, loss=5.18, perplexity=165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.38389, perplexity: 164.877\n",
      "  epoch time: 1149.81s\n",
      "Epoch 07/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (7): 100%|| 18856/18856 [16:24<00:00, 19.15it/s, loss=5.89, perplexity=156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.61404, perplexity: 160.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (7): 100%|| 4680/4680 [02:48<00:00, 27.81it/s, loss=5.16, perplexity=157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.40089, perplexity: 156.668\n",
      "  epoch time: 1153.87s\n",
      "Epoch 08/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (8): 100%|| 18856/18856 [16:22<00:00, 19.20it/s, loss=4.72, perplexity=150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.57669, perplexity: 152.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (8): 100%|| 4680/4680 [02:48<00:00, 27.84it/s, loss=5.09, perplexity=151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.41372, perplexity: 150.291\n",
      "  epoch time: 1151.05s\n",
      "Epoch 09/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (9): 100%|| 18856/18856 [16:22<00:00, 19.19it/s, loss=4.41, perplexity=144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.52016, perplexity: 147.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (9): 100%|| 4680/4680 [02:48<00:00, 27.85it/s, loss=5.27, perplexity=145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.42510, perplexity: 144.833\n",
      "  epoch time: 1151.40s\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (10): 100%|| 18856/18856 [16:23<00:00, 19.17it/s, loss=4.26, perplexity=140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train loss: 4.49284, perplexity: 142.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (10): 100%|| 4680/4680 [02:47<00:00, 27.90it/s, loss=5.14, perplexity=141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val   loss: 5.43464, perplexity: 140.332\n",
      "  epoch time: 1152.26s\n",
      "Best val_loss: 5.378121027589473 (epoch: 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'MemeGenerationResnet',\n",
       " 'name': 'MemeGenerationResnet@13.11.2023-21:58:11',\n",
       " 'dir': './logs_resnet/MemeGenerationResnet@13.11.2023-21:58:11',\n",
       " 'checkpoint_dir': './checkpoints_resnet',\n",
       " 'best_model_path': 'MemeGenerationResnet.best.pth',\n",
       " 'epochs': 10,\n",
       " 'iterations': 188560}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(TITLE, log_dir=LOGS_DIR, checkpoint_dir=CHECKPOINTS_DIR, device=device)\n",
    "\n",
    "# Create model instance\n",
    "model = MemeGenerationLSTM(\n",
    "    embedding=embedding,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    encoder_dropout=ENCODER_DROPOUT,\n",
    "    decoder_dropout=DECODER_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# Training helpers\n",
    "criterion_main = nn.CrossEntropyLoss(ignore_index=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]).to(device)\n",
    "criterion_aux = Perplexity(ignore_index=embedding.stoi[SPECIAL_TOKENS[\"PAD\"]]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=1, factor=0.8\n",
    "    )\n",
    "\n",
    "trainer.train_model(\n",
    "    model=model,\n",
    "    dataloaders={\"train\": train_dataloader, \"val\": test_dataloader},\n",
    "    optimizer=optimizer,\n",
    "    criterion_main=criterion_main,\n",
    "    criterion_aux=criterion_aux,\n",
    "    n_epochs=EPOCHS,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best checkpoint\n",
    "best_model = MemeGenerationLSTM.from_pretrained(os.path.join(CHECKPOINTS_DIR, TITLE + '.best.pth')).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the validation data\n",
    "validation_data = {}\n",
    "for batch in test_dataloader:\n",
    "    for i, image_name in enumerate(batch.image_names):\n",
    "        if image_name not in validation_data:\n",
    "            validation_data[image_name] = {\n",
    "                'image_tensor': batch.images[i],\n",
    "                'captions': []\n",
    "            }\n",
    "        caption = [embedding.itos[index.item()] for index in batch.captions[i] if index.item() != embedding.stoi[SPECIAL_TOKENS[\"PAD\"]] and index.item() != embedding.stoi[SPECIAL_TOKENS['EOS']]]\n",
    "        validation_data[image_name]['captions'].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BLEU Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sum = 0 # Initialize the score\n",
    "for image_name, data in validation_data.items():\n",
    "\n",
    "    candidate_corpus = []\n",
    "\n",
    "    processed_image = data['image_tensor']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = best_model.cpu().generate(\n",
    "                    processed_image.unsqueeze(0).cpu(), max_len=25, temperature=1.0, beam_width=10, top_k=10\n",
    "        )\n",
    "        for entry in output:            \n",
    "            # Convert the predicted output to a list of words\n",
    "            generated_caption = [\n",
    "                embedding.itos[index.item()]\n",
    "                for index in entry\n",
    "                if index.item() != 0 and index.item() != embedding.stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
    "            ]\n",
    "            candidate_corpus.append(generated_caption)\n",
    "        \n",
    "        # Get the references corpus for the processed_image\n",
    "        reference_corpus = data['captions']\n",
    "        \n",
    "        # As the top 2 sentences are generates, we will pass the reference corpus twice\n",
    "        score = bleu_score(candidate_corpus, [reference_corpus] * 2, max_n=3, weights=[0.6, 0.25, 0.15])\n",
    "        score_sum += score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average \u001b[1mBLEU\u001b[0m score on validation dataset is 0.315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average {color.BOLD}BLEU{color.END} score on validation dataset is {score_sum/len(validation_data.keys()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate CIDEr Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/490 [00:00<?, ?it/s]PTBTokenizer tokenized 16 tokens at 695.63 tokens per second.\n",
      "PTBTokenizer tokenized 1510 tokens at 54211.44 tokens per second.\n",
      "  0%|          | 1/490 [00:00<05:18,  1.54it/s]PTBTokenizer tokenized 21 tokens at 923.78 tokens per second.\n",
      "PTBTokenizer tokenized 1199 tokens at 44508.60 tokens per second.\n",
      "  0%|          | 2/490 [00:01<05:09,  1.57it/s]PTBTokenizer tokenized 32 tokens at 1379.63 tokens per second.\n",
      "PTBTokenizer tokenized 2136 tokens at 75701.42 tokens per second.\n",
      "  1%|          | 3/490 [00:02<05:46,  1.40it/s]PTBTokenizer tokenized 28 tokens at 1213.91 tokens per second.\n",
      "PTBTokenizer tokenized 310 tokens at 12830.88 tokens per second.\n",
      "  1%|          | 4/490 [00:02<05:59,  1.35it/s]PTBTokenizer tokenized 14 tokens at 611.18 tokens per second.\n",
      "PTBTokenizer tokenized 2036 tokens at 69807.52 tokens per second.\n",
      "  1%|          | 5/490 [00:03<05:24,  1.49it/s]PTBTokenizer tokenized 19 tokens at 850.59 tokens per second.\n",
      "PTBTokenizer tokenized 1534 tokens at 57806.31 tokens per second.\n",
      "  1%|          | 6/490 [00:04<05:30,  1.47it/s]PTBTokenizer tokenized 26 tokens at 1147.77 tokens per second.\n",
      "PTBTokenizer tokenized 1686 tokens at 55780.99 tokens per second.\n",
      "  1%|         | 7/490 [00:04<05:34,  1.45it/s]PTBTokenizer tokenized 14 tokens at 603.34 tokens per second.\n",
      "PTBTokenizer tokenized 2537 tokens at 84384.53 tokens per second.\n",
      "  2%|         | 8/490 [00:05<05:30,  1.46it/s]PTBTokenizer tokenized 17 tokens at 752.34 tokens per second.\n",
      "PTBTokenizer tokenized 1351 tokens at 48951.47 tokens per second.\n",
      "  2%|         | 9/490 [00:06<05:08,  1.56it/s]PTBTokenizer tokenized 34 tokens at 1439.17 tokens per second.\n",
      "PTBTokenizer tokenized 41 tokens at 1741.32 tokens per second.\n",
      "  2%|         | 10/490 [00:06<05:26,  1.47it/s]PTBTokenizer tokenized 18 tokens at 738.30 tokens per second.\n",
      "PTBTokenizer tokenized 3183 tokens at 102237.13 tokens per second.\n",
      "  2%|         | 11/490 [00:07<04:56,  1.61it/s]PTBTokenizer tokenized 21 tokens at 864.18 tokens per second.\n",
      "PTBTokenizer tokenized 2519 tokens at 88132.55 tokens per second.\n",
      "  2%|         | 12/490 [00:07<05:05,  1.56it/s]PTBTokenizer tokenized 14 tokens at 605.01 tokens per second.\n",
      "PTBTokenizer tokenized 1616 tokens at 60116.73 tokens per second.\n",
      "  3%|         | 13/490 [00:08<04:36,  1.73it/s]PTBTokenizer tokenized 28 tokens at 1212.67 tokens per second.\n",
      "PTBTokenizer tokenized 2236 tokens at 77135.97 tokens per second.\n",
      "  3%|         | 14/490 [00:09<04:37,  1.72it/s]PTBTokenizer tokenized 14 tokens at 614.49 tokens per second.\n",
      "PTBTokenizer tokenized 809 tokens at 30838.37 tokens per second.\n",
      "  3%|         | 15/490 [00:09<04:25,  1.79it/s]PTBTokenizer tokenized 11 tokens at 481.21 tokens per second.\n",
      "PTBTokenizer tokenized 441 tokens at 17972.13 tokens per second.\n",
      "  3%|         | 16/490 [00:10<04:22,  1.81it/s]PTBTokenizer tokenized 17 tokens at 736.51 tokens per second.\n",
      "PTBTokenizer tokenized 1456 tokens at 53818.07 tokens per second.\n",
      "  3%|         | 17/490 [00:10<04:26,  1.77it/s]PTBTokenizer tokenized 10 tokens at 448.31 tokens per second.\n",
      "PTBTokenizer tokenized 1607 tokens at 59060.46 tokens per second.\n",
      "  4%|         | 18/490 [00:11<04:31,  1.74it/s]PTBTokenizer tokenized 25 tokens at 1114.63 tokens per second.\n",
      "PTBTokenizer tokenized 1192 tokens at 45310.40 tokens per second.\n",
      "  4%|         | 19/490 [00:11<04:32,  1.73it/s]PTBTokenizer tokenized 20 tokens at 871.47 tokens per second.\n",
      "PTBTokenizer tokenized 3176 tokens at 101726.65 tokens per second.\n",
      "  4%|         | 20/490 [00:12<04:41,  1.67it/s]PTBTokenizer tokenized 23 tokens at 1027.54 tokens per second.\n",
      "PTBTokenizer tokenized 1954 tokens at 67090.32 tokens per second.\n",
      "  4%|         | 21/490 [00:13<04:43,  1.66it/s]PTBTokenizer tokenized 17 tokens at 729.67 tokens per second.\n",
      "PTBTokenizer tokenized 3044 tokens at 96363.42 tokens per second.\n",
      "  4%|         | 22/490 [00:13<04:59,  1.56it/s]PTBTokenizer tokenized 19 tokens at 842.08 tokens per second.\n",
      "PTBTokenizer tokenized 822 tokens at 30837.63 tokens per second.\n",
      "  5%|         | 23/490 [00:14<04:57,  1.57it/s]PTBTokenizer tokenized 15 tokens at 678.68 tokens per second.\n",
      "PTBTokenizer tokenized 2613 tokens at 86187.70 tokens per second.\n",
      "  5%|         | 24/490 [00:15<04:49,  1.61it/s]PTBTokenizer tokenized 18 tokens at 784.05 tokens per second.\n",
      "PTBTokenizer tokenized 1407 tokens at 51614.75 tokens per second.\n",
      "  5%|         | 25/490 [00:15<04:56,  1.57it/s]PTBTokenizer tokenized 17 tokens at 750.54 tokens per second.\n",
      "PTBTokenizer tokenized 2081 tokens at 73356.02 tokens per second.\n",
      "  5%|         | 26/490 [00:16<04:41,  1.65it/s]PTBTokenizer tokenized 13 tokens at 573.39 tokens per second.\n",
      "PTBTokenizer tokenized 4 tokens at 181.24 tokens per second.\n",
      "  6%|         | 27/490 [00:16<04:41,  1.65it/s]PTBTokenizer tokenized 33 tokens at 1408.48 tokens per second.\n",
      "PTBTokenizer tokenized 750 tokens at 28670.55 tokens per second.\n",
      "  6%|         | 28/490 [00:17<04:57,  1.56it/s]PTBTokenizer tokenized 29 tokens at 1258.00 tokens per second.\n",
      "PTBTokenizer tokenized 1464 tokens at 54462.25 tokens per second.\n",
      "  6%|         | 29/490 [00:18<05:09,  1.49it/s]PTBTokenizer tokenized 12 tokens at 523.31 tokens per second.\n",
      "PTBTokenizer tokenized 2661 tokens at 90282.72 tokens per second.\n",
      "  6%|         | 30/490 [00:18<04:50,  1.58it/s]PTBTokenizer tokenized 11 tokens at 482.01 tokens per second.\n",
      "PTBTokenizer tokenized 2314 tokens at 83123.08 tokens per second.\n",
      "  6%|         | 31/490 [00:19<04:45,  1.61it/s]PTBTokenizer tokenized 24 tokens at 1048.38 tokens per second.\n",
      "PTBTokenizer tokenized 1917 tokens at 68716.16 tokens per second.\n",
      "  7%|         | 32/490 [00:20<04:45,  1.60it/s]PTBTokenizer tokenized 16 tokens at 689.38 tokens per second.\n",
      "PTBTokenizer tokenized 1631 tokens at 59275.34 tokens per second.\n",
      "  7%|         | 33/490 [00:20<04:42,  1.62it/s]PTBTokenizer tokenized 19 tokens at 825.51 tokens per second.\n",
      "PTBTokenizer tokenized 1980 tokens at 69302.41 tokens per second.\n",
      "  7%|         | 34/490 [00:21<04:54,  1.55it/s]PTBTokenizer tokenized 14 tokens at 562.00 tokens per second.\n",
      "PTBTokenizer tokenized 2898 tokens at 99685.95 tokens per second.\n",
      "  7%|         | 35/490 [00:21<04:42,  1.61it/s]PTBTokenizer tokenized 23 tokens at 987.77 tokens per second.\n",
      "PTBTokenizer tokenized 2175 tokens at 72404.47 tokens per second.\n",
      "  7%|         | 36/490 [00:22<04:33,  1.66it/s]PTBTokenizer tokenized 14 tokens at 620.92 tokens per second.\n",
      "PTBTokenizer tokenized 765 tokens at 29206.80 tokens per second.\n",
      "  8%|         | 37/490 [00:22<04:14,  1.78it/s]PTBTokenizer tokenized 17 tokens at 744.63 tokens per second.\n",
      "PTBTokenizer tokenized 2022 tokens at 71599.33 tokens per second.\n",
      "  8%|         | 38/490 [00:23<04:01,  1.87it/s]PTBTokenizer tokenized 16 tokens at 705.87 tokens per second.\n",
      "PTBTokenizer tokenized 1993 tokens at 70497.08 tokens per second.\n",
      "  8%|         | 39/490 [00:24<04:21,  1.72it/s]PTBTokenizer tokenized 28 tokens at 1233.67 tokens per second.\n",
      "PTBTokenizer tokenized 2568 tokens at 83725.09 tokens per second.\n",
      "  8%|         | 40/490 [00:24<04:37,  1.62it/s]PTBTokenizer tokenized 13 tokens at 579.15 tokens per second.\n",
      "PTBTokenizer tokenized 1552 tokens at 55820.44 tokens per second.\n",
      "  8%|         | 41/490 [00:25<04:32,  1.65it/s]PTBTokenizer tokenized 26 tokens at 1160.97 tokens per second.\n",
      "PTBTokenizer tokenized 28 tokens at 1232.32 tokens per second.\n",
      "  9%|         | 42/490 [00:26<04:43,  1.58it/s]PTBTokenizer tokenized 18 tokens at 795.39 tokens per second.\n",
      "PTBTokenizer tokenized 1827 tokens at 66943.30 tokens per second.\n",
      "  9%|         | 43/490 [00:26<04:45,  1.57it/s]PTBTokenizer tokenized 19 tokens at 851.93 tokens per second.\n",
      "PTBTokenizer tokenized 1884 tokens at 69282.52 tokens per second.\n",
      "  9%|         | 44/490 [00:27<04:43,  1.57it/s]PTBTokenizer tokenized 13 tokens at 573.09 tokens per second.\n",
      "PTBTokenizer tokenized 1658 tokens at 60923.08 tokens per second.\n",
      "  9%|         | 45/490 [00:27<04:35,  1.61it/s]PTBTokenizer tokenized 13 tokens at 581.66 tokens per second.\n",
      "PTBTokenizer tokenized 1464 tokens at 56219.71 tokens per second.\n",
      "  9%|         | 46/490 [00:28<04:17,  1.73it/s]PTBTokenizer tokenized 17 tokens at 743.83 tokens per second.\n",
      "PTBTokenizer tokenized 2144 tokens at 78550.29 tokens per second.\n",
      " 10%|         | 47/490 [00:29<04:18,  1.71it/s]PTBTokenizer tokenized 10 tokens at 446.83 tokens per second.\n",
      "PTBTokenizer tokenized 39 tokens at 1693.02 tokens per second.\n",
      " 10%|         | 48/490 [00:29<04:23,  1.67it/s]PTBTokenizer tokenized 21 tokens at 944.32 tokens per second.\n",
      "PTBTokenizer tokenized 935 tokens at 36276.09 tokens per second.\n",
      " 10%|         | 49/490 [00:30<04:20,  1.69it/s]PTBTokenizer tokenized 12 tokens at 528.78 tokens per second.\n",
      "PTBTokenizer tokenized 2087 tokens at 75028.58 tokens per second.\n",
      " 10%|         | 50/490 [00:30<04:21,  1.68it/s]PTBTokenizer tokenized 33 tokens at 1406.27 tokens per second.\n",
      "PTBTokenizer tokenized 2303 tokens at 70418.16 tokens per second.\n",
      " 10%|         | 51/490 [00:31<04:34,  1.60it/s]PTBTokenizer tokenized 16 tokens at 691.40 tokens per second.\n",
      "PTBTokenizer tokenized 212 tokens at 8807.65 tokens per second.\n",
      " 11%|         | 52/490 [00:32<04:35,  1.59it/s]PTBTokenizer tokenized 14 tokens at 582.53 tokens per second.\n",
      "PTBTokenizer tokenized 1958 tokens at 69469.69 tokens per second.\n",
      " 11%|         | 53/490 [00:32<04:25,  1.64it/s]PTBTokenizer tokenized 27 tokens at 1208.48 tokens per second.\n",
      "PTBTokenizer tokenized 174 tokens at 7410.44 tokens per second.\n",
      " 11%|         | 54/490 [00:33<04:25,  1.64it/s]PTBTokenizer tokenized 20 tokens at 888.13 tokens per second.\n",
      "PTBTokenizer tokenized 1645 tokens at 60777.00 tokens per second.\n",
      " 11%|         | 55/490 [00:33<04:08,  1.75it/s]PTBTokenizer tokenized 21 tokens at 943.11 tokens per second.\n",
      "PTBTokenizer tokenized 765 tokens at 29593.92 tokens per second.\n",
      " 11%|        | 56/490 [00:34<04:12,  1.72it/s]PTBTokenizer tokenized 15 tokens at 650.38 tokens per second.\n",
      "PTBTokenizer tokenized 1641 tokens at 59170.16 tokens per second.\n",
      " 12%|        | 57/490 [00:34<03:59,  1.81it/s]PTBTokenizer tokenized 14 tokens at 603.30 tokens per second.\n",
      "PTBTokenizer tokenized 1491 tokens at 55309.62 tokens per second.\n",
      " 12%|        | 58/490 [00:35<03:57,  1.82it/s]PTBTokenizer tokenized 14 tokens at 640.96 tokens per second.\n",
      "PTBTokenizer tokenized 50 tokens at 2215.45 tokens per second.\n",
      " 12%|        | 59/490 [00:35<03:49,  1.88it/s]PTBTokenizer tokenized 20 tokens at 907.17 tokens per second.\n",
      "PTBTokenizer tokenized 2043 tokens at 72875.06 tokens per second.\n",
      " 12%|        | 60/490 [00:36<04:03,  1.77it/s]PTBTokenizer tokenized 24 tokens at 992.16 tokens per second.\n",
      "PTBTokenizer tokenized 1817 tokens at 65780.29 tokens per second.\n",
      " 12%|        | 61/490 [00:37<04:11,  1.70it/s]PTBTokenizer tokenized 19 tokens at 856.96 tokens per second.\n",
      "PTBTokenizer tokenized 2009 tokens at 73016.19 tokens per second.\n",
      " 13%|        | 62/490 [00:37<03:53,  1.83it/s]PTBTokenizer tokenized 13 tokens at 567.36 tokens per second.\n",
      "PTBTokenizer tokenized 1746 tokens at 62697.12 tokens per second.\n",
      " 13%|        | 63/490 [00:38<03:42,  1.92it/s]PTBTokenizer tokenized 26 tokens at 1083.41 tokens per second.\n",
      "PTBTokenizer tokenized 1621 tokens at 59263.14 tokens per second.\n",
      " 13%|        | 64/490 [00:38<04:10,  1.70it/s]PTBTokenizer tokenized 18 tokens at 783.33 tokens per second.\n",
      "PTBTokenizer tokenized 1221 tokens at 44559.53 tokens per second.\n",
      " 13%|        | 65/490 [00:39<04:30,  1.57it/s]PTBTokenizer tokenized 14 tokens at 612.43 tokens per second.\n",
      "PTBTokenizer tokenized 1616 tokens at 58863.08 tokens per second.\n",
      " 13%|        | 66/490 [00:40<04:22,  1.62it/s]PTBTokenizer tokenized 14 tokens at 629.68 tokens per second.\n",
      "PTBTokenizer tokenized 2556 tokens at 83552.87 tokens per second.\n",
      " 14%|        | 67/490 [00:40<04:18,  1.63it/s]PTBTokenizer tokenized 31 tokens at 1392.98 tokens per second.\n",
      "PTBTokenizer tokenized 1637 tokens at 62032.35 tokens per second.\n",
      " 14%|        | 68/490 [00:41<04:26,  1.58it/s]PTBTokenizer tokenized 21 tokens at 927.54 tokens per second.\n",
      "PTBTokenizer tokenized 2253 tokens at 78002.89 tokens per second.\n",
      " 14%|        | 69/490 [00:42<04:11,  1.68it/s]PTBTokenizer tokenized 9 tokens at 393.29 tokens per second.\n",
      "PTBTokenizer tokenized 1049 tokens at 40165.67 tokens per second.\n",
      " 14%|        | 70/490 [00:42<04:24,  1.59it/s]PTBTokenizer tokenized 11 tokens at 481.07 tokens per second.\n",
      "PTBTokenizer tokenized 221 tokens at 9167.78 tokens per second.\n",
      " 14%|        | 71/490 [00:43<04:28,  1.56it/s]PTBTokenizer tokenized 16 tokens at 693.63 tokens per second.\n",
      "PTBTokenizer tokenized 487 tokens at 19467.98 tokens per second.\n",
      " 15%|        | 72/490 [00:44<04:27,  1.56it/s]PTBTokenizer tokenized 24 tokens at 1078.85 tokens per second.\n",
      "PTBTokenizer tokenized 1616 tokens at 56768.00 tokens per second.\n",
      " 15%|        | 73/490 [00:44<04:32,  1.53it/s]PTBTokenizer tokenized 14 tokens at 618.53 tokens per second.\n",
      "PTBTokenizer tokenized 1960 tokens at 70873.73 tokens per second.\n",
      " 15%|        | 74/490 [00:45<04:10,  1.66it/s]PTBTokenizer tokenized 14 tokens at 621.53 tokens per second.\n",
      "PTBTokenizer tokenized 2347 tokens at 84402.05 tokens per second.\n",
      " 15%|        | 75/490 [00:45<04:08,  1.67it/s]PTBTokenizer tokenized 19 tokens at 802.36 tokens per second.\n",
      "PTBTokenizer tokenized 2882 tokens at 95558.66 tokens per second.\n",
      " 16%|        | 76/490 [00:46<03:58,  1.73it/s]PTBTokenizer tokenized 20 tokens at 874.91 tokens per second.\n",
      "PTBTokenizer tokenized 1731 tokens at 63327.03 tokens per second.\n",
      " 16%|        | 77/490 [00:46<03:45,  1.83it/s]PTBTokenizer tokenized 43 tokens at 1875.33 tokens per second.\n",
      "PTBTokenizer tokenized 1997 tokens at 73254.15 tokens per second.\n",
      " 16%|        | 78/490 [00:47<03:56,  1.74it/s]PTBTokenizer tokenized 25 tokens at 1125.79 tokens per second.\n",
      "PTBTokenizer tokenized 2412 tokens at 85178.48 tokens per second.\n",
      " 16%|        | 79/490 [00:48<04:07,  1.66it/s]PTBTokenizer tokenized 14 tokens at 620.96 tokens per second.\n",
      "PTBTokenizer tokenized 1795 tokens at 65677.82 tokens per second.\n",
      " 16%|        | 80/490 [00:48<04:00,  1.70it/s]PTBTokenizer tokenized 15 tokens at 653.73 tokens per second.\n",
      "PTBTokenizer tokenized 1878 tokens at 67258.39 tokens per second.\n",
      " 17%|        | 81/490 [00:49<04:05,  1.66it/s]PTBTokenizer tokenized 17 tokens at 761.64 tokens per second.\n",
      "PTBTokenizer tokenized 1865 tokens at 65046.05 tokens per second.\n",
      " 17%|        | 82/490 [00:49<03:57,  1.72it/s]PTBTokenizer tokenized 20 tokens at 877.17 tokens per second.\n",
      "PTBTokenizer tokenized 618 tokens at 24328.90 tokens per second.\n",
      " 17%|        | 83/490 [00:50<04:04,  1.66it/s]PTBTokenizer tokenized 44 tokens at 1850.27 tokens per second.\n",
      "PTBTokenizer tokenized 1771 tokens at 61621.37 tokens per second.\n",
      " 17%|        | 84/490 [00:51<04:09,  1.63it/s]PTBTokenizer tokenized 26 tokens at 1157.72 tokens per second.\n",
      "PTBTokenizer tokenized 506 tokens at 20207.37 tokens per second.\n",
      " 17%|        | 85/490 [00:51<04:15,  1.58it/s]PTBTokenizer tokenized 11 tokens at 494.38 tokens per second.\n",
      "PTBTokenizer tokenized 578 tokens at 22126.13 tokens per second.\n",
      " 18%|        | 86/490 [00:52<03:49,  1.76it/s]PTBTokenizer tokenized 20 tokens at 883.30 tokens per second.\n",
      "PTBTokenizer tokenized 2720 tokens at 94383.25 tokens per second.\n",
      " 18%|        | 87/490 [00:52<04:10,  1.61it/s]PTBTokenizer tokenized 34 tokens at 1504.32 tokens per second.\n",
      "PTBTokenizer tokenized 1898 tokens at 69471.92 tokens per second.\n",
      " 18%|        | 88/490 [00:53<04:11,  1.60it/s]PTBTokenizer tokenized 34 tokens at 1513.89 tokens per second.\n",
      "PTBTokenizer tokenized 1515 tokens at 56064.29 tokens per second.\n",
      " 18%|        | 89/490 [00:54<04:16,  1.56it/s]PTBTokenizer tokenized 17 tokens at 754.53 tokens per second.\n",
      "PTBTokenizer tokenized 502 tokens at 19033.03 tokens per second.\n",
      " 18%|        | 90/490 [00:54<04:19,  1.54it/s]PTBTokenizer tokenized 21 tokens at 922.64 tokens per second.\n",
      "PTBTokenizer tokenized 2679 tokens at 89991.26 tokens per second.\n",
      " 19%|        | 91/490 [00:55<04:18,  1.54it/s]PTBTokenizer tokenized 40 tokens at 1749.66 tokens per second.\n",
      "PTBTokenizer tokenized 1862 tokens at 68328.54 tokens per second.\n",
      " 19%|        | 92/490 [00:56<04:18,  1.54it/s]PTBTokenizer tokenized 13 tokens at 570.04 tokens per second.\n",
      "PTBTokenizer tokenized 1593 tokens at 58262.84 tokens per second.\n",
      " 19%|        | 93/490 [00:56<03:45,  1.76it/s]PTBTokenizer tokenized 36 tokens at 1274.08 tokens per second.\n",
      "PTBTokenizer tokenized 1941 tokens at 69361.09 tokens per second.\n",
      " 19%|        | 94/490 [00:57<03:56,  1.67it/s]PTBTokenizer tokenized 18 tokens at 778.91 tokens per second.\n",
      "PTBTokenizer tokenized 1289 tokens at 48732.61 tokens per second.\n",
      " 19%|        | 95/490 [00:57<04:00,  1.64it/s]PTBTokenizer tokenized 24 tokens at 1099.96 tokens per second.\n",
      "PTBTokenizer tokenized 1149 tokens at 45096.88 tokens per second.\n",
      " 20%|        | 96/490 [00:58<04:08,  1.59it/s]PTBTokenizer tokenized 12 tokens at 524.76 tokens per second.\n",
      "PTBTokenizer tokenized 36 tokens at 1535.86 tokens per second.\n",
      " 20%|        | 97/490 [00:59<03:55,  1.67it/s]PTBTokenizer tokenized 33 tokens at 1396.39 tokens per second.\n",
      "PTBTokenizer tokenized 1910 tokens at 64704.72 tokens per second.\n",
      " 20%|        | 98/490 [00:59<04:01,  1.62it/s]PTBTokenizer tokenized 16 tokens at 712.59 tokens per second.\n",
      "PTBTokenizer tokenized 1951 tokens at 70785.81 tokens per second.\n",
      " 20%|        | 99/490 [01:00<03:53,  1.67it/s]PTBTokenizer tokenized 19 tokens at 833.62 tokens per second.\n",
      "PTBTokenizer tokenized 3799 tokens at 122438.78 tokens per second.\n",
      " 20%|        | 100/490 [01:01<04:02,  1.61it/s]PTBTokenizer tokenized 17 tokens at 742.01 tokens per second.\n",
      "PTBTokenizer tokenized 1906 tokens at 66955.57 tokens per second.\n",
      " 21%|        | 101/490 [01:01<04:05,  1.59it/s]PTBTokenizer tokenized 25 tokens at 1097.24 tokens per second.\n",
      "PTBTokenizer tokenized 2004 tokens at 72356.99 tokens per second.\n",
      " 21%|        | 102/490 [01:02<04:11,  1.54it/s]PTBTokenizer tokenized 27 tokens at 1172.69 tokens per second.\n",
      "PTBTokenizer tokenized 3415 tokens at 111004.82 tokens per second.\n",
      " 21%|        | 103/490 [01:03<04:23,  1.47it/s]PTBTokenizer tokenized 24 tokens at 1054.38 tokens per second.\n",
      "PTBTokenizer tokenized 1570 tokens at 58340.33 tokens per second.\n",
      " 21%|        | 104/490 [01:03<04:12,  1.53it/s]PTBTokenizer tokenized 24 tokens at 1045.51 tokens per second.\n",
      "PTBTokenizer tokenized 1984 tokens at 70715.10 tokens per second.\n",
      " 21%|       | 105/490 [01:04<03:52,  1.66it/s]PTBTokenizer tokenized 16 tokens at 723.24 tokens per second.\n",
      "PTBTokenizer tokenized 2259 tokens at 79750.01 tokens per second.\n",
      " 22%|       | 106/490 [01:04<03:42,  1.73it/s]PTBTokenizer tokenized 18 tokens at 797.80 tokens per second.\n",
      "PTBTokenizer tokenized 2529 tokens at 88935.55 tokens per second.\n",
      " 22%|       | 107/490 [01:05<03:50,  1.66it/s]PTBTokenizer tokenized 18 tokens at 805.58 tokens per second.\n",
      "PTBTokenizer tokenized 2265 tokens at 75915.86 tokens per second.\n",
      " 22%|       | 108/490 [01:05<03:45,  1.70it/s]PTBTokenizer tokenized 31 tokens at 1366.16 tokens per second.\n",
      "PTBTokenizer tokenized 2930 tokens at 98831.97 tokens per second.\n",
      " 22%|       | 109/490 [01:06<03:53,  1.63it/s]PTBTokenizer tokenized 20 tokens at 875.28 tokens per second.\n",
      "PTBTokenizer tokenized 1138 tokens at 42278.74 tokens per second.\n",
      " 22%|       | 110/490 [01:07<03:44,  1.69it/s]PTBTokenizer tokenized 24 tokens at 1043.35 tokens per second.\n",
      "PTBTokenizer tokenized 1102 tokens at 41307.78 tokens per second.\n",
      " 23%|       | 111/490 [01:07<03:50,  1.65it/s]PTBTokenizer tokenized 33 tokens at 1464.28 tokens per second.\n",
      "PTBTokenizer tokenized 2179 tokens at 75361.93 tokens per second.\n",
      " 23%|       | 112/490 [01:08<03:56,  1.60it/s]PTBTokenizer tokenized 10 tokens at 437.39 tokens per second.\n",
      "PTBTokenizer tokenized 1436 tokens at 53812.44 tokens per second.\n",
      " 23%|       | 113/490 [01:09<03:52,  1.62it/s]PTBTokenizer tokenized 14 tokens at 619.86 tokens per second.\n",
      "PTBTokenizer tokenized 42 tokens at 1802.81 tokens per second.\n",
      " 23%|       | 114/490 [01:09<03:38,  1.72it/s]PTBTokenizer tokenized 26 tokens at 1160.74 tokens per second.\n",
      "PTBTokenizer tokenized 1508 tokens at 56523.78 tokens per second.\n",
      " 23%|       | 115/490 [01:10<03:32,  1.76it/s]PTBTokenizer tokenized 18 tokens at 801.57 tokens per second.\n",
      "PTBTokenizer tokenized 1963 tokens at 68987.92 tokens per second.\n",
      " 24%|       | 116/490 [01:10<03:19,  1.87it/s]PTBTokenizer tokenized 13 tokens at 573.33 tokens per second.\n",
      "PTBTokenizer tokenized 1777 tokens at 60561.26 tokens per second.\n",
      " 24%|       | 117/490 [01:10<03:02,  2.05it/s]PTBTokenizer tokenized 27 tokens at 1165.88 tokens per second.\n",
      "PTBTokenizer tokenized 918 tokens at 33797.37 tokens per second.\n",
      " 24%|       | 118/490 [01:11<03:13,  1.92it/s]PTBTokenizer tokenized 15 tokens at 657.20 tokens per second.\n",
      "PTBTokenizer tokenized 1807 tokens at 63814.36 tokens per second.\n",
      " 24%|       | 119/490 [01:12<03:13,  1.92it/s]PTBTokenizer tokenized 18 tokens at 799.67 tokens per second.\n",
      "PTBTokenizer tokenized 1828 tokens at 65098.74 tokens per second.\n",
      " 24%|       | 120/490 [01:12<03:08,  1.96it/s]PTBTokenizer tokenized 23 tokens at 996.29 tokens per second.\n",
      "PTBTokenizer tokenized 1630 tokens at 59989.05 tokens per second.\n",
      " 25%|       | 121/490 [01:13<03:22,  1.82it/s]PTBTokenizer tokenized 31 tokens at 1338.13 tokens per second.\n",
      "PTBTokenizer tokenized 615 tokens at 24655.08 tokens per second.\n",
      " 25%|       | 122/490 [01:13<03:33,  1.72it/s]PTBTokenizer tokenized 27 tokens at 1167.75 tokens per second.\n",
      "PTBTokenizer tokenized 2285 tokens at 80812.12 tokens per second.\n",
      " 25%|       | 123/490 [01:14<03:42,  1.65it/s]PTBTokenizer tokenized 34 tokens at 1482.42 tokens per second.\n",
      "PTBTokenizer tokenized 2262 tokens at 78908.48 tokens per second.\n",
      " 25%|       | 124/490 [01:15<03:47,  1.61it/s]PTBTokenizer tokenized 24 tokens at 1048.63 tokens per second.\n",
      "PTBTokenizer tokenized 1462 tokens at 53758.00 tokens per second.\n",
      " 26%|       | 125/490 [01:15<03:51,  1.58it/s]PTBTokenizer tokenized 24 tokens at 1048.06 tokens per second.\n",
      "PTBTokenizer tokenized 1903 tokens at 68453.39 tokens per second.\n",
      " 26%|       | 126/490 [01:16<03:48,  1.59it/s]PTBTokenizer tokenized 16 tokens at 688.16 tokens per second.\n",
      "PTBTokenizer tokenized 1721 tokens at 64026.51 tokens per second.\n",
      " 26%|       | 127/490 [01:17<03:48,  1.59it/s]PTBTokenizer tokenized 12 tokens at 478.40 tokens per second.\n",
      "PTBTokenizer tokenized 1831 tokens at 65165.85 tokens per second.\n",
      " 26%|       | 128/490 [01:17<03:33,  1.70it/s]PTBTokenizer tokenized 22 tokens at 982.31 tokens per second.\n",
      "PTBTokenizer tokenized 1777 tokens at 64512.99 tokens per second.\n",
      " 26%|       | 129/490 [01:18<03:30,  1.71it/s]PTBTokenizer tokenized 14 tokens at 621.52 tokens per second.\n",
      "PTBTokenizer tokenized 2224 tokens at 78146.01 tokens per second.\n",
      " 27%|       | 130/490 [01:18<03:25,  1.76it/s]PTBTokenizer tokenized 13 tokens at 591.42 tokens per second.\n",
      "PTBTokenizer tokenized 1971 tokens at 72013.77 tokens per second.\n",
      " 27%|       | 131/490 [01:19<03:17,  1.81it/s]PTBTokenizer tokenized 18 tokens at 776.26 tokens per second.\n",
      "PTBTokenizer tokenized 3098 tokens at 107906.12 tokens per second.\n",
      " 27%|       | 132/490 [01:19<03:22,  1.77it/s]PTBTokenizer tokenized 12 tokens at 532.52 tokens per second.\n",
      "PTBTokenizer tokenized 1921 tokens at 60644.98 tokens per second.\n",
      " 27%|       | 133/490 [01:20<03:21,  1.78it/s]PTBTokenizer tokenized 12 tokens at 492.06 tokens per second.\n",
      "PTBTokenizer tokenized 2133 tokens at 76301.98 tokens per second.\n",
      " 27%|       | 134/490 [01:20<03:30,  1.69it/s]PTBTokenizer tokenized 36 tokens at 1585.18 tokens per second.\n",
      "PTBTokenizer tokenized 1700 tokens at 59438.23 tokens per second.\n",
      " 28%|       | 135/490 [01:21<03:39,  1.61it/s]PTBTokenizer tokenized 16 tokens at 709.62 tokens per second.\n",
      "PTBTokenizer tokenized 1794 tokens at 62617.50 tokens per second.\n",
      " 28%|       | 136/490 [01:22<03:32,  1.67it/s]PTBTokenizer tokenized 10 tokens at 437.70 tokens per second.\n",
      "PTBTokenizer tokenized 2318 tokens at 82584.46 tokens per second.\n",
      " 28%|       | 137/490 [01:22<03:31,  1.67it/s]PTBTokenizer tokenized 17 tokens at 739.65 tokens per second.\n",
      "PTBTokenizer tokenized 1265 tokens at 47398.27 tokens per second.\n",
      " 28%|       | 138/490 [01:23<03:26,  1.71it/s]PTBTokenizer tokenized 13 tokens at 569.21 tokens per second.\n",
      "PTBTokenizer tokenized 1630 tokens at 58784.72 tokens per second.\n",
      " 28%|       | 139/490 [01:23<03:17,  1.78it/s]PTBTokenizer tokenized 14 tokens at 632.23 tokens per second.\n",
      "PTBTokenizer tokenized 2042 tokens at 73868.27 tokens per second.\n",
      " 29%|       | 140/490 [01:24<03:12,  1.82it/s]PTBTokenizer tokenized 15 tokens at 616.30 tokens per second.\n",
      "PTBTokenizer tokenized 2573 tokens at 88834.00 tokens per second.\n",
      " 29%|       | 141/490 [01:24<03:13,  1.80it/s]PTBTokenizer tokenized 29 tokens at 1228.97 tokens per second.\n",
      "PTBTokenizer tokenized 1125 tokens at 43740.60 tokens per second.\n",
      " 29%|       | 142/490 [01:25<03:27,  1.67it/s]PTBTokenizer tokenized 14 tokens at 611.91 tokens per second.\n",
      "PTBTokenizer tokenized 1768 tokens at 61605.17 tokens per second.\n",
      " 29%|       | 143/490 [01:26<03:23,  1.70it/s]PTBTokenizer tokenized 33 tokens at 1447.24 tokens per second.\n",
      "PTBTokenizer tokenized 300 tokens at 12425.82 tokens per second.\n",
      " 29%|       | 144/490 [01:26<03:39,  1.57it/s]PTBTokenizer tokenized 22 tokens at 984.51 tokens per second.\n",
      "PTBTokenizer tokenized 1990 tokens at 71680.18 tokens per second.\n",
      " 30%|       | 145/490 [01:27<03:50,  1.50it/s]PTBTokenizer tokenized 35 tokens at 1551.32 tokens per second.\n",
      "PTBTokenizer tokenized 1983 tokens at 72897.17 tokens per second.\n",
      " 30%|       | 146/490 [01:28<03:59,  1.44it/s]PTBTokenizer tokenized 11 tokens at 488.46 tokens per second.\n",
      "PTBTokenizer tokenized 45 tokens at 2015.91 tokens per second.\n",
      " 30%|       | 147/490 [01:29<03:58,  1.44it/s]PTBTokenizer tokenized 32 tokens at 1428.71 tokens per second.\n",
      "PTBTokenizer tokenized 2168 tokens at 78754.00 tokens per second.\n",
      " 30%|       | 148/490 [01:29<03:54,  1.46it/s]PTBTokenizer tokenized 22 tokens at 987.49 tokens per second.\n",
      "PTBTokenizer tokenized 2040 tokens at 72392.46 tokens per second.\n",
      " 30%|       | 149/490 [01:30<03:53,  1.46it/s]PTBTokenizer tokenized 15 tokens at 654.87 tokens per second.\n",
      "PTBTokenizer tokenized 2954 tokens at 100223.63 tokens per second.\n",
      " 31%|       | 150/490 [01:31<03:35,  1.58it/s]PTBTokenizer tokenized 38 tokens at 1556.94 tokens per second.\n",
      "PTBTokenizer tokenized 223 tokens at 9282.01 tokens per second.\n",
      " 31%|       | 151/490 [01:31<03:36,  1.57it/s]PTBTokenizer tokenized 19 tokens at 847.20 tokens per second.\n",
      "PTBTokenizer tokenized 200 tokens at 8500.60 tokens per second.\n",
      " 31%|       | 152/490 [01:32<03:32,  1.59it/s]PTBTokenizer tokenized 32 tokens at 1432.50 tokens per second.\n",
      "PTBTokenizer tokenized 1326 tokens at 48820.17 tokens per second.\n",
      " 31%|       | 153/490 [01:32<03:28,  1.61it/s]PTBTokenizer tokenized 22 tokens at 1012.49 tokens per second.\n",
      "PTBTokenizer tokenized 1578 tokens at 58469.80 tokens per second.\n",
      " 31%|      | 154/490 [01:33<03:29,  1.60it/s]PTBTokenizer tokenized 28 tokens at 1218.68 tokens per second.\n",
      "PTBTokenizer tokenized 1401 tokens at 51765.97 tokens per second.\n",
      " 32%|      | 155/490 [01:34<03:31,  1.58it/s]PTBTokenizer tokenized 38 tokens at 1646.27 tokens per second.\n",
      "PTBTokenizer tokenized 2201 tokens at 77062.44 tokens per second.\n",
      " 32%|      | 156/490 [01:34<03:35,  1.55it/s]PTBTokenizer tokenized 26 tokens at 1186.25 tokens per second.\n",
      "PTBTokenizer tokenized 1846 tokens at 65424.03 tokens per second.\n",
      " 32%|      | 157/490 [01:35<03:35,  1.55it/s]PTBTokenizer tokenized 26 tokens at 1140.51 tokens per second.\n",
      "PTBTokenizer tokenized 3022 tokens at 95756.17 tokens per second.\n",
      " 32%|      | 158/490 [01:36<03:33,  1.55it/s]PTBTokenizer tokenized 30 tokens at 1339.76 tokens per second.\n",
      "PTBTokenizer tokenized 2002 tokens at 71313.07 tokens per second.\n",
      " 32%|      | 159/490 [01:36<03:31,  1.56it/s]PTBTokenizer tokenized 26 tokens at 1153.52 tokens per second.\n",
      "PTBTokenizer tokenized 744 tokens at 28771.36 tokens per second.\n",
      " 33%|      | 160/490 [01:37<03:33,  1.55it/s]PTBTokenizer tokenized 30 tokens at 1304.30 tokens per second.\n",
      "PTBTokenizer tokenized 2138 tokens at 76374.12 tokens per second.\n",
      " 33%|      | 161/490 [01:38<03:33,  1.54it/s]PTBTokenizer tokenized 13 tokens at 584.19 tokens per second.\n",
      "PTBTokenizer tokenized 1771 tokens at 66026.13 tokens per second.\n",
      " 33%|      | 162/490 [01:38<03:21,  1.63it/s]PTBTokenizer tokenized 13 tokens at 560.26 tokens per second.\n",
      "PTBTokenizer tokenized 1776 tokens at 65556.36 tokens per second.\n",
      " 33%|      | 163/490 [01:39<03:12,  1.70it/s]PTBTokenizer tokenized 14 tokens at 577.94 tokens per second.\n",
      "PTBTokenizer tokenized 449 tokens at 17592.00 tokens per second.\n",
      " 33%|      | 164/490 [01:39<03:03,  1.77it/s]PTBTokenizer tokenized 32 tokens at 1430.06 tokens per second.\n",
      "PTBTokenizer tokenized 1579 tokens at 57006.85 tokens per second.\n",
      " 34%|      | 165/490 [01:40<03:06,  1.74it/s]PTBTokenizer tokenized 13 tokens at 579.44 tokens per second.\n",
      "PTBTokenizer tokenized 2395 tokens at 85307.19 tokens per second.\n",
      " 34%|      | 166/490 [01:40<03:05,  1.74it/s]PTBTokenizer tokenized 29 tokens at 1305.20 tokens per second.\n",
      "PTBTokenizer tokenized 1491 tokens at 54823.96 tokens per second.\n",
      " 34%|      | 167/490 [01:41<03:07,  1.72it/s]PTBTokenizer tokenized 25 tokens at 1127.13 tokens per second.\n",
      "PTBTokenizer tokenized 1944 tokens at 68268.46 tokens per second.\n",
      " 34%|      | 168/490 [01:41<03:08,  1.71it/s]PTBTokenizer tokenized 38 tokens at 1677.39 tokens per second.\n",
      "PTBTokenizer tokenized 943 tokens at 36835.63 tokens per second.\n",
      " 34%|      | 169/490 [01:42<03:16,  1.63it/s]PTBTokenizer tokenized 10 tokens at 445.45 tokens per second.\n",
      "PTBTokenizer tokenized 2183 tokens at 77542.09 tokens per second.\n",
      " 35%|      | 170/490 [01:43<03:06,  1.72it/s]PTBTokenizer tokenized 22 tokens at 988.73 tokens per second.\n",
      "PTBTokenizer tokenized 801 tokens at 31841.53 tokens per second.\n",
      " 35%|      | 171/490 [01:43<03:12,  1.65it/s]PTBTokenizer tokenized 13 tokens at 573.21 tokens per second.\n",
      "PTBTokenizer tokenized 2104 tokens at 74988.74 tokens per second.\n",
      " 35%|      | 172/490 [01:44<03:16,  1.62it/s]PTBTokenizer tokenized 11 tokens at 495.49 tokens per second.\n",
      "PTBTokenizer tokenized 1012 tokens at 39587.13 tokens per second.\n",
      " 35%|      | 173/490 [01:45<03:05,  1.71it/s]PTBTokenizer tokenized 19 tokens at 841.77 tokens per second.\n",
      "PTBTokenizer tokenized 1632 tokens at 61586.99 tokens per second.\n",
      " 36%|      | 174/490 [01:45<03:01,  1.74it/s]PTBTokenizer tokenized 20 tokens at 900.49 tokens per second.\n",
      "PTBTokenizer tokenized 2067 tokens at 74946.33 tokens per second.\n",
      " 36%|      | 175/490 [01:46<03:00,  1.74it/s]PTBTokenizer tokenized 15 tokens at 651.19 tokens per second.\n",
      "PTBTokenizer tokenized 2093 tokens at 70530.13 tokens per second.\n",
      " 36%|      | 176/490 [01:46<03:03,  1.71it/s]PTBTokenizer tokenized 18 tokens at 797.81 tokens per second.\n",
      "PTBTokenizer tokenized 2390 tokens at 83558.42 tokens per second.\n",
      " 36%|      | 177/490 [01:47<03:06,  1.68it/s]PTBTokenizer tokenized 18 tokens at 805.04 tokens per second.\n",
      "PTBTokenizer tokenized 1535 tokens at 56067.93 tokens per second.\n",
      " 36%|      | 178/490 [01:47<03:06,  1.67it/s]PTBTokenizer tokenized 38 tokens at 1656.81 tokens per second.\n",
      "PTBTokenizer tokenized 1490 tokens at 53737.19 tokens per second.\n",
      " 37%|      | 179/490 [01:48<03:16,  1.58it/s]PTBTokenizer tokenized 27 tokens at 1177.46 tokens per second.\n",
      "PTBTokenizer tokenized 1653 tokens at 60520.28 tokens per second.\n",
      " 37%|      | 180/490 [01:49<03:25,  1.51it/s]PTBTokenizer tokenized 22 tokens at 958.48 tokens per second.\n",
      "PTBTokenizer tokenized 877 tokens at 34608.74 tokens per second.\n",
      " 37%|      | 181/490 [01:49<03:18,  1.56it/s]PTBTokenizer tokenized 30 tokens at 1297.90 tokens per second.\n",
      "PTBTokenizer tokenized 1670 tokens at 60761.26 tokens per second.\n",
      " 37%|      | 182/490 [01:50<03:24,  1.51it/s]PTBTokenizer tokenized 33 tokens at 1441.06 tokens per second.\n",
      "PTBTokenizer tokenized 2023 tokens at 73302.39 tokens per second.\n",
      " 37%|      | 183/490 [01:51<03:32,  1.45it/s]PTBTokenizer tokenized 34 tokens at 1489.98 tokens per second.\n",
      "PTBTokenizer tokenized 2138 tokens at 76276.91 tokens per second.\n",
      " 38%|      | 184/490 [01:52<03:36,  1.42it/s]PTBTokenizer tokenized 11 tokens at 483.93 tokens per second.\n",
      "PTBTokenizer tokenized 1304 tokens at 49186.48 tokens per second.\n",
      " 38%|      | 185/490 [01:52<03:22,  1.50it/s]PTBTokenizer tokenized 15 tokens at 670.59 tokens per second.\n",
      "PTBTokenizer tokenized 1146 tokens at 44018.81 tokens per second.\n",
      " 38%|      | 186/490 [01:53<03:22,  1.50it/s]PTBTokenizer tokenized 33 tokens at 1509.10 tokens per second.\n",
      "PTBTokenizer tokenized 1550 tokens at 57017.67 tokens per second.\n",
      " 38%|      | 187/490 [01:54<03:22,  1.50it/s]PTBTokenizer tokenized 23 tokens at 1015.31 tokens per second.\n",
      "PTBTokenizer tokenized 1472 tokens at 52761.41 tokens per second.\n",
      " 38%|      | 188/490 [01:54<03:28,  1.45it/s]PTBTokenizer tokenized 27 tokens at 1168.82 tokens per second.\n",
      "PTBTokenizer tokenized 1223 tokens at 45939.24 tokens per second.\n",
      " 39%|      | 189/490 [01:55<03:30,  1.43it/s]PTBTokenizer tokenized 24 tokens at 1066.45 tokens per second.\n",
      "PTBTokenizer tokenized 1368 tokens at 51077.31 tokens per second.\n",
      " 39%|      | 190/490 [01:56<03:25,  1.46it/s]PTBTokenizer tokenized 30 tokens at 1324.82 tokens per second.\n",
      "PTBTokenizer tokenized 292 tokens at 12265.56 tokens per second.\n",
      " 39%|      | 191/490 [01:56<03:23,  1.47it/s]PTBTokenizer tokenized 17 tokens at 753.24 tokens per second.\n",
      "PTBTokenizer tokenized 1993 tokens at 70647.28 tokens per second.\n",
      " 39%|      | 192/490 [01:57<03:21,  1.48it/s]PTBTokenizer tokenized 30 tokens at 1326.44 tokens per second.\n",
      "PTBTokenizer tokenized 1560 tokens at 57758.63 tokens per second.\n",
      " 39%|      | 193/490 [01:58<03:29,  1.42it/s]PTBTokenizer tokenized 8 tokens at 352.84 tokens per second.\n",
      "PTBTokenizer tokenized 1736 tokens at 63644.44 tokens per second.\n",
      " 40%|      | 194/490 [01:58<03:02,  1.63it/s]PTBTokenizer tokenized 26 tokens at 1143.23 tokens per second.\n",
      "PTBTokenizer tokenized 1210 tokens at 45283.80 tokens per second.\n",
      " 40%|      | 195/490 [01:59<03:14,  1.52it/s]PTBTokenizer tokenized 14 tokens at 611.84 tokens per second.\n",
      "PTBTokenizer tokenized 2033 tokens at 71368.15 tokens per second.\n",
      " 40%|      | 196/490 [02:00<03:04,  1.60it/s]PTBTokenizer tokenized 22 tokens at 962.72 tokens per second.\n",
      "PTBTokenizer tokenized 1673 tokens at 62828.05 tokens per second.\n",
      " 40%|      | 197/490 [02:00<03:01,  1.61it/s]PTBTokenizer tokenized 21 tokens at 915.06 tokens per second.\n",
      "PTBTokenizer tokenized 1098 tokens at 39056.17 tokens per second.\n",
      " 40%|      | 198/490 [02:01<03:02,  1.60it/s]PTBTokenizer tokenized 21 tokens at 910.81 tokens per second.\n",
      "PTBTokenizer tokenized 1202 tokens at 46452.57 tokens per second.\n",
      " 41%|      | 199/490 [02:01<02:56,  1.65it/s]PTBTokenizer tokenized 22 tokens at 968.74 tokens per second.\n",
      "PTBTokenizer tokenized 269 tokens at 11229.09 tokens per second.\n",
      " 41%|      | 200/490 [02:02<02:58,  1.63it/s]PTBTokenizer tokenized 29 tokens at 1300.61 tokens per second.\n",
      "PTBTokenizer tokenized 1581 tokens at 58296.19 tokens per second.\n",
      " 41%|      | 201/490 [02:03<03:06,  1.55it/s]PTBTokenizer tokenized 25 tokens at 1155.40 tokens per second.\n",
      "PTBTokenizer tokenized 3081 tokens at 106133.98 tokens per second.\n",
      " 41%|      | 202/490 [02:03<03:09,  1.52it/s]PTBTokenizer tokenized 24 tokens at 1018.50 tokens per second.\n",
      "PTBTokenizer tokenized 1475 tokens at 56215.16 tokens per second.\n",
      " 41%|     | 203/490 [02:04<03:15,  1.47it/s]PTBTokenizer tokenized 10 tokens at 456.25 tokens per second.\n",
      "PTBTokenizer tokenized 416 tokens at 17114.37 tokens per second.\n",
      " 42%|     | 204/490 [02:05<03:03,  1.56it/s]PTBTokenizer tokenized 19 tokens at 815.79 tokens per second.\n",
      "PTBTokenizer tokenized 2267 tokens at 83853.58 tokens per second.\n",
      " 42%|     | 205/490 [02:05<02:59,  1.59it/s]PTBTokenizer tokenized 25 tokens at 1144.52 tokens per second.\n",
      "PTBTokenizer tokenized 1508 tokens at 56119.87 tokens per second.\n",
      " 42%|     | 206/490 [02:06<02:55,  1.62it/s]PTBTokenizer tokenized 25 tokens at 1085.45 tokens per second.\n",
      "PTBTokenizer tokenized 1638 tokens at 59637.93 tokens per second.\n",
      " 42%|     | 207/490 [02:07<03:07,  1.51it/s]PTBTokenizer tokenized 32 tokens at 1431.27 tokens per second.\n",
      "PTBTokenizer tokenized 1882 tokens at 68544.53 tokens per second.\n",
      " 42%|     | 208/490 [02:07<03:14,  1.45it/s]PTBTokenizer tokenized 31 tokens at 1380.44 tokens per second.\n",
      "PTBTokenizer tokenized 1717 tokens at 64269.25 tokens per second.\n",
      " 43%|     | 209/490 [02:08<03:16,  1.43it/s]PTBTokenizer tokenized 17 tokens at 743.85 tokens per second.\n",
      "PTBTokenizer tokenized 2414 tokens at 85198.75 tokens per second.\n",
      " 43%|     | 210/490 [02:09<02:57,  1.58it/s]PTBTokenizer tokenized 12 tokens at 520.96 tokens per second.\n",
      "PTBTokenizer tokenized 1873 tokens at 68027.32 tokens per second.\n",
      " 43%|     | 211/490 [02:09<02:49,  1.65it/s]PTBTokenizer tokenized 19 tokens at 842.11 tokens per second.\n",
      "PTBTokenizer tokenized 2998 tokens at 98509.01 tokens per second.\n",
      " 43%|     | 212/490 [02:10<02:48,  1.65it/s]PTBTokenizer tokenized 43 tokens at 1835.56 tokens per second.\n",
      "PTBTokenizer tokenized 537 tokens at 20728.03 tokens per second.\n",
      " 43%|     | 213/490 [02:10<02:55,  1.58it/s]PTBTokenizer tokenized 24 tokens at 1056.20 tokens per second.\n",
      "PTBTokenizer tokenized 2371 tokens at 79498.11 tokens per second.\n",
      " 44%|     | 214/490 [02:11<02:55,  1.57it/s]PTBTokenizer tokenized 28 tokens at 1219.61 tokens per second.\n",
      "PTBTokenizer tokenized 1747 tokens at 63026.58 tokens per second.\n",
      " 44%|     | 215/490 [02:12<02:52,  1.59it/s]PTBTokenizer tokenized 18 tokens at 762.31 tokens per second.\n",
      "PTBTokenizer tokenized 1812 tokens at 65744.98 tokens per second.\n",
      " 44%|     | 216/490 [02:12<02:49,  1.62it/s]PTBTokenizer tokenized 27 tokens at 1188.40 tokens per second.\n",
      "PTBTokenizer tokenized 706 tokens at 26385.83 tokens per second.\n",
      " 44%|     | 217/490 [02:13<02:53,  1.57it/s]PTBTokenizer tokenized 10 tokens at 451.23 tokens per second.\n",
      "PTBTokenizer tokenized 1752 tokens at 64328.02 tokens per second.\n",
      " 44%|     | 218/490 [02:14<02:48,  1.61it/s]PTBTokenizer tokenized 27 tokens at 1199.57 tokens per second.\n",
      "PTBTokenizer tokenized 1162 tokens at 43187.30 tokens per second.\n",
      " 45%|     | 219/490 [02:14<02:55,  1.55it/s]PTBTokenizer tokenized 26 tokens at 1152.68 tokens per second.\n",
      "PTBTokenizer tokenized 2796 tokens at 96233.10 tokens per second.\n",
      " 45%|     | 220/490 [02:15<02:50,  1.58it/s]PTBTokenizer tokenized 12 tokens at 517.60 tokens per second.\n",
      "PTBTokenizer tokenized 140 tokens at 6054.84 tokens per second.\n",
      " 45%|     | 221/490 [02:15<02:36,  1.71it/s]PTBTokenizer tokenized 14 tokens at 622.69 tokens per second.\n",
      "PTBTokenizer tokenized 14 tokens at 586.43 tokens per second.\n",
      " 45%|     | 222/490 [02:16<02:30,  1.78it/s]PTBTokenizer tokenized 12 tokens at 521.89 tokens per second.\n",
      "PTBTokenizer tokenized 2509 tokens at 87515.08 tokens per second.\n",
      " 46%|     | 223/490 [02:16<02:31,  1.76it/s]PTBTokenizer tokenized 17 tokens at 743.80 tokens per second.\n",
      "PTBTokenizer tokenized 1788 tokens at 64420.29 tokens per second.\n",
      " 46%|     | 224/490 [02:17<02:37,  1.69it/s]PTBTokenizer tokenized 17 tokens at 755.38 tokens per second.\n",
      "PTBTokenizer tokenized 2223 tokens at 79421.48 tokens per second.\n",
      " 46%|     | 225/490 [02:18<02:37,  1.68it/s]PTBTokenizer tokenized 12 tokens at 516.94 tokens per second.\n",
      "PTBTokenizer tokenized 1471 tokens at 52126.86 tokens per second.\n",
      " 46%|     | 226/490 [02:18<02:28,  1.78it/s]PTBTokenizer tokenized 23 tokens at 1001.97 tokens per second.\n",
      "PTBTokenizer tokenized 2819 tokens at 93571.45 tokens per second.\n",
      " 46%|     | 227/490 [02:19<02:41,  1.63it/s]PTBTokenizer tokenized 33 tokens at 1455.10 tokens per second.\n",
      "PTBTokenizer tokenized 373 tokens at 15346.39 tokens per second.\n",
      " 47%|     | 228/490 [02:20<02:45,  1.58it/s]PTBTokenizer tokenized 18 tokens at 807.55 tokens per second.\n",
      "PTBTokenizer tokenized 1871 tokens at 65873.91 tokens per second.\n",
      " 47%|     | 229/490 [02:20<02:44,  1.59it/s]PTBTokenizer tokenized 15 tokens at 668.31 tokens per second.\n",
      "PTBTokenizer tokenized 1396 tokens at 49246.31 tokens per second.\n",
      " 47%|     | 230/490 [02:21<02:33,  1.69it/s]PTBTokenizer tokenized 15 tokens at 669.03 tokens per second.\n",
      "PTBTokenizer tokenized 1096 tokens at 39553.66 tokens per second.\n",
      " 47%|     | 231/490 [02:21<02:31,  1.71it/s]PTBTokenizer tokenized 16 tokens at 698.20 tokens per second.\n",
      "PTBTokenizer tokenized 2258 tokens at 80756.69 tokens per second.\n",
      " 47%|     | 232/490 [02:22<02:36,  1.65it/s]PTBTokenizer tokenized 15 tokens at 680.74 tokens per second.\n",
      "PTBTokenizer tokenized 2200 tokens at 79037.44 tokens per second.\n",
      " 48%|     | 233/490 [02:23<02:35,  1.65it/s]PTBTokenizer tokenized 15 tokens at 642.18 tokens per second.\n",
      "PTBTokenizer tokenized 946 tokens at 34836.05 tokens per second.\n",
      " 48%|     | 234/490 [02:23<02:33,  1.67it/s]PTBTokenizer tokenized 38 tokens at 1701.56 tokens per second.\n",
      "PTBTokenizer tokenized 1678 tokens at 60278.20 tokens per second.\n",
      " 48%|     | 235/490 [02:24<02:41,  1.58it/s]PTBTokenizer tokenized 20 tokens at 880.32 tokens per second.\n",
      "PTBTokenizer tokenized 1831 tokens at 66667.93 tokens per second.\n",
      " 48%|     | 236/490 [02:24<02:29,  1.70it/s]PTBTokenizer tokenized 30 tokens at 1304.75 tokens per second.\n",
      "PTBTokenizer tokenized 2038 tokens at 73823.56 tokens per second.\n",
      " 48%|     | 237/490 [02:25<02:39,  1.58it/s]PTBTokenizer tokenized 12 tokens at 533.19 tokens per second.\n",
      "PTBTokenizer tokenized 1655 tokens at 59466.40 tokens per second.\n",
      " 49%|     | 238/490 [02:26<02:34,  1.63it/s]PTBTokenizer tokenized 19 tokens at 844.13 tokens per second.\n",
      "PTBTokenizer tokenized 1241 tokens at 47102.16 tokens per second.\n",
      " 49%|     | 239/490 [02:26<02:35,  1.62it/s]PTBTokenizer tokenized 16 tokens at 711.91 tokens per second.\n",
      "PTBTokenizer tokenized 1456 tokens at 55373.62 tokens per second.\n",
      " 49%|     | 240/490 [02:27<02:31,  1.65it/s]PTBTokenizer tokenized 14 tokens at 615.16 tokens per second.\n",
      "PTBTokenizer tokenized 1596 tokens at 58416.99 tokens per second.\n",
      " 49%|     | 241/490 [02:27<02:25,  1.72it/s]PTBTokenizer tokenized 15 tokens at 656.01 tokens per second.\n",
      "PTBTokenizer tokenized 2578 tokens at 87610.78 tokens per second.\n",
      " 49%|     | 242/490 [02:28<02:25,  1.70it/s]PTBTokenizer tokenized 21 tokens at 920.50 tokens per second.\n",
      "PTBTokenizer tokenized 479 tokens at 19161.33 tokens per second.\n",
      " 50%|     | 243/490 [02:29<02:29,  1.65it/s]PTBTokenizer tokenized 29 tokens at 1259.22 tokens per second.\n",
      "PTBTokenizer tokenized 1622 tokens at 59140.21 tokens per second.\n",
      " 50%|     | 244/490 [02:29<02:33,  1.60it/s]PTBTokenizer tokenized 25 tokens at 1103.49 tokens per second.\n",
      "PTBTokenizer tokenized 2017 tokens at 70158.45 tokens per second.\n",
      " 50%|     | 245/490 [02:30<02:33,  1.59it/s]PTBTokenizer tokenized 31 tokens at 1350.17 tokens per second.\n",
      "PTBTokenizer tokenized 64 tokens at 2767.57 tokens per second.\n",
      " 50%|     | 246/490 [02:31<02:39,  1.53it/s]PTBTokenizer tokenized 12 tokens at 490.76 tokens per second.\n",
      "PTBTokenizer tokenized 854 tokens at 33571.36 tokens per second.\n",
      " 50%|     | 247/490 [02:31<02:31,  1.60it/s]PTBTokenizer tokenized 23 tokens at 1026.09 tokens per second.\n",
      "PTBTokenizer tokenized 2160 tokens at 76186.41 tokens per second.\n",
      " 51%|     | 248/490 [02:32<02:37,  1.53it/s]PTBTokenizer tokenized 22 tokens at 980.80 tokens per second.\n",
      "PTBTokenizer tokenized 2687 tokens at 93000.91 tokens per second.\n",
      " 51%|     | 249/490 [02:33<02:36,  1.54it/s]PTBTokenizer tokenized 24 tokens at 1046.71 tokens per second.\n",
      "PTBTokenizer tokenized 2637 tokens at 87913.70 tokens per second.\n",
      " 51%|     | 250/490 [02:33<02:42,  1.48it/s]PTBTokenizer tokenized 17 tokens at 760.53 tokens per second.\n",
      "PTBTokenizer tokenized 2291 tokens at 82218.38 tokens per second.\n",
      " 51%|     | 251/490 [02:34<02:43,  1.46it/s]PTBTokenizer tokenized 14 tokens at 625.36 tokens per second.\n",
      "PTBTokenizer tokenized 1967 tokens at 71214.42 tokens per second.\n",
      " 51%|    | 252/490 [02:35<02:35,  1.53it/s]PTBTokenizer tokenized 27 tokens at 1225.15 tokens per second.\n",
      "PTBTokenizer tokenized 1416 tokens at 49433.70 tokens per second.\n",
      " 52%|    | 253/490 [02:35<02:39,  1.49it/s]PTBTokenizer tokenized 36 tokens at 1561.81 tokens per second.\n",
      "PTBTokenizer tokenized 1467 tokens at 55055.12 tokens per second.\n",
      " 52%|    | 254/490 [02:36<02:42,  1.45it/s]PTBTokenizer tokenized 19 tokens at 853.72 tokens per second.\n",
      "PTBTokenizer tokenized 2139 tokens at 75241.83 tokens per second.\n",
      " 52%|    | 255/490 [02:37<02:34,  1.52it/s]PTBTokenizer tokenized 33 tokens at 1451.57 tokens per second.\n",
      "PTBTokenizer tokenized 1362 tokens at 51275.69 tokens per second.\n",
      " 52%|    | 256/490 [02:37<02:37,  1.48it/s]PTBTokenizer tokenized 12 tokens at 524.15 tokens per second.\n",
      "PTBTokenizer tokenized 1676 tokens at 61363.31 tokens per second.\n",
      " 52%|    | 257/490 [02:38<02:36,  1.49it/s]PTBTokenizer tokenized 16 tokens at 706.90 tokens per second.\n",
      "PTBTokenizer tokenized 1689 tokens at 61150.53 tokens per second.\n",
      " 53%|    | 258/490 [02:39<02:35,  1.49it/s]PTBTokenizer tokenized 12 tokens at 539.08 tokens per second.\n",
      "PTBTokenizer tokenized 1794 tokens at 66361.93 tokens per second.\n",
      " 53%|    | 259/490 [02:39<02:33,  1.50it/s]PTBTokenizer tokenized 14 tokens at 616.45 tokens per second.\n",
      "PTBTokenizer tokenized 617 tokens at 24563.88 tokens per second.\n",
      " 53%|    | 260/490 [02:40<02:32,  1.51it/s]PTBTokenizer tokenized 11 tokens at 475.40 tokens per second.\n",
      "PTBTokenizer tokenized 352 tokens at 14626.20 tokens per second.\n",
      " 53%|    | 261/490 [02:40<02:15,  1.69it/s]PTBTokenizer tokenized 16 tokens at 710.62 tokens per second.\n",
      "PTBTokenizer tokenized 1819 tokens at 65232.90 tokens per second.\n",
      " 53%|    | 262/490 [02:41<02:24,  1.58it/s]PTBTokenizer tokenized 34 tokens at 1486.77 tokens per second.\n",
      "PTBTokenizer tokenized 3066 tokens at 100425.04 tokens per second.\n",
      " 54%|    | 263/490 [02:42<02:31,  1.50it/s]PTBTokenizer tokenized 31 tokens at 1345.45 tokens per second.\n",
      "PTBTokenizer tokenized 1352 tokens at 50277.23 tokens per second.\n",
      " 54%|    | 264/490 [02:43<02:32,  1.48it/s]PTBTokenizer tokenized 33 tokens at 1437.82 tokens per second.\n",
      "PTBTokenizer tokenized 2107 tokens at 72549.95 tokens per second.\n",
      " 54%|    | 265/490 [02:43<02:32,  1.47it/s]PTBTokenizer tokenized 15 tokens at 663.11 tokens per second.\n",
      "PTBTokenizer tokenized 2240 tokens at 79857.17 tokens per second.\n",
      " 54%|    | 266/490 [02:44<02:20,  1.59it/s]PTBTokenizer tokenized 28 tokens at 1258.06 tokens per second.\n",
      "PTBTokenizer tokenized 1549 tokens at 57075.56 tokens per second.\n",
      " 54%|    | 267/490 [02:44<02:25,  1.53it/s]PTBTokenizer tokenized 26 tokens at 1131.95 tokens per second.\n",
      "PTBTokenizer tokenized 1947 tokens at 70738.66 tokens per second.\n",
      " 55%|    | 268/490 [02:45<02:30,  1.47it/s]PTBTokenizer tokenized 33 tokens at 1399.25 tokens per second.\n",
      "PTBTokenizer tokenized 2055 tokens at 74707.61 tokens per second.\n",
      " 55%|    | 269/490 [02:46<02:33,  1.44it/s]PTBTokenizer tokenized 15 tokens at 674.37 tokens per second.\n",
      "PTBTokenizer tokenized 1793 tokens at 64370.55 tokens per second.\n",
      " 55%|    | 270/490 [02:47<02:28,  1.48it/s]PTBTokenizer tokenized 14 tokens at 612.32 tokens per second.\n",
      "PTBTokenizer tokenized 3034 tokens at 101321.42 tokens per second.\n",
      " 55%|    | 271/490 [02:47<02:18,  1.58it/s]PTBTokenizer tokenized 11 tokens at 482.63 tokens per second.\n",
      "PTBTokenizer tokenized 1355 tokens at 50046.72 tokens per second.\n",
      " 56%|    | 272/490 [02:48<02:05,  1.74it/s]PTBTokenizer tokenized 12 tokens at 539.91 tokens per second.\n",
      "PTBTokenizer tokenized 1107 tokens at 41230.99 tokens per second.\n",
      " 56%|    | 273/490 [02:48<02:04,  1.75it/s]PTBTokenizer tokenized 30 tokens at 1301.26 tokens per second.\n",
      "PTBTokenizer tokenized 1774 tokens at 61910.03 tokens per second.\n",
      " 56%|    | 274/490 [02:49<02:10,  1.65it/s]PTBTokenizer tokenized 12 tokens at 527.49 tokens per second.\n",
      "PTBTokenizer tokenized 1635 tokens at 60240.95 tokens per second.\n",
      " 56%|    | 275/490 [02:49<02:07,  1.69it/s]PTBTokenizer tokenized 13 tokens at 582.34 tokens per second.\n",
      "PTBTokenizer tokenized 1628 tokens at 60806.28 tokens per second.\n",
      " 56%|    | 276/490 [02:50<02:02,  1.75it/s]PTBTokenizer tokenized 30 tokens at 1316.45 tokens per second.\n",
      "PTBTokenizer tokenized 544 tokens at 21562.62 tokens per second.\n",
      " 57%|    | 277/490 [02:51<02:11,  1.62it/s]PTBTokenizer tokenized 32 tokens at 1402.61 tokens per second.\n",
      "PTBTokenizer tokenized 2020 tokens at 71011.53 tokens per second.\n",
      " 57%|    | 278/490 [02:51<02:12,  1.60it/s]PTBTokenizer tokenized 19 tokens at 829.71 tokens per second.\n",
      "PTBTokenizer tokenized 1744 tokens at 64357.67 tokens per second.\n",
      " 57%|    | 279/490 [02:52<02:09,  1.63it/s]PTBTokenizer tokenized 22 tokens at 959.98 tokens per second.\n",
      "PTBTokenizer tokenized 2164 tokens at 72792.89 tokens per second.\n",
      " 57%|    | 280/490 [02:52<02:05,  1.67it/s]PTBTokenizer tokenized 15 tokens at 661.71 tokens per second.\n",
      "PTBTokenizer tokenized 423 tokens at 17207.18 tokens per second.\n",
      " 57%|    | 281/490 [02:53<02:05,  1.67it/s]PTBTokenizer tokenized 18 tokens at 804.48 tokens per second.\n",
      "PTBTokenizer tokenized 1482 tokens at 55217.55 tokens per second.\n",
      " 58%|    | 282/490 [02:54<02:10,  1.59it/s]PTBTokenizer tokenized 20 tokens at 905.13 tokens per second.\n",
      "PTBTokenizer tokenized 2014 tokens at 72700.15 tokens per second.\n",
      " 58%|    | 283/490 [02:54<02:11,  1.57it/s]PTBTokenizer tokenized 20 tokens at 920.02 tokens per second.\n",
      "PTBTokenizer tokenized 2437 tokens at 83656.52 tokens per second.\n",
      " 58%|    | 284/490 [02:55<02:16,  1.51it/s]PTBTokenizer tokenized 23 tokens at 1012.68 tokens per second.\n",
      "PTBTokenizer tokenized 1544 tokens at 57576.68 tokens per second.\n",
      " 58%|    | 285/490 [02:56<02:20,  1.46it/s]PTBTokenizer tokenized 13 tokens at 573.26 tokens per second.\n",
      "PTBTokenizer tokenized 2022 tokens at 73605.86 tokens per second.\n",
      " 58%|    | 286/490 [02:56<02:12,  1.54it/s]PTBTokenizer tokenized 12 tokens at 505.98 tokens per second.\n",
      "PTBTokenizer tokenized 672 tokens at 26156.09 tokens per second.\n",
      " 59%|    | 287/490 [02:57<01:58,  1.72it/s]PTBTokenizer tokenized 26 tokens at 1174.78 tokens per second.\n",
      "PTBTokenizer tokenized 1569 tokens at 58687.56 tokens per second.\n",
      " 59%|    | 288/490 [02:57<02:06,  1.60it/s]PTBTokenizer tokenized 27 tokens at 1197.49 tokens per second.\n",
      "PTBTokenizer tokenized 1793 tokens at 64355.76 tokens per second.\n",
      " 59%|    | 289/490 [02:58<02:11,  1.53it/s]PTBTokenizer tokenized 19 tokens at 837.65 tokens per second.\n",
      "PTBTokenizer tokenized 1852 tokens at 65031.89 tokens per second.\n",
      " 59%|    | 290/490 [02:59<02:10,  1.53it/s]PTBTokenizer tokenized 14 tokens at 621.94 tokens per second.\n",
      "PTBTokenizer tokenized 1969 tokens at 69568.50 tokens per second.\n",
      " 59%|    | 291/490 [02:59<02:07,  1.56it/s]PTBTokenizer tokenized 22 tokens at 937.85 tokens per second.\n",
      "PTBTokenizer tokenized 1467 tokens at 54577.77 tokens per second.\n",
      " 60%|    | 292/490 [03:00<02:04,  1.59it/s]PTBTokenizer tokenized 33 tokens at 1481.49 tokens per second.\n",
      "PTBTokenizer tokenized 2041 tokens at 70717.43 tokens per second.\n",
      " 60%|    | 293/490 [03:01<02:10,  1.51it/s]PTBTokenizer tokenized 20 tokens at 889.15 tokens per second.\n",
      "PTBTokenizer tokenized 575 tokens at 22531.57 tokens per second.\n",
      " 60%|    | 294/490 [03:02<02:13,  1.47it/s]PTBTokenizer tokenized 11 tokens at 494.14 tokens per second.\n",
      "PTBTokenizer tokenized 2222 tokens at 80663.90 tokens per second.\n",
      " 60%|    | 295/490 [03:02<02:05,  1.55it/s]PTBTokenizer tokenized 18 tokens at 826.52 tokens per second.\n",
      "PTBTokenizer tokenized 7 tokens at 306.22 tokens per second.\n",
      " 60%|    | 296/490 [03:03<02:00,  1.61it/s]PTBTokenizer tokenized 28 tokens at 1220.25 tokens per second.\n",
      "PTBTokenizer tokenized 2798 tokens at 96468.63 tokens per second.\n",
      " 61%|    | 297/490 [03:03<02:07,  1.52it/s]PTBTokenizer tokenized 11 tokens at 485.44 tokens per second.\n",
      "PTBTokenizer tokenized 1854 tokens at 67401.77 tokens per second.\n",
      " 61%|    | 298/490 [03:04<02:02,  1.57it/s]PTBTokenizer tokenized 27 tokens at 1201.51 tokens per second.\n",
      "PTBTokenizer tokenized 2764 tokens at 93953.33 tokens per second.\n",
      " 61%|    | 299/490 [03:05<02:06,  1.50it/s]PTBTokenizer tokenized 10 tokens at 439.86 tokens per second.\n",
      "PTBTokenizer tokenized 919 tokens at 35039.72 tokens per second.\n",
      " 61%|    | 300/490 [03:05<01:59,  1.59it/s]PTBTokenizer tokenized 26 tokens at 1160.60 tokens per second.\n",
      "PTBTokenizer tokenized 2595 tokens at 89896.22 tokens per second.\n",
      " 61%|   | 301/490 [03:06<02:04,  1.52it/s]PTBTokenizer tokenized 30 tokens at 1246.11 tokens per second.\n",
      "PTBTokenizer tokenized 1464 tokens at 55757.35 tokens per second.\n",
      " 62%|   | 302/490 [03:07<02:09,  1.45it/s]PTBTokenizer tokenized 8 tokens at 353.91 tokens per second.\n",
      "PTBTokenizer tokenized 4125 tokens at 134124.31 tokens per second.\n",
      " 62%|   | 303/490 [03:07<01:53,  1.65it/s]PTBTokenizer tokenized 10 tokens at 438.15 tokens per second.\n",
      "PTBTokenizer tokenized 784 tokens at 30127.76 tokens per second.\n",
      " 62%|   | 304/490 [03:08<01:39,  1.86it/s]PTBTokenizer tokenized 13 tokens at 570.72 tokens per second.\n",
      "PTBTokenizer tokenized 1634 tokens at 59949.51 tokens per second.\n",
      " 62%|   | 305/490 [03:08<01:46,  1.74it/s]PTBTokenizer tokenized 33 tokens at 1475.48 tokens per second.\n",
      "PTBTokenizer tokenized 2175 tokens at 77917.09 tokens per second.\n",
      " 62%|   | 306/490 [03:09<01:51,  1.64it/s]PTBTokenizer tokenized 15 tokens at 662.82 tokens per second.\n",
      "PTBTokenizer tokenized 2124 tokens at 67794.62 tokens per second.\n",
      " 63%|   | 307/490 [03:09<01:47,  1.70it/s]PTBTokenizer tokenized 30 tokens at 1320.41 tokens per second.\n",
      "PTBTokenizer tokenized 100 tokens at 4222.76 tokens per second.\n",
      " 63%|   | 308/490 [03:10<01:54,  1.58it/s]PTBTokenizer tokenized 25 tokens at 1109.11 tokens per second.\n",
      "PTBTokenizer tokenized 2548 tokens at 87925.77 tokens per second.\n",
      " 63%|   | 309/490 [03:11<01:50,  1.64it/s]PTBTokenizer tokenized 28 tokens at 1220.87 tokens per second.\n",
      "PTBTokenizer tokenized 2429 tokens at 85816.46 tokens per second.\n",
      " 63%|   | 310/490 [03:11<01:51,  1.62it/s]PTBTokenizer tokenized 27 tokens at 1198.30 tokens per second.\n",
      "PTBTokenizer tokenized 3392 tokens at 111919.51 tokens per second.\n",
      " 63%|   | 311/490 [03:12<01:55,  1.55it/s]PTBTokenizer tokenized 14 tokens at 626.32 tokens per second.\n",
      "PTBTokenizer tokenized 2521 tokens at 87448.37 tokens per second.\n",
      " 64%|   | 312/490 [03:13<01:44,  1.70it/s]PTBTokenizer tokenized 31 tokens at 1389.29 tokens per second.\n",
      "PTBTokenizer tokenized 1681 tokens at 60816.39 tokens per second.\n",
      " 64%|   | 313/490 [03:13<01:49,  1.62it/s]PTBTokenizer tokenized 15 tokens at 652.64 tokens per second.\n",
      "PTBTokenizer tokenized 1921 tokens at 69008.30 tokens per second.\n",
      " 64%|   | 314/490 [03:14<01:54,  1.53it/s]PTBTokenizer tokenized 27 tokens at 1220.19 tokens per second.\n",
      "PTBTokenizer tokenized 3283 tokens at 112464.57 tokens per second.\n",
      " 64%|   | 315/490 [03:15<01:56,  1.50it/s]PTBTokenizer tokenized 9 tokens at 406.19 tokens per second.\n",
      "PTBTokenizer tokenized 165 tokens at 6670.77 tokens per second.\n",
      " 64%|   | 316/490 [03:15<01:54,  1.51it/s]PTBTokenizer tokenized 23 tokens at 1005.10 tokens per second.\n",
      "PTBTokenizer tokenized 3103 tokens at 105335.76 tokens per second.\n",
      " 65%|   | 317/490 [03:16<01:54,  1.51it/s]PTBTokenizer tokenized 12 tokens at 534.78 tokens per second.\n",
      "PTBTokenizer tokenized 1719 tokens at 61294.53 tokens per second.\n",
      " 65%|   | 318/490 [03:16<01:44,  1.64it/s]PTBTokenizer tokenized 17 tokens at 764.45 tokens per second.\n",
      "PTBTokenizer tokenized 546 tokens at 21003.84 tokens per second.\n",
      " 65%|   | 319/490 [03:17<01:40,  1.70it/s]PTBTokenizer tokenized 23 tokens at 1004.86 tokens per second.\n",
      "PTBTokenizer tokenized 2621 tokens at 85185.34 tokens per second.\n",
      " 65%|   | 320/490 [03:18<01:42,  1.65it/s]PTBTokenizer tokenized 15 tokens at 685.89 tokens per second.\n",
      "PTBTokenizer tokenized 535 tokens at 21873.05 tokens per second.\n",
      " 66%|   | 321/490 [03:18<01:35,  1.77it/s]PTBTokenizer tokenized 27 tokens at 1206.42 tokens per second.\n",
      "PTBTokenizer tokenized 2308 tokens at 82082.40 tokens per second.\n",
      " 66%|   | 322/490 [03:19<01:39,  1.69it/s]PTBTokenizer tokenized 17 tokens at 755.04 tokens per second.\n",
      "PTBTokenizer tokenized 1759 tokens at 63953.56 tokens per second.\n",
      " 66%|   | 323/490 [03:19<01:33,  1.79it/s]PTBTokenizer tokenized 32 tokens at 1414.73 tokens per second.\n",
      "PTBTokenizer tokenized 1457 tokens at 54489.87 tokens per second.\n",
      " 66%|   | 324/490 [03:20<01:37,  1.70it/s]PTBTokenizer tokenized 17 tokens at 745.52 tokens per second.\n",
      "PTBTokenizer tokenized 1070 tokens at 40930.94 tokens per second.\n",
      " 66%|   | 325/490 [03:21<01:43,  1.60it/s]PTBTokenizer tokenized 38 tokens at 1713.15 tokens per second.\n",
      "PTBTokenizer tokenized 1396 tokens at 51196.14 tokens per second.\n",
      " 67%|   | 326/490 [03:21<01:45,  1.55it/s]PTBTokenizer tokenized 17 tokens at 737.56 tokens per second.\n",
      "PTBTokenizer tokenized 2189 tokens at 78188.96 tokens per second.\n",
      " 67%|   | 327/490 [03:22<01:39,  1.63it/s]PTBTokenizer tokenized 27 tokens at 1205.75 tokens per second.\n",
      "PTBTokenizer tokenized 1353 tokens at 50985.83 tokens per second.\n",
      " 67%|   | 328/490 [03:22<01:39,  1.63it/s]PTBTokenizer tokenized 23 tokens at 1030.53 tokens per second.\n",
      "PTBTokenizer tokenized 965 tokens at 36495.63 tokens per second.\n",
      " 67%|   | 329/490 [03:23<01:41,  1.58it/s]PTBTokenizer tokenized 10 tokens at 450.70 tokens per second.\n",
      "PTBTokenizer tokenized 1689 tokens at 61155.21 tokens per second.\n",
      " 67%|   | 330/490 [03:24<01:39,  1.60it/s]PTBTokenizer tokenized 28 tokens at 1266.08 tokens per second.\n",
      "PTBTokenizer tokenized 410 tokens at 16926.67 tokens per second.\n",
      " 68%|   | 331/490 [03:24<01:46,  1.50it/s]PTBTokenizer tokenized 24 tokens at 1090.88 tokens per second.\n",
      "PTBTokenizer tokenized 2135 tokens at 76862.99 tokens per second.\n",
      " 68%|   | 332/490 [03:25<01:44,  1.51it/s]PTBTokenizer tokenized 30 tokens at 1354.22 tokens per second.\n",
      "PTBTokenizer tokenized 2224 tokens at 79746.37 tokens per second.\n",
      " 68%|   | 333/490 [03:26<01:46,  1.47it/s]PTBTokenizer tokenized 40 tokens at 1801.46 tokens per second.\n",
      "PTBTokenizer tokenized 1348 tokens at 49527.44 tokens per second.\n",
      " 68%|   | 334/490 [03:27<01:48,  1.44it/s]PTBTokenizer tokenized 19 tokens at 828.53 tokens per second.\n",
      "PTBTokenizer tokenized 2017 tokens at 72078.41 tokens per second.\n",
      " 68%|   | 335/490 [03:27<01:49,  1.42it/s]PTBTokenizer tokenized 38 tokens at 1657.89 tokens per second.\n",
      "PTBTokenizer tokenized 2170 tokens at 77780.83 tokens per second.\n",
      " 69%|   | 336/490 [03:28<01:51,  1.39it/s]PTBTokenizer tokenized 14 tokens at 622.07 tokens per second.\n",
      "PTBTokenizer tokenized 2029 tokens at 73616.87 tokens per second.\n",
      " 69%|   | 337/490 [03:29<01:42,  1.49it/s]PTBTokenizer tokenized 18 tokens at 791.14 tokens per second.\n",
      "PTBTokenizer tokenized 2693 tokens at 91087.98 tokens per second.\n",
      " 69%|   | 338/490 [03:29<01:40,  1.51it/s]PTBTokenizer tokenized 17 tokens at 749.76 tokens per second.\n",
      "PTBTokenizer tokenized 1722 tokens at 64525.83 tokens per second.\n",
      " 69%|   | 339/490 [03:30<01:40,  1.51it/s]PTBTokenizer tokenized 42 tokens at 1903.87 tokens per second.\n",
      "PTBTokenizer tokenized 2578 tokens at 91403.29 tokens per second.\n",
      " 69%|   | 340/490 [03:31<01:43,  1.45it/s]PTBTokenizer tokenized 24 tokens at 1064.38 tokens per second.\n",
      "PTBTokenizer tokenized 3012 tokens at 100860.64 tokens per second.\n",
      " 70%|   | 341/490 [03:31<01:42,  1.45it/s]PTBTokenizer tokenized 34 tokens at 1477.79 tokens per second.\n",
      "PTBTokenizer tokenized 1143 tokens at 40868.39 tokens per second.\n",
      " 70%|   | 342/490 [03:32<01:44,  1.42it/s]PTBTokenizer tokenized 34 tokens at 1468.36 tokens per second.\n",
      "PTBTokenizer tokenized 2500 tokens at 85457.75 tokens per second.\n",
      " 70%|   | 343/490 [03:33<01:44,  1.41it/s]PTBTokenizer tokenized 10 tokens at 442.09 tokens per second.\n",
      "PTBTokenizer tokenized 1392 tokens at 52843.10 tokens per second.\n",
      " 70%|   | 344/490 [03:33<01:33,  1.57it/s]PTBTokenizer tokenized 22 tokens at 951.51 tokens per second.\n",
      "PTBTokenizer tokenized 1698 tokens at 62229.23 tokens per second.\n",
      " 70%|   | 345/490 [03:34<01:37,  1.49it/s]PTBTokenizer tokenized 13 tokens at 581.09 tokens per second.\n",
      "PTBTokenizer tokenized 1695 tokens at 61617.23 tokens per second.\n",
      " 71%|   | 346/490 [03:35<01:32,  1.55it/s]PTBTokenizer tokenized 15 tokens at 660.52 tokens per second.\n",
      "PTBTokenizer tokenized 1877 tokens at 67892.21 tokens per second.\n",
      " 71%|   | 347/490 [03:35<01:29,  1.60it/s]PTBTokenizer tokenized 15 tokens at 669.01 tokens per second.\n",
      "PTBTokenizer tokenized 2439 tokens at 81458.73 tokens per second.\n",
      " 71%|   | 348/490 [03:36<01:31,  1.56it/s]PTBTokenizer tokenized 21 tokens at 929.96 tokens per second.\n",
      "PTBTokenizer tokenized 2095 tokens at 77610.59 tokens per second.\n",
      " 71%|   | 349/490 [03:37<01:31,  1.54it/s]PTBTokenizer tokenized 11 tokens at 484.49 tokens per second.\n",
      "PTBTokenizer tokenized 1146 tokens at 43837.33 tokens per second.\n",
      " 71%|  | 350/490 [03:37<01:28,  1.59it/s]PTBTokenizer tokenized 22 tokens at 961.13 tokens per second.\n",
      "PTBTokenizer tokenized 1560 tokens at 57441.03 tokens per second.\n",
      " 72%|  | 351/490 [03:38<01:30,  1.54it/s]PTBTokenizer tokenized 25 tokens at 1034.05 tokens per second.\n",
      "PTBTokenizer tokenized 2536 tokens at 89223.16 tokens per second.\n",
      " 72%|  | 352/490 [03:39<01:30,  1.53it/s]PTBTokenizer tokenized 23 tokens at 1020.96 tokens per second.\n",
      "PTBTokenizer tokenized 2440 tokens at 84814.17 tokens per second.\n",
      " 72%|  | 353/490 [03:39<01:31,  1.50it/s]PTBTokenizer tokenized 24 tokens at 1058.54 tokens per second.\n",
      "PTBTokenizer tokenized 1796 tokens at 65261.56 tokens per second.\n",
      " 72%|  | 354/490 [03:40<01:30,  1.50it/s]PTBTokenizer tokenized 20 tokens at 883.84 tokens per second.\n",
      "PTBTokenizer tokenized 1643 tokens at 60033.34 tokens per second.\n",
      " 72%|  | 355/490 [03:41<01:29,  1.51it/s]PTBTokenizer tokenized 28 tokens at 1252.78 tokens per second.\n",
      "PTBTokenizer tokenized 174 tokens at 7298.62 tokens per second.\n",
      " 73%|  | 356/490 [03:41<01:29,  1.51it/s]PTBTokenizer tokenized 37 tokens at 1616.40 tokens per second.\n",
      "PTBTokenizer tokenized 2049 tokens at 70538.79 tokens per second.\n",
      " 73%|  | 357/490 [03:42<01:32,  1.43it/s]PTBTokenizer tokenized 35 tokens at 1543.55 tokens per second.\n",
      "PTBTokenizer tokenized 695 tokens at 27503.27 tokens per second.\n",
      " 73%|  | 358/490 [03:43<01:30,  1.46it/s]PTBTokenizer tokenized 30 tokens at 1327.55 tokens per second.\n",
      "PTBTokenizer tokenized 159 tokens at 6715.23 tokens per second.\n",
      " 73%|  | 359/490 [03:43<01:30,  1.44it/s]PTBTokenizer tokenized 42 tokens at 1856.38 tokens per second.\n",
      "PTBTokenizer tokenized 533 tokens at 21261.33 tokens per second.\n",
      " 73%|  | 360/490 [03:44<01:32,  1.41it/s]PTBTokenizer tokenized 15 tokens at 660.21 tokens per second.\n",
      "PTBTokenizer tokenized 29 tokens at 1269.11 tokens per second.\n",
      " 74%|  | 361/490 [03:45<01:23,  1.54it/s]PTBTokenizer tokenized 15 tokens at 650.96 tokens per second.\n",
      "PTBTokenizer tokenized 2201 tokens at 74285.23 tokens per second.\n",
      " 74%|  | 362/490 [03:45<01:23,  1.52it/s]PTBTokenizer tokenized 18 tokens at 769.97 tokens per second.\n",
      "PTBTokenizer tokenized 1580 tokens at 57848.83 tokens per second.\n",
      " 74%|  | 363/490 [03:46<01:23,  1.51it/s]PTBTokenizer tokenized 25 tokens at 1119.34 tokens per second.\n",
      "PTBTokenizer tokenized 2518 tokens at 88926.58 tokens per second.\n",
      " 74%|  | 364/490 [03:47<01:25,  1.48it/s]PTBTokenizer tokenized 19 tokens at 826.27 tokens per second.\n",
      "PTBTokenizer tokenized 1169 tokens at 44322.89 tokens per second.\n",
      " 74%|  | 365/490 [03:47<01:23,  1.50it/s]PTBTokenizer tokenized 13 tokens at 553.22 tokens per second.\n",
      "PTBTokenizer tokenized 1642 tokens at 60029.32 tokens per second.\n",
      " 75%|  | 366/490 [03:48<01:19,  1.56it/s]PTBTokenizer tokenized 28 tokens at 1184.09 tokens per second.\n",
      "PTBTokenizer tokenized 1693 tokens at 60884.88 tokens per second.\n",
      " 75%|  | 367/490 [03:49<01:23,  1.47it/s]PTBTokenizer tokenized 27 tokens at 1177.74 tokens per second.\n",
      "PTBTokenizer tokenized 1239 tokens at 46259.81 tokens per second.\n",
      " 75%|  | 368/490 [03:49<01:26,  1.41it/s]PTBTokenizer tokenized 42 tokens at 1896.23 tokens per second.\n",
      "PTBTokenizer tokenized 263 tokens at 11021.09 tokens per second.\n",
      " 75%|  | 369/490 [03:50<01:26,  1.39it/s]PTBTokenizer tokenized 28 tokens at 1216.48 tokens per second.\n",
      "PTBTokenizer tokenized 2075 tokens at 73875.60 tokens per second.\n",
      " 76%|  | 370/490 [03:51<01:23,  1.45it/s]PTBTokenizer tokenized 39 tokens at 1709.90 tokens per second.\n",
      "PTBTokenizer tokenized 2223 tokens at 79042.60 tokens per second.\n",
      " 76%|  | 371/490 [03:52<01:24,  1.42it/s]PTBTokenizer tokenized 17 tokens at 743.06 tokens per second.\n",
      "PTBTokenizer tokenized 1849 tokens at 67924.27 tokens per second.\n",
      " 76%|  | 372/490 [03:52<01:19,  1.48it/s]PTBTokenizer tokenized 30 tokens at 1319.17 tokens per second.\n",
      "PTBTokenizer tokenized 1902 tokens at 67896.40 tokens per second.\n",
      " 76%|  | 373/490 [03:53<01:22,  1.43it/s]PTBTokenizer tokenized 26 tokens at 1150.68 tokens per second.\n",
      "PTBTokenizer tokenized 666 tokens at 25611.57 tokens per second.\n",
      " 76%|  | 374/490 [03:54<01:23,  1.40it/s]PTBTokenizer tokenized 26 tokens at 1141.62 tokens per second.\n",
      "PTBTokenizer tokenized 2174 tokens at 78937.04 tokens per second.\n",
      " 77%|  | 375/490 [03:54<01:23,  1.38it/s]PTBTokenizer tokenized 18 tokens at 751.68 tokens per second.\n",
      "PTBTokenizer tokenized 1706 tokens at 60751.73 tokens per second.\n",
      " 77%|  | 376/490 [03:55<01:22,  1.38it/s]PTBTokenizer tokenized 17 tokens at 755.15 tokens per second.\n",
      "PTBTokenizer tokenized 1443 tokens at 54387.08 tokens per second.\n",
      " 77%|  | 377/490 [03:56<01:19,  1.43it/s]PTBTokenizer tokenized 37 tokens at 1554.26 tokens per second.\n",
      "PTBTokenizer tokenized 2820 tokens at 98572.70 tokens per second.\n",
      " 77%|  | 378/490 [03:57<01:19,  1.41it/s]PTBTokenizer tokenized 11 tokens at 484.12 tokens per second.\n",
      "PTBTokenizer tokenized 1311 tokens at 49463.57 tokens per second.\n",
      " 77%|  | 379/490 [03:57<01:12,  1.52it/s]PTBTokenizer tokenized 43 tokens at 1856.05 tokens per second.\n",
      "PTBTokenizer tokenized 15 tokens at 664.47 tokens per second.\n",
      " 78%|  | 380/490 [03:58<01:14,  1.47it/s]PTBTokenizer tokenized 25 tokens at 1102.35 tokens per second.\n",
      "PTBTokenizer tokenized 2496 tokens at 87651.21 tokens per second.\n",
      " 78%|  | 381/490 [03:59<01:16,  1.42it/s]PTBTokenizer tokenized 30 tokens at 1277.66 tokens per second.\n",
      "PTBTokenizer tokenized 1519 tokens at 55847.72 tokens per second.\n",
      " 78%|  | 382/490 [03:59<01:16,  1.40it/s]PTBTokenizer tokenized 15 tokens at 661.29 tokens per second.\n",
      "PTBTokenizer tokenized 1614 tokens at 59028.45 tokens per second.\n",
      " 78%|  | 383/490 [04:00<01:17,  1.38it/s]PTBTokenizer tokenized 12 tokens at 535.45 tokens per second.\n",
      "PTBTokenizer tokenized 2433 tokens at 86808.64 tokens per second.\n",
      " 78%|  | 384/490 [04:01<01:13,  1.44it/s]PTBTokenizer tokenized 35 tokens at 1474.05 tokens per second.\n",
      "PTBTokenizer tokenized 1427 tokens at 49852.70 tokens per second.\n",
      " 79%|  | 385/490 [04:01<01:12,  1.44it/s]PTBTokenizer tokenized 12 tokens at 541.47 tokens per second.\n",
      "PTBTokenizer tokenized 2316 tokens at 79675.10 tokens per second.\n",
      " 79%|  | 386/490 [04:02<01:12,  1.44it/s]PTBTokenizer tokenized 14 tokens at 618.98 tokens per second.\n",
      "PTBTokenizer tokenized 1372 tokens at 51908.82 tokens per second.\n",
      " 79%|  | 387/490 [04:03<01:05,  1.57it/s]PTBTokenizer tokenized 22 tokens at 968.43 tokens per second.\n",
      "PTBTokenizer tokenized 1905 tokens at 68481.10 tokens per second.\n",
      " 79%|  | 388/490 [04:03<01:06,  1.54it/s]PTBTokenizer tokenized 21 tokens at 916.09 tokens per second.\n",
      "PTBTokenizer tokenized 2037 tokens at 72193.91 tokens per second.\n",
      " 79%|  | 389/490 [04:04<01:03,  1.59it/s]PTBTokenizer tokenized 24 tokens at 1024.48 tokens per second.\n",
      "PTBTokenizer tokenized 2242 tokens at 78100.72 tokens per second.\n",
      " 80%|  | 390/490 [04:04<01:01,  1.63it/s]PTBTokenizer tokenized 11 tokens at 482.55 tokens per second.\n",
      "PTBTokenizer tokenized 833 tokens at 32299.29 tokens per second.\n",
      " 80%|  | 391/490 [04:05<00:56,  1.74it/s]PTBTokenizer tokenized 14 tokens at 624.16 tokens per second.\n",
      "PTBTokenizer tokenized 1558 tokens at 55826.82 tokens per second.\n",
      " 80%|  | 392/490 [04:05<00:55,  1.77it/s]PTBTokenizer tokenized 33 tokens at 1454.29 tokens per second.\n",
      "PTBTokenizer tokenized 1529 tokens at 57613.32 tokens per second.\n",
      " 80%|  | 393/490 [04:06<00:56,  1.71it/s]PTBTokenizer tokenized 14 tokens at 629.69 tokens per second.\n",
      "PTBTokenizer tokenized 1303 tokens at 48271.01 tokens per second.\n",
      " 80%|  | 394/490 [04:07<00:53,  1.78it/s]PTBTokenizer tokenized 24 tokens at 1082.16 tokens per second.\n",
      "PTBTokenizer tokenized 4924 tokens at 155214.38 tokens per second.\n",
      " 81%|  | 395/490 [04:07<00:57,  1.65it/s]PTBTokenizer tokenized 19 tokens at 815.71 tokens per second.\n",
      "PTBTokenizer tokenized 2801 tokens at 93116.37 tokens per second.\n",
      " 81%|  | 396/490 [04:08<00:56,  1.66it/s]PTBTokenizer tokenized 21 tokens at 928.23 tokens per second.\n",
      "PTBTokenizer tokenized 1054 tokens at 36680.86 tokens per second.\n",
      " 81%|  | 397/490 [04:08<00:53,  1.73it/s]PTBTokenizer tokenized 35 tokens at 1574.40 tokens per second.\n",
      "PTBTokenizer tokenized 1845 tokens at 67480.91 tokens per second.\n",
      " 81%|  | 398/490 [04:09<00:54,  1.68it/s]PTBTokenizer tokenized 20 tokens at 898.97 tokens per second.\n",
      "PTBTokenizer tokenized 1044 tokens at 40710.89 tokens per second.\n",
      " 81%| | 399/490 [04:10<00:55,  1.63it/s]PTBTokenizer tokenized 13 tokens at 575.28 tokens per second.\n",
      "PTBTokenizer tokenized 1667 tokens at 60689.68 tokens per second.\n",
      " 82%| | 400/490 [04:10<00:52,  1.72it/s]PTBTokenizer tokenized 37 tokens at 1698.43 tokens per second.\n",
      "PTBTokenizer tokenized 1869 tokens at 64948.08 tokens per second.\n",
      " 82%| | 401/490 [04:11<00:54,  1.63it/s]PTBTokenizer tokenized 20 tokens at 887.68 tokens per second.\n",
      "PTBTokenizer tokenized 305 tokens at 12691.67 tokens per second.\n",
      " 82%| | 402/490 [04:11<00:52,  1.67it/s]PTBTokenizer tokenized 13 tokens at 578.19 tokens per second.\n",
      "PTBTokenizer tokenized 1956 tokens at 69714.38 tokens per second.\n",
      " 82%| | 403/490 [04:12<00:49,  1.77it/s]PTBTokenizer tokenized 18 tokens at 802.39 tokens per second.\n",
      "PTBTokenizer tokenized 1992 tokens at 71825.77 tokens per second.\n",
      " 82%| | 404/490 [04:12<00:49,  1.74it/s]PTBTokenizer tokenized 29 tokens at 1243.93 tokens per second.\n",
      "PTBTokenizer tokenized 2119 tokens at 76315.37 tokens per second.\n",
      " 83%| | 405/490 [04:13<00:51,  1.66it/s]PTBTokenizer tokenized 29 tokens at 1286.54 tokens per second.\n",
      "PTBTokenizer tokenized 1464 tokens at 54737.10 tokens per second.\n",
      " 83%| | 406/490 [04:14<00:50,  1.66it/s]PTBTokenizer tokenized 24 tokens at 1040.72 tokens per second.\n",
      "PTBTokenizer tokenized 2062 tokens at 74827.25 tokens per second.\n",
      " 83%| | 407/490 [04:14<00:51,  1.61it/s]PTBTokenizer tokenized 28 tokens at 1230.94 tokens per second.\n",
      "PTBTokenizer tokenized 1900 tokens at 68894.70 tokens per second.\n",
      " 83%| | 408/490 [04:15<00:49,  1.65it/s]PTBTokenizer tokenized 36 tokens at 1483.44 tokens per second.\n",
      "PTBTokenizer tokenized 1454 tokens at 53971.21 tokens per second.\n",
      " 83%| | 409/490 [04:16<00:50,  1.61it/s]PTBTokenizer tokenized 34 tokens at 1477.46 tokens per second.\n",
      "PTBTokenizer tokenized 2094 tokens at 75252.28 tokens per second.\n",
      " 84%| | 410/490 [04:16<00:51,  1.55it/s]PTBTokenizer tokenized 16 tokens at 692.25 tokens per second.\n",
      "PTBTokenizer tokenized 1728 tokens at 62473.25 tokens per second.\n",
      " 84%| | 411/490 [04:17<00:51,  1.54it/s]PTBTokenizer tokenized 16 tokens at 715.55 tokens per second.\n",
      "PTBTokenizer tokenized 1542 tokens at 56851.28 tokens per second.\n",
      " 84%| | 412/490 [04:17<00:46,  1.68it/s]PTBTokenizer tokenized 26 tokens at 1135.18 tokens per second.\n",
      "PTBTokenizer tokenized 2835 tokens at 99307.32 tokens per second.\n",
      " 84%| | 413/490 [04:18<00:48,  1.60it/s]PTBTokenizer tokenized 12 tokens at 529.42 tokens per second.\n",
      "PTBTokenizer tokenized 1911 tokens at 68281.58 tokens per second.\n",
      " 84%| | 414/490 [04:19<00:45,  1.67it/s]PTBTokenizer tokenized 12 tokens at 493.51 tokens per second.\n",
      "PTBTokenizer tokenized 1917 tokens at 70518.90 tokens per second.\n",
      " 85%| | 415/490 [04:19<00:44,  1.68it/s]PTBTokenizer tokenized 18 tokens at 806.27 tokens per second.\n",
      "PTBTokenizer tokenized 1707 tokens at 64228.27 tokens per second.\n",
      " 85%| | 416/490 [04:20<00:43,  1.71it/s]PTBTokenizer tokenized 12 tokens at 531.31 tokens per second.\n",
      "PTBTokenizer tokenized 2451 tokens at 85837.40 tokens per second.\n",
      " 85%| | 417/490 [04:20<00:44,  1.66it/s]PTBTokenizer tokenized 29 tokens at 1274.20 tokens per second.\n",
      "PTBTokenizer tokenized 1649 tokens at 60191.76 tokens per second.\n",
      " 85%| | 418/490 [04:21<00:43,  1.64it/s]PTBTokenizer tokenized 29 tokens at 1284.44 tokens per second.\n",
      "PTBTokenizer tokenized 2115 tokens at 67601.44 tokens per second.\n",
      " 86%| | 419/490 [04:22<00:45,  1.57it/s]PTBTokenizer tokenized 19 tokens at 823.15 tokens per second.\n",
      "PTBTokenizer tokenized 2629 tokens at 91918.31 tokens per second.\n",
      " 86%| | 420/490 [04:22<00:41,  1.67it/s]PTBTokenizer tokenized 16 tokens at 659.54 tokens per second.\n",
      "PTBTokenizer tokenized 2946 tokens at 98493.79 tokens per second.\n",
      " 86%| | 421/490 [04:23<00:40,  1.69it/s]PTBTokenizer tokenized 15 tokens at 661.27 tokens per second.\n",
      "PTBTokenizer tokenized 1410 tokens at 51046.45 tokens per second.\n",
      " 86%| | 422/490 [04:24<00:42,  1.62it/s]PTBTokenizer tokenized 15 tokens at 662.99 tokens per second.\n",
      "PTBTokenizer tokenized 1536 tokens at 57250.56 tokens per second.\n",
      " 86%| | 423/490 [04:24<00:40,  1.67it/s]PTBTokenizer tokenized 20 tokens at 905.59 tokens per second.\n",
      "PTBTokenizer tokenized 788 tokens at 30882.94 tokens per second.\n",
      " 87%| | 424/490 [04:25<00:41,  1.60it/s]PTBTokenizer tokenized 12 tokens at 518.46 tokens per second.\n",
      "PTBTokenizer tokenized 101 tokens at 4273.14 tokens per second.\n",
      " 87%| | 425/490 [04:25<00:38,  1.70it/s]PTBTokenizer tokenized 17 tokens at 769.72 tokens per second.\n",
      "PTBTokenizer tokenized 991 tokens at 37416.34 tokens per second.\n",
      " 87%| | 426/490 [04:26<00:37,  1.69it/s]PTBTokenizer tokenized 37 tokens at 1658.84 tokens per second.\n",
      "PTBTokenizer tokenized 1330 tokens at 50993.30 tokens per second.\n",
      " 87%| | 427/490 [04:27<00:40,  1.57it/s]PTBTokenizer tokenized 10 tokens at 452.97 tokens per second.\n",
      "PTBTokenizer tokenized 605 tokens at 23953.01 tokens per second.\n",
      " 87%| | 428/490 [04:27<00:41,  1.50it/s]PTBTokenizer tokenized 28 tokens at 1243.75 tokens per second.\n",
      "PTBTokenizer tokenized 422 tokens at 17622.59 tokens per second.\n",
      " 88%| | 429/490 [04:28<00:41,  1.48it/s]PTBTokenizer tokenized 16 tokens at 708.04 tokens per second.\n",
      "PTBTokenizer tokenized 1515 tokens at 55845.29 tokens per second.\n",
      " 88%| | 430/490 [04:29<00:38,  1.54it/s]PTBTokenizer tokenized 10 tokens at 441.85 tokens per second.\n",
      "PTBTokenizer tokenized 1262 tokens at 48429.11 tokens per second.\n",
      " 88%| | 431/490 [04:29<00:36,  1.62it/s]PTBTokenizer tokenized 16 tokens at 727.62 tokens per second.\n",
      "PTBTokenizer tokenized 1816 tokens at 66143.88 tokens per second.\n",
      " 88%| | 432/490 [04:30<00:37,  1.54it/s]PTBTokenizer tokenized 14 tokens at 620.58 tokens per second.\n",
      "PTBTokenizer tokenized 143 tokens at 6191.77 tokens per second.\n",
      " 88%| | 433/490 [04:31<00:36,  1.56it/s]PTBTokenizer tokenized 19 tokens at 849.26 tokens per second.\n",
      "PTBTokenizer tokenized 1928 tokens at 69459.23 tokens per second.\n",
      " 89%| | 434/490 [04:31<00:36,  1.53it/s]PTBTokenizer tokenized 13 tokens at 560.37 tokens per second.\n",
      "PTBTokenizer tokenized 8 tokens at 355.49 tokens per second.\n",
      " 89%| | 435/490 [04:32<00:32,  1.69it/s]PTBTokenizer tokenized 27 tokens at 1182.43 tokens per second.\n",
      "PTBTokenizer tokenized 1873 tokens at 68000.53 tokens per second.\n",
      " 89%| | 436/490 [04:32<00:34,  1.57it/s]PTBTokenizer tokenized 23 tokens at 1026.84 tokens per second.\n",
      "PTBTokenizer tokenized 3023 tokens at 96413.02 tokens per second.\n",
      " 89%| | 437/490 [04:33<00:35,  1.50it/s]PTBTokenizer tokenized 22 tokens at 983.09 tokens per second.\n",
      "PTBTokenizer tokenized 1721 tokens at 62499.25 tokens per second.\n",
      " 89%| | 438/490 [04:34<00:33,  1.55it/s]PTBTokenizer tokenized 12 tokens at 525.54 tokens per second.\n",
      "PTBTokenizer tokenized 2285 tokens at 72684.34 tokens per second.\n",
      " 90%| | 439/490 [04:34<00:32,  1.59it/s]PTBTokenizer tokenized 35 tokens at 1550.92 tokens per second.\n",
      "PTBTokenizer tokenized 1343 tokens at 50529.16 tokens per second.\n",
      " 90%| | 440/490 [04:35<00:32,  1.53it/s]PTBTokenizer tokenized 13 tokens at 527.26 tokens per second.\n",
      "PTBTokenizer tokenized 1470 tokens at 51762.68 tokens per second.\n",
      " 90%| | 441/490 [04:36<00:32,  1.50it/s]PTBTokenizer tokenized 16 tokens at 721.32 tokens per second.\n",
      "PTBTokenizer tokenized 1519 tokens at 58037.69 tokens per second.\n",
      " 90%| | 442/490 [04:36<00:31,  1.53it/s]PTBTokenizer tokenized 15 tokens at 663.08 tokens per second.\n",
      "PTBTokenizer tokenized 819 tokens at 31399.00 tokens per second.\n",
      " 90%| | 443/490 [04:37<00:30,  1.55it/s]PTBTokenizer tokenized 18 tokens at 813.31 tokens per second.\n",
      "PTBTokenizer tokenized 3019 tokens at 105916.36 tokens per second.\n",
      " 91%| | 444/490 [04:38<00:29,  1.57it/s]PTBTokenizer tokenized 13 tokens at 584.20 tokens per second.\n",
      "PTBTokenizer tokenized 1741 tokens at 65150.45 tokens per second.\n",
      " 91%| | 445/490 [04:38<00:28,  1.59it/s]PTBTokenizer tokenized 20 tokens at 876.80 tokens per second.\n",
      "PTBTokenizer tokenized 1622 tokens at 60559.86 tokens per second.\n",
      " 91%| | 446/490 [04:39<00:27,  1.62it/s]PTBTokenizer tokenized 14 tokens at 619.08 tokens per second.\n",
      "PTBTokenizer tokenized 1242 tokens at 46260.11 tokens per second.\n",
      " 91%| | 447/490 [04:39<00:26,  1.64it/s]PTBTokenizer tokenized 23 tokens at 1018.90 tokens per second.\n",
      "PTBTokenizer tokenized 1715 tokens at 62704.61 tokens per second.\n",
      " 91%|| 448/490 [04:40<00:26,  1.58it/s]PTBTokenizer tokenized 39 tokens at 1740.85 tokens per second.\n",
      "PTBTokenizer tokenized 484 tokens at 19404.17 tokens per second.\n",
      " 92%|| 449/490 [04:41<00:26,  1.53it/s]PTBTokenizer tokenized 20 tokens at 889.46 tokens per second.\n",
      "PTBTokenizer tokenized 1011 tokens at 38063.65 tokens per second.\n",
      " 92%|| 450/490 [04:42<00:26,  1.53it/s]PTBTokenizer tokenized 31 tokens at 1364.74 tokens per second.\n",
      "PTBTokenizer tokenized 1701 tokens at 61881.10 tokens per second.\n",
      " 92%|| 451/490 [04:42<00:25,  1.52it/s]PTBTokenizer tokenized 25 tokens at 1087.98 tokens per second.\n",
      "PTBTokenizer tokenized 2577 tokens at 89942.00 tokens per second.\n",
      " 92%|| 452/490 [04:43<00:23,  1.63it/s]PTBTokenizer tokenized 25 tokens at 1080.35 tokens per second.\n",
      "PTBTokenizer tokenized 2302 tokens at 79335.06 tokens per second.\n",
      " 92%|| 453/490 [04:43<00:23,  1.58it/s]PTBTokenizer tokenized 38 tokens at 1643.14 tokens per second.\n",
      "PTBTokenizer tokenized 1911 tokens at 68478.50 tokens per second.\n",
      " 93%|| 454/490 [04:44<00:23,  1.53it/s]PTBTokenizer tokenized 15 tokens at 654.89 tokens per second.\n",
      "PTBTokenizer tokenized 1530 tokens at 56285.07 tokens per second.\n",
      " 93%|| 455/490 [04:45<00:21,  1.59it/s]PTBTokenizer tokenized 15 tokens at 647.74 tokens per second.\n",
      "PTBTokenizer tokenized 1973 tokens at 70000.58 tokens per second.\n",
      " 93%|| 456/490 [04:45<00:20,  1.63it/s]PTBTokenizer tokenized 31 tokens at 1360.08 tokens per second.\n",
      "PTBTokenizer tokenized 1954 tokens at 69377.82 tokens per second.\n",
      " 93%|| 457/490 [04:46<00:21,  1.57it/s]PTBTokenizer tokenized 34 tokens at 1508.68 tokens per second.\n",
      "PTBTokenizer tokenized 1808 tokens at 65378.41 tokens per second.\n",
      " 93%|| 458/490 [04:47<00:20,  1.54it/s]PTBTokenizer tokenized 26 tokens at 1161.44 tokens per second.\n",
      "PTBTokenizer tokenized 1619 tokens at 59445.64 tokens per second.\n",
      " 94%|| 459/490 [04:47<00:20,  1.51it/s]PTBTokenizer tokenized 18 tokens at 780.38 tokens per second.\n",
      "PTBTokenizer tokenized 1534 tokens at 55704.19 tokens per second.\n",
      " 94%|| 460/490 [04:48<00:19,  1.57it/s]PTBTokenizer tokenized 24 tokens at 1055.44 tokens per second.\n",
      "PTBTokenizer tokenized 2246 tokens at 77701.53 tokens per second.\n",
      " 94%|| 461/490 [04:48<00:18,  1.58it/s]PTBTokenizer tokenized 28 tokens at 1241.17 tokens per second.\n",
      "PTBTokenizer tokenized 603 tokens at 23139.06 tokens per second.\n",
      " 94%|| 462/490 [04:49<00:17,  1.56it/s]PTBTokenizer tokenized 19 tokens at 840.49 tokens per second.\n",
      "PTBTokenizer tokenized 1875 tokens at 64727.43 tokens per second.\n",
      " 94%|| 463/490 [04:50<00:16,  1.61it/s]PTBTokenizer tokenized 10 tokens at 433.78 tokens per second.\n",
      "PTBTokenizer tokenized 2269 tokens at 78905.65 tokens per second.\n",
      " 95%|| 464/490 [04:50<00:15,  1.65it/s]PTBTokenizer tokenized 27 tokens at 1191.31 tokens per second.\n",
      "PTBTokenizer tokenized 543 tokens at 20881.13 tokens per second.\n",
      " 95%|| 465/490 [04:51<00:15,  1.58it/s]PTBTokenizer tokenized 14 tokens at 602.89 tokens per second.\n",
      "PTBTokenizer tokenized 1665 tokens at 61341.61 tokens per second.\n",
      " 95%|| 466/490 [04:52<00:14,  1.65it/s]PTBTokenizer tokenized 18 tokens at 757.62 tokens per second.\n",
      "PTBTokenizer tokenized 1286 tokens at 48268.96 tokens per second.\n",
      " 95%|| 467/490 [04:52<00:14,  1.58it/s]PTBTokenizer tokenized 24 tokens at 1063.09 tokens per second.\n",
      "PTBTokenizer tokenized 2397 tokens at 83262.82 tokens per second.\n",
      " 96%|| 468/490 [04:53<00:14,  1.52it/s]PTBTokenizer tokenized 40 tokens at 1748.91 tokens per second.\n",
      "PTBTokenizer tokenized 1437 tokens at 54218.12 tokens per second.\n",
      " 96%|| 469/490 [04:54<00:14,  1.47it/s]PTBTokenizer tokenized 25 tokens at 1113.06 tokens per second.\n",
      "PTBTokenizer tokenized 2667 tokens at 90822.05 tokens per second.\n",
      " 96%|| 470/490 [04:54<00:13,  1.48it/s]PTBTokenizer tokenized 16 tokens at 716.75 tokens per second.\n",
      "PTBTokenizer tokenized 1901 tokens at 68826.97 tokens per second.\n",
      " 96%|| 471/490 [04:55<00:13,  1.40it/s]PTBTokenizer tokenized 27 tokens at 1199.09 tokens per second.\n",
      "PTBTokenizer tokenized 3337 tokens at 111000.65 tokens per second.\n",
      " 96%|| 472/490 [04:56<00:12,  1.41it/s]PTBTokenizer tokenized 20 tokens at 894.39 tokens per second.\n",
      "PTBTokenizer tokenized 708 tokens at 27871.39 tokens per second.\n",
      " 97%|| 473/490 [04:57<00:12,  1.40it/s]PTBTokenizer tokenized 14 tokens at 624.14 tokens per second.\n",
      "PTBTokenizer tokenized 1183 tokens at 45602.47 tokens per second.\n",
      " 97%|| 474/490 [04:57<00:11,  1.37it/s]PTBTokenizer tokenized 14 tokens at 615.62 tokens per second.\n",
      "PTBTokenizer tokenized 1264 tokens at 47550.90 tokens per second.\n",
      " 97%|| 475/490 [04:58<00:10,  1.46it/s]PTBTokenizer tokenized 12 tokens at 546.51 tokens per second.\n",
      "PTBTokenizer tokenized 3185 tokens at 109082.27 tokens per second.\n",
      " 97%|| 476/490 [04:59<00:09,  1.51it/s]PTBTokenizer tokenized 26 tokens at 1150.43 tokens per second.\n",
      "PTBTokenizer tokenized 1478 tokens at 55761.20 tokens per second.\n",
      " 97%|| 477/490 [04:59<00:08,  1.55it/s]PTBTokenizer tokenized 41 tokens at 1784.95 tokens per second.\n",
      "PTBTokenizer tokenized 1548 tokens at 55755.51 tokens per second.\n",
      " 98%|| 478/490 [05:00<00:08,  1.49it/s]PTBTokenizer tokenized 11 tokens at 484.75 tokens per second.\n",
      "PTBTokenizer tokenized 1631 tokens at 61960.67 tokens per second.\n",
      " 98%|| 479/490 [05:01<00:07,  1.46it/s]PTBTokenizer tokenized 32 tokens at 1406.32 tokens per second.\n",
      "PTBTokenizer tokenized 2840 tokens at 93398.97 tokens per second.\n",
      " 98%|| 480/490 [05:01<00:06,  1.43it/s]PTBTokenizer tokenized 23 tokens at 1018.52 tokens per second.\n",
      "PTBTokenizer tokenized 1692 tokens at 61211.37 tokens per second.\n",
      " 98%|| 481/490 [05:02<00:06,  1.41it/s]PTBTokenizer tokenized 18 tokens at 796.39 tokens per second.\n",
      "PTBTokenizer tokenized 2066 tokens at 71196.43 tokens per second.\n",
      " 98%|| 482/490 [05:03<00:05,  1.40it/s]PTBTokenizer tokenized 10 tokens at 435.81 tokens per second.\n",
      "PTBTokenizer tokenized 2267 tokens at 79033.48 tokens per second.\n",
      " 99%|| 483/490 [05:03<00:04,  1.51it/s]PTBTokenizer tokenized 14 tokens at 634.70 tokens per second.\n",
      "PTBTokenizer tokenized 1350 tokens at 49216.31 tokens per second.\n",
      " 99%|| 484/490 [05:04<00:03,  1.61it/s]PTBTokenizer tokenized 33 tokens at 1376.87 tokens per second.\n",
      "PTBTokenizer tokenized 1899 tokens at 70493.99 tokens per second.\n",
      " 99%|| 485/490 [05:05<00:03,  1.53it/s]PTBTokenizer tokenized 41 tokens at 1817.59 tokens per second.\n",
      "PTBTokenizer tokenized 2145 tokens at 76688.74 tokens per second.\n",
      " 99%|| 486/490 [05:05<00:02,  1.50it/s]PTBTokenizer tokenized 20 tokens at 883.30 tokens per second.\n",
      "PTBTokenizer tokenized 1902 tokens at 67723.71 tokens per second.\n",
      " 99%|| 487/490 [05:06<00:01,  1.56it/s]PTBTokenizer tokenized 17 tokens at 749.34 tokens per second.\n",
      "PTBTokenizer tokenized 2396 tokens at 84017.18 tokens per second.\n",
      "100%|| 488/490 [05:06<00:01,  1.58it/s]PTBTokenizer tokenized 30 tokens at 1348.62 tokens per second.\n",
      "PTBTokenizer tokenized 1531 tokens at 51200.73 tokens per second.\n",
      "100%|| 489/490 [05:07<00:00,  1.50it/s]PTBTokenizer tokenized 14 tokens at 631.62 tokens per second.\n",
      "PTBTokenizer tokenized 1945 tokens at 71113.35 tokens per second.\n",
      "100%|| 490/490 [05:08<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_ptb = PTBTokenizer()\n",
    "cider_scorer = CiderScorer()\n",
    "\n",
    "for image_name, data in tqdm(validation_data.items()):\n",
    "    gts = {}\n",
    "    res = {}\n",
    "    candidate_corpus = []\n",
    "\n",
    "    processed_image = data['image_tensor']\n",
    "    with torch.no_grad():\n",
    "        output = best_model.cpu().generate(\n",
    "                    processed_image.unsqueeze(0).cpu(), max_len=25, temperature=1.0, beam_width=10, top_k=10\n",
    "        )\n",
    "        for entry in output:            \n",
    "            # Convert the predicted output to a list of words\n",
    "            generated_caption = [\n",
    "                embedding.itos[index.item()]\n",
    "                for index in entry\n",
    "                if index.item() != 0 and index.item() != embedding.stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
    "            ]\n",
    "            # Merge the generated caption, as it will be sent to the tokenizer created by stanford\n",
    "            sentence_generated_caption = \" \".join(generated_caption)\n",
    "\n",
    "            candidate_corpus.append(sentence_generated_caption)\n",
    "        # the candidate sentences should be in one list comma seperated\n",
    "        gts[image_name] = candidate_corpus    \n",
    "        \n",
    "        gts = tokenizer_ptb.tokenize(gts)\n",
    "        \n",
    "        # the reference sentences should be in one list comma seperated\n",
    "        res[image_name] = [\" \".join(caption) for caption in data['captions']]\n",
    "        \n",
    "        res = tokenizer_ptb.tokenize(res)   \n",
    "        \n",
    "        # For every image we have 2 candidate captions\n",
    "        for i in range(len(candidate_corpus)):\n",
    "            cider_scorer += (gts[image_name][i], res[image_name])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average \u001b[1mCIDEr\u001b[0m score on the validation dataset is 0.059\n"
     ]
    }
   ],
   "source": [
    "cider_score, cider_scores = cider_scorer.compute_score()\n",
    "print(f\"Average {color.BOLD}CIDEr{color.END} score on the validation dataset is {cider_score:.3f}\")\n",
    "# print(f\"Individual CIDEr score for the images are: {[score for score in cider_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BERT Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 490/490 [25:09<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "p_scores = []\n",
    "r_scores = []\n",
    "f_scores = []\n",
    "\n",
    "for image_name, data in tqdm(validation_data.items()):\n",
    "    \n",
    "    candidate_corpus = []\n",
    "\n",
    "    processed_image = data['image_tensor']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = best_model.cpu().generate(\n",
    "                    processed_image.unsqueeze(0).cpu(), max_len=25, temperature=1.0, beam_width=10, top_k=10\n",
    "        )\n",
    "        for entry in output:            \n",
    "            # Convert the predicted output to a list of words\n",
    "            generated_caption = [\n",
    "                embedding.itos[index.item()]\n",
    "                for index in entry\n",
    "                if index.item() != 0 and index.item() != embedding.stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
    "            ]\n",
    "            # Merge the generated caption, as it will be sent to the model tokenizer\n",
    "            sentence_generated_caption = \" \".join(generated_caption)\n",
    "\n",
    "            candidate_corpus.append(sentence_generated_caption)\n",
    "        \n",
    "        # the reference sentences should be in one list comma seperated\n",
    "        reference_corpus = [\" \".join(caption) for caption in data['captions']]\n",
    "        reference_corpus = [reference_corpus] * 2\n",
    "        \n",
    "        # The model \"microsoft/deberta-large-mnli\" is used because the correlation with human evaluation is higher\n",
    "        # as compared to default model and bert-base-uncased model\n",
    "        # By default it uses the best layer of the specidied model as specidied in sheet at link below\n",
    "        # Ref:- https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 \n",
    "        P_mul,R_mul,F_mul=score(candidate_corpus, reference_corpus,lang=\"en\", model_type=\"microsoft/deberta-large-mnli\",batch_size=100,device=device)\n",
    "        \n",
    "        # Append the average of the P, R, F scores for the 2 generated captions per image\n",
    "        p_scores.append(P_mul.mean())\n",
    "        r_scores.append(R_mul.mean())\n",
    "        f_scores.append(F_mul.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average \u001b[1mPrecision\u001b[0m score on the validation dataset is 0.566\n",
      "Average \u001b[1mRecall\u001b[0m score on the validation dataset is 0.561\n",
      "Average \u001b[1mF1\u001b[0m score on the validation dataset is 0.549\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of tensors to a tensor\n",
    "p_scores_tensor = torch.stack(p_scores)\n",
    "r_scores_tensor = torch.stack(r_scores)\n",
    "f_scores_tensor = torch.stack(f_scores)\n",
    "\n",
    "# print the average of precision, recall and F1 score across all the validation dataset images\n",
    "print(f\"Average {color.BOLD}Precision{color.END} score on the validation dataset is {p_scores_tensor.mean():.3f}\")\n",
    "print(f\"Average {color.BOLD}Recall{color.END} score on the validation dataset is {r_scores_tensor.mean():.3f}\")\n",
    "print(f\"Average {color.BOLD}F1{color.END} score on the validation dataset is {f_scores_tensor.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate BART Score on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 490/490 [20:21<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "bart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\n",
    "bartscores = []\n",
    "\n",
    "for image_name, data in tqdm(validation_data.items()):\n",
    "    \n",
    "    candidate_corpus = []\n",
    "\n",
    "    processed_image = data['image_tensor']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = best_model.cpu().generate(\n",
    "                    processed_image.unsqueeze(0).cpu(), max_len=25, temperature=1.0, beam_width=10, top_k=10\n",
    "        )\n",
    "        for entry in output:            \n",
    "            # Convert the predicted output to a list of words\n",
    "            generated_caption = [\n",
    "                embedding.itos[index.item()]\n",
    "                for index in entry\n",
    "                if index.item() != 0 and index.item() != embedding.stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
    "            ]\n",
    "            # Merge the generated caption, as it will be sent to the model tokenizer\n",
    "            sentence_generated_caption = \" \".join(generated_caption)\n",
    "\n",
    "            candidate_corpus.append(sentence_generated_caption)\n",
    "        \n",
    "        # the reference sentences should be in one list comma seperated\n",
    "        reference_corpus = [\" \".join(caption) for caption in data['captions']]\n",
    "        reference_corpus = [reference_corpus] * len(candidate_corpus)\n",
    "        \n",
    "        bartscore = bart_scorer.multi_ref_score(candidate_corpus, reference_corpus, agg=\"max\", batch_size=100)\n",
    "\n",
    "        # Append the average of the two bartscores for the 2 generated captions per image\n",
    "        avg_bartscore = sum(bartscore)/len(bartscore)\n",
    "        bartscores.append(avg_bartscore)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average \u001b[1mBART\u001b[0m score on the validation dataset is -4.069\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average BART Score on the entire validation dataset\n",
    "valdata_bartscore = sum(bartscores)/len(bartscores)\n",
    "print(f\"Average {color.BOLD}BART{color.END} score on the validation dataset is {valdata_bartscore:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvmeme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
