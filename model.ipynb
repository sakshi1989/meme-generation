{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set pytorch device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bairathirahul/Workspace/image-captioning/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd()  # get the current working directory\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data') # Concatenate the folder name which contains images\n",
    "DATA_DIR  # Check the final path value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368342, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['yuno.jpg', 'y-u-no.jpg',\n",
       "       'the real slim shady y you no stand up ?'], dtype='<U689')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "meme_captions = np.load(os.path.join(DATA_DIR, 'meme_captions.npy'), allow_pickle=True)\n",
    "print(meme_captions.shape)\n",
    "meme_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-u-no.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GloVe' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bairathirahul/Workspace/image-captioning/model.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m model \u001b[39m=\u001b[39m CaptionEncoder(\u001b[39m200\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m embedding \u001b[39m=\u001b[39m model(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m embedding \u001b[39m=\u001b[39m CaptionEncoder(\u001b[39m200\u001b[39m)(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(embedding\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/deephumor/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/bairathirahul/Workspace/image-captioning/model.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mget_vecs_by_tokens(tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bairathirahul/Workspace/image-captioning/model.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(vectors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GloVe' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Torch Module using GloVe Embeddings\n",
    "class CaptionEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(CaptionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        \n",
    "        self.embedding = GloVe(name='twitter.27B', dim=embed_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        tokens = self.tokenizer(x)\n",
    "\n",
    "        print(self.embedding.device)\n",
    "\n",
    "        vectors = self.embedding.get_vecs_by_tokens(tokens)\n",
    "\n",
    "        return self.dropout(vectors)\n",
    "    \n",
    "text = meme_captions[0][1]\n",
    "print(text)\n",
    "\n",
    "model = CaptionEncoder(200).to(device)\n",
    "\n",
    "embedding = model(text)\n",
    "\n",
    "embedding = CaptionEncoder(200)(text)\n",
    "print(embedding.device)\n",
    "print(embedding.shape)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1927, -0.0785, -0.5948, -0.2257,  0.1356, -0.1189, -1.0368, -0.7186,\n",
       "          0.3053, -0.1520,  0.7308, -0.0852, -0.7530, -0.2627, -0.3564,  0.2473,\n",
       "          0.4449,  0.2278, -0.1392, -0.4025,  0.1228, -1.1415,  0.5589,  0.1911,\n",
       "          0.2443,  0.2190, -0.8402,  0.3068, -0.0629, -0.2389, -0.1502, -0.4937,\n",
       "         -0.3047,  0.4230, -0.0375,  0.5902,  0.1465, -0.4917, -0.1273, -0.6709,\n",
       "          0.8164,  0.8518,  0.3649, -0.4915,  0.9703, -0.8993, -0.4560,  0.8441,\n",
       "         -0.2487,  0.1850, -0.2462,  0.3159, -0.2192,  0.3389, -0.3913, -0.4828,\n",
       "          0.1312, -1.5410,  0.6713, -0.7781, -0.2192, -0.6062, -0.0609,  1.2553,\n",
       "         -0.4616,  0.0454,  1.1359,  0.4332,  0.3019,  1.0750,  0.7409,  0.5057,\n",
       "         -0.2241, -0.4372, -0.1643, -0.6472, -0.2351,  0.1329,  0.7971,  0.1797,\n",
       "          0.9245,  0.2165, -0.7465,  0.2341, -1.2789,  0.0423,  0.0151,  0.4509,\n",
       "         -0.4216, -1.0433, -1.0251,  1.4620, -0.8937,  0.4200, -0.7396, -0.3900,\n",
       "          0.8238,  0.7280,  0.3034,  0.9298, -0.2603,  0.2319, -0.7606, -0.3627,\n",
       "          0.3754,  0.7093,  0.9252, -0.5953,  0.2102,  0.6080, -0.0080,  0.0210,\n",
       "         -0.4743, -0.2855, -0.1170, -0.7796,  0.5734,  0.4495, -0.8694, -0.4723,\n",
       "          0.2770, -0.8724, -0.6170,  0.2984,  0.7837,  0.6887, -0.0852,  1.0773,\n",
       "          1.0247,  1.1371,  0.7725,  0.5631, -0.1914, -0.7510, -0.0849, -0.7548,\n",
       "         -1.3829, -0.2565, -0.5601, -0.7730,  0.9021, -0.9767,  0.4469, -0.5589,\n",
       "         -1.4073, -0.0368,  0.1868,  1.0495,  0.6786,  0.6419,  0.8192, -0.3801,\n",
       "          0.2325,  0.0922, -0.6330,  1.1845,  0.1528, -0.5767, -0.3635,  0.0629,\n",
       "         -0.5313, -0.8077, -2.2574,  0.3606, -0.1089, -0.2156,  0.1691, -0.4221,\n",
       "          0.0196, -0.1368, -0.6267, -0.8086,  0.1203, -0.3833,  0.4888, -0.8852,\n",
       "         -0.3972,  0.4823, -0.8029, -0.5178, -1.3406, -0.6444,  0.7167, -0.6379,\n",
       "         -0.1087, -0.2952,  1.5368, -0.5770,  0.5052,  1.4104,  0.6129, -0.7309,\n",
       "          0.6106,  0.4981, -0.4317, -0.1913,  0.4404,  0.0888, -1.1753, -0.4960]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Torch module for image encoding using Inception V3\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.inception = models.inception_v3(\n",
    "            weights=models.Inception_V3_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        self.inception.eval()\n",
    "\n",
    "        self.fc = nn.Linear(in_features=1000, out_features=embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        vectors = self.inception(x)\n",
    "\n",
    "        return self.fc(vectors)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = PIL.Image.open(os.path.join(DATA_DIR, \"memes\", \"y-u-no.jpg\"))\n",
    "\n",
    "    image_tensors = transforms.ToTensor()(image)\n",
    "\n",
    "    image_tensors_resized = transforms.Resize((299, 299), antialias=True)(\n",
    "        image_tensors\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    embedding = ImageEncoder(200)(image_tensors_resized)\n",
    "\n",
    "print(embedding.shape)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelEncoder(nn.Module):\n",
    "    \"\"\"ImageLabel encoder.\n",
    "\n",
    "    Encodes images and text labels into a single embedding of size `emb_dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tokens, emb_dim=256, dropout=0.2):\n",
    "        \"\"\"Initializes LabelEncoder.\n",
    "\n",
    "        Args:\n",
    "            num_tokens: number of tokens in the vocabulary\n",
    "            emb_dim (int): dimensions of the output embedding\n",
    "            dropout (float): dropout for the encoded features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(emb_dim, dropout)\n",
    "        self.label_encoder = LabelEncoder(num_tokens, emb_dim, dropout)\n",
    "        self.linear = nn.Linear(2 * emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (torch.Tensor): input images of shape `[bs, width, height]`\n",
    "            labels (torch.Tensor): input text labels of shape `[bs, seq_len]`\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: combined image-label embedding of shape `[bs, emb_dim]`\n",
    "        \"\"\"\n",
    "        image_emb = self.image_encoder(images)\n",
    "        label_emb = self.label_encoder(labels)\n",
    "\n",
    "        emb = torch.cat([image_emb, label_emb], dim=1)\n",
    "        emb = self.dropout(self.linear(emb))\n",
    "\n",
    "        return emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deephumor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
